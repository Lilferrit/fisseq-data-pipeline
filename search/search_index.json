{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FISSEQ Data Pipeline Welcome to the FISSEQ Data Pipeline documentation. This project provides a reproducible, configurable pipeline for processing FISSEQ cell profiling data, including cleaning, normalization, harmonization, and stratified evaluation. Features Command-line interface (CLI) Access the pipeline with a single entry point: fisseq-data-pipeline [validate|run|configure] For more details on command line usage see Pipeline . Data cleaning Remove invalid rows/columns and rare label\u2013batch pairs. See Filter . Normalization Compute z-score normalization statistics on control samples and apply them across the dataset. See Normalize . Harmonization Apply ComBat batch correction via neuroHarmonize. See Harmonize . Installation This package in its current state should be considered experimental, and is thus not hosted on PyPI. However, the package may be installed directly from Github using the command: pip install git+https://github.com/Lilferrit/fisseq-data-pipeline.git You may also clone the repository and install dependencies: git clone https://github.com/your-org/fisseq-data-pipeline.git cd fisseq-data-pipeline pip install -e . Running the Pipeline After installation the pipeline can be run from the command line. For more details see Pipeline . Configuration The pipeline may be configured using a yaml configuration file. For more details see Configuration .","title":"Home"},{"location":"#fisseq-data-pipeline","text":"Welcome to the FISSEQ Data Pipeline documentation. This project provides a reproducible, configurable pipeline for processing FISSEQ cell profiling data, including cleaning, normalization, harmonization, and stratified evaluation.","title":"FISSEQ Data Pipeline"},{"location":"#features","text":"","title":"Features"},{"location":"#command-line-interface-cli","text":"Access the pipeline with a single entry point: fisseq-data-pipeline [validate|run|configure] For more details on command line usage see Pipeline .","title":"Command-line interface (CLI)"},{"location":"#data-cleaning","text":"Remove invalid rows/columns and rare label\u2013batch pairs. See Filter .","title":"Data cleaning"},{"location":"#normalization","text":"Compute z-score normalization statistics on control samples and apply them across the dataset. See Normalize .","title":"Normalization"},{"location":"#harmonization","text":"Apply ComBat batch correction via neuroHarmonize. See Harmonize .","title":"Harmonization"},{"location":"#installation","text":"This package in its current state should be considered experimental, and is thus not hosted on PyPI. However, the package may be installed directly from Github using the command: pip install git+https://github.com/Lilferrit/fisseq-data-pipeline.git You may also clone the repository and install dependencies: git clone https://github.com/your-org/fisseq-data-pipeline.git cd fisseq-data-pipeline pip install -e .","title":"Installation"},{"location":"#running-the-pipeline","text":"After installation the pipeline can be run from the command line. For more details see Pipeline .","title":"Running the Pipeline"},{"location":"#configuration","text":"The pipeline may be configured using a yaml configuration file. For more details see Configuration .","title":"Configuration"},{"location":"configuration/","text":"Configuration YAML The FISSEQ pipeline is configured with a YAML file that defines how to interpret the dataset. A default configuration ( config.yaml ) ships with the pipeline and is used if no custom configuration is provided. Default configuration # Regex or list to select feature columns # (CellProfiler columns start with an uppercase and contain an underscore) feature_cols: \"^[A-Z][A-Za-z0-9]*_.*\" # SQL-like WHERE clause to select control samples. # The query will be interpolated into: # SELECT * FROM self WHERE {control_sample_query} control_sample_query: \"variantClass = 'WT'\" # The name of the column containing the batch identifier batch_col_name: \"tile_experiment_well\" # Column containing biological labels label_col_name: \"aaChanges\" Field descriptions feature_cols Type: str (regex pattern) or list[str] (explicit list). Default: ^[A-Z][A-Za-z0-9]*_.* Defines which columns are treated as features. For CellProfiler outputs, this matches columns starting with an uppercase letter and containing an underscore. control_sample_query Type: str (SQL-like WHERE clause). Default: variantClass = 'WT' Used to flag control samples. Applied as a boolean mask when constructing metadata. Example control_sample_query: \"treatment = 'DMSO'\" batch_col_name Type: str Default: tile_experiment_well Column containing batch identifiers (e.g., well, experiment, or run). Used for stratification and harmonization. label_col_name Type: str Default: aaChanges Column containing biological labels (e.g., variant information). Used for stratification during validation.","title":"Configuration"},{"location":"configuration/#configuration-yaml","text":"The FISSEQ pipeline is configured with a YAML file that defines how to interpret the dataset. A default configuration ( config.yaml ) ships with the pipeline and is used if no custom configuration is provided.","title":"Configuration YAML"},{"location":"configuration/#default-configuration","text":"# Regex or list to select feature columns # (CellProfiler columns start with an uppercase and contain an underscore) feature_cols: \"^[A-Z][A-Za-z0-9]*_.*\" # SQL-like WHERE clause to select control samples. # The query will be interpolated into: # SELECT * FROM self WHERE {control_sample_query} control_sample_query: \"variantClass = 'WT'\" # The name of the column containing the batch identifier batch_col_name: \"tile_experiment_well\" # Column containing biological labels label_col_name: \"aaChanges\"","title":"Default configuration"},{"location":"configuration/#field-descriptions","text":"","title":"Field descriptions"},{"location":"configuration/#feature_cols","text":"Type: str (regex pattern) or list[str] (explicit list). Default: ^[A-Z][A-Za-z0-9]*_.* Defines which columns are treated as features. For CellProfiler outputs, this matches columns starting with an uppercase letter and containing an underscore.","title":"feature_cols"},{"location":"configuration/#control_sample_query","text":"Type: str (SQL-like WHERE clause). Default: variantClass = 'WT' Used to flag control samples. Applied as a boolean mask when constructing metadata.","title":"control_sample_query"},{"location":"configuration/#example","text":"control_sample_query: \"treatment = 'DMSO'\"","title":"Example"},{"location":"configuration/#batch_col_name","text":"Type: str Default: tile_experiment_well Column containing batch identifiers (e.g., well, experiment, or run). Used for stratification and harmonization.","title":"batch_col_name"},{"location":"configuration/#label_col_name","text":"Type: str Default: aaChanges Column containing biological labels (e.g., variant information). Used for stratification during validation.","title":"label_col_name"},{"location":"filter/","text":"Data Cleaning Utilities The fisseq_data_pipeline.filter module provides functions to clean and filter feature/metadata tables prior to normalization and harmonization. These utilities are invoked automatically in the pipeline, but can also be used independently. Overview clean_data : Removes invalid rows/columns from feature and metadata tables while keeping them aligned. drop_infrequent_pairs : Drops rows from rare (label, batch) groups according to a configurable threshold. Environment variables FISSEQ_PIPELINE_MIN_CLASS_MEMBERS Minimum number of samples required per (label, batch) group when running drop_infrequent_pairs . Default: 2 . Example: # Require at least 5 samples per label\u2013batch group FISSEQ_PIPELINE_MIN_CLASS_MEMBERS=5 fisseq-data-pipeline validate ... Example Usage import polars as pl from fisseq_data_pipeline.filter import clean_data, drop_infrequent_pairs # Example feature matrix feature_df = pl.DataFrame({ \"f1\": [1.0, 2.0, float(\"nan\"), 4.0], \"f2\": [5.0, 6.0, 7.0, 8.0], }) # Example metadata with batch + label meta_df = pl.DataFrame({ \"_label\": [\"A\", \"A\", \"B\", \"B\"], \"_batch\": [\"X\", \"Y\", \"X\", \"Y\"], }) # Clean non-finite and zero-variance columns/rows feature_df, meta_df = clean_data(feature_df, meta_df) # Drop infrequent (label, batch) pairs feature_df, meta_df = drop_infrequent_pairs(feature_df, meta_df) API reference fisseq_data_pipeline . filter . clean_data ( feature_df , meta_data_df ) Clean feature and metadata tables and keep them row-aligned. The cleaning pipeline performs four passes: 1) Drop feature columns that are entirely non-finite. 2) Drop rows that contain any remaining non-finite value across. 3) Drop feature columns with (near) zero variance. All feature columns must be numeric Parameters: feature_df ( DataFrame ) \u2013 Feature-only table, shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata aligned row-wise with feature_df . Must contain \"_label\" and \"_batch\" columns. Returns: ( DataFrame , DataFrame ) \u2013 The cleaned (feature_df, meta_data_df) , with invalid rows/columns removed and row alignment preserved. Source code in src/fisseq_data_pipeline/filter.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def clean_data ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame ) -> Tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\" Clean feature and metadata tables and keep them row-aligned. The cleaning pipeline performs four passes: 1) Drop feature columns that are entirely non-finite. 2) Drop rows that contain any remaining non-finite value across. 3) Drop feature columns with (near) zero variance. All feature columns must be numeric Parameters ---------- feature_df : pl.DataFrame Feature-only table, shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata aligned row-wise with ``feature_df``. Must contain ``\"_label\"`` and ``\"_batch\"`` columns. Returns ------- (pl.DataFrame, pl.DataFrame) The cleaned ``(feature_df, meta_data_df)``, with invalid rows/columns removed and row alignment preserved. \"\"\" # Drop columns containing all non-finite values feature_df = feature_df . fill_null ( float ( \"nan\" )) n_rows = feature_df . height is_all_nonfinite = feature_df . select ( ~ pl . all () . is_finite () . any ()) all_nonfinite_cols = [ c for c in feature_df . columns if is_all_nonfinite . get_column ( c ) . item () ] feature_df = feature_df . select ( pl . exclude ( all_nonfinite_cols )) logging . info ( \"Removed %d columns containing only non-finite values\" , len ( all_nonfinite_cols ) ) # Drop rows containing remaining non-finite values row_mask = feature_df . select ( pl . all_horizontal ( pl . all () . is_finite ())) . to_series () feature_df = feature_df . filter ( row_mask ) meta_data_df = meta_data_df . filter ( row_mask ) logging . debug ( \"Dropped %d rows containing non-finite values\" , n_rows - feature_df . height ) # Drop rows that have 0 variance variances = feature_df . var () . row ( 0 , named = True ) zero_var_cols = [ c for c in feature_df . columns if float ( variances [ c ]) < np . finfo ( np . float32 ) . eps ] feature_df = feature_df . select ( pl . exclude ( zero_var_cols )) logging . info ( \"Removed %d columns containing zero variance\" , len ( zero_var_cols )) return feature_df , meta_data_df fisseq_data_pipeline . filter . drop_infrequent_pairs ( feature_df , meta_data_df ) Remove rows belonging to rare ( _label , _batch ) groups. Rows are grouped by the concatenation of _label and _batch . Any group with a sample count less than FISSEQ_PIPELINE_MIN_CLASS_MEMBERS (default: 2) is dropped. Parameters: feature_df ( DataFrame ) \u2013 Feature-only table of shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata table row-aligned with feature_df . Must include \"_label\" and \"_batch\" columns. Returns: ( DataFrame , DataFrame ) \u2013 A tuple (feature_df, meta_data_df) with rows from infrequent label\u2013batch groups removed. Alignment is preserved. Source code in src/fisseq_data_pipeline/filter.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def drop_infrequent_pairs ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame ) -> Tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\" Remove rows belonging to rare (``_label``, ``_batch``) groups. Rows are grouped by the concatenation of ``_label`` and ``_batch``. Any group with a sample count less than ``FISSEQ_PIPELINE_MIN_CLASS_MEMBERS`` (default: 2) is dropped. Parameters ---------- feature_df : pl.DataFrame Feature-only table of shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata table row-aligned with ``feature_df``. Must include ``\"_label\"`` and ``\"_batch\"`` columns. Returns ------- (pl.DataFrame, pl.DataFrame) A tuple ``(feature_df, meta_data_df)`` with rows from infrequent label\u2013batch groups removed. Alignment is preserved. \"\"\" # Drop rows that have infrequent (batch, label) batch pairs n_rows = feature_df . height label_batch = ( meta_data_df . get_column ( \"_label\" ) + \"_\" + meta_data_df . get_column ( \"_batch\" ) ) . alias ( \"_label_batch\" ) label_batch_counts = label_batch . value_counts () . filter ( pl . col ( \"count\" ) >= MINIMUM_CLASS_MEMBERS ) label_batch_freq_mask = label_batch . is_in ( label_batch_counts . get_column ( \"_label_batch\" ) ) feature_df = feature_df . filter ( label_batch_freq_mask ) meta_data_df = meta_data_df . filter ( label_batch_freq_mask ) logging . info ( \"Dropped %d rows containing batch label pairs with frequency less than %d \" , n_rows - feature_df . height , MINIMUM_CLASS_MEMBERS , ) return feature_df , meta_data_df","title":"Filter"},{"location":"filter/#data-cleaning-utilities","text":"The fisseq_data_pipeline.filter module provides functions to clean and filter feature/metadata tables prior to normalization and harmonization. These utilities are invoked automatically in the pipeline, but can also be used independently.","title":"Data Cleaning Utilities"},{"location":"filter/#overview","text":"clean_data : Removes invalid rows/columns from feature and metadata tables while keeping them aligned. drop_infrequent_pairs : Drops rows from rare (label, batch) groups according to a configurable threshold.","title":"Overview"},{"location":"filter/#environment-variables","text":"FISSEQ_PIPELINE_MIN_CLASS_MEMBERS Minimum number of samples required per (label, batch) group when running drop_infrequent_pairs . Default: 2 . Example: # Require at least 5 samples per label\u2013batch group FISSEQ_PIPELINE_MIN_CLASS_MEMBERS=5 fisseq-data-pipeline validate ...","title":"Environment variables"},{"location":"filter/#example-usage","text":"import polars as pl from fisseq_data_pipeline.filter import clean_data, drop_infrequent_pairs # Example feature matrix feature_df = pl.DataFrame({ \"f1\": [1.0, 2.0, float(\"nan\"), 4.0], \"f2\": [5.0, 6.0, 7.0, 8.0], }) # Example metadata with batch + label meta_df = pl.DataFrame({ \"_label\": [\"A\", \"A\", \"B\", \"B\"], \"_batch\": [\"X\", \"Y\", \"X\", \"Y\"], }) # Clean non-finite and zero-variance columns/rows feature_df, meta_df = clean_data(feature_df, meta_df) # Drop infrequent (label, batch) pairs feature_df, meta_df = drop_infrequent_pairs(feature_df, meta_df)","title":"Example Usage"},{"location":"filter/#api-reference","text":"","title":"API reference"},{"location":"filter/#fisseq_data_pipeline.filter.clean_data","text":"Clean feature and metadata tables and keep them row-aligned. The cleaning pipeline performs four passes: 1) Drop feature columns that are entirely non-finite. 2) Drop rows that contain any remaining non-finite value across. 3) Drop feature columns with (near) zero variance. All feature columns must be numeric Parameters: feature_df ( DataFrame ) \u2013 Feature-only table, shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata aligned row-wise with feature_df . Must contain \"_label\" and \"_batch\" columns. Returns: ( DataFrame , DataFrame ) \u2013 The cleaned (feature_df, meta_data_df) , with invalid rows/columns removed and row alignment preserved. Source code in src/fisseq_data_pipeline/filter.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def clean_data ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame ) -> Tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\" Clean feature and metadata tables and keep them row-aligned. The cleaning pipeline performs four passes: 1) Drop feature columns that are entirely non-finite. 2) Drop rows that contain any remaining non-finite value across. 3) Drop feature columns with (near) zero variance. All feature columns must be numeric Parameters ---------- feature_df : pl.DataFrame Feature-only table, shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata aligned row-wise with ``feature_df``. Must contain ``\"_label\"`` and ``\"_batch\"`` columns. Returns ------- (pl.DataFrame, pl.DataFrame) The cleaned ``(feature_df, meta_data_df)``, with invalid rows/columns removed and row alignment preserved. \"\"\" # Drop columns containing all non-finite values feature_df = feature_df . fill_null ( float ( \"nan\" )) n_rows = feature_df . height is_all_nonfinite = feature_df . select ( ~ pl . all () . is_finite () . any ()) all_nonfinite_cols = [ c for c in feature_df . columns if is_all_nonfinite . get_column ( c ) . item () ] feature_df = feature_df . select ( pl . exclude ( all_nonfinite_cols )) logging . info ( \"Removed %d columns containing only non-finite values\" , len ( all_nonfinite_cols ) ) # Drop rows containing remaining non-finite values row_mask = feature_df . select ( pl . all_horizontal ( pl . all () . is_finite ())) . to_series () feature_df = feature_df . filter ( row_mask ) meta_data_df = meta_data_df . filter ( row_mask ) logging . debug ( \"Dropped %d rows containing non-finite values\" , n_rows - feature_df . height ) # Drop rows that have 0 variance variances = feature_df . var () . row ( 0 , named = True ) zero_var_cols = [ c for c in feature_df . columns if float ( variances [ c ]) < np . finfo ( np . float32 ) . eps ] feature_df = feature_df . select ( pl . exclude ( zero_var_cols )) logging . info ( \"Removed %d columns containing zero variance\" , len ( zero_var_cols )) return feature_df , meta_data_df","title":"clean_data"},{"location":"filter/#fisseq_data_pipeline.filter.drop_infrequent_pairs","text":"Remove rows belonging to rare ( _label , _batch ) groups. Rows are grouped by the concatenation of _label and _batch . Any group with a sample count less than FISSEQ_PIPELINE_MIN_CLASS_MEMBERS (default: 2) is dropped. Parameters: feature_df ( DataFrame ) \u2013 Feature-only table of shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata table row-aligned with feature_df . Must include \"_label\" and \"_batch\" columns. Returns: ( DataFrame , DataFrame ) \u2013 A tuple (feature_df, meta_data_df) with rows from infrequent label\u2013batch groups removed. Alignment is preserved. Source code in src/fisseq_data_pipeline/filter.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def drop_infrequent_pairs ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame ) -> Tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\" Remove rows belonging to rare (``_label``, ``_batch``) groups. Rows are grouped by the concatenation of ``_label`` and ``_batch``. Any group with a sample count less than ``FISSEQ_PIPELINE_MIN_CLASS_MEMBERS`` (default: 2) is dropped. Parameters ---------- feature_df : pl.DataFrame Feature-only table of shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata table row-aligned with ``feature_df``. Must include ``\"_label\"`` and ``\"_batch\"`` columns. Returns ------- (pl.DataFrame, pl.DataFrame) A tuple ``(feature_df, meta_data_df)`` with rows from infrequent label\u2013batch groups removed. Alignment is preserved. \"\"\" # Drop rows that have infrequent (batch, label) batch pairs n_rows = feature_df . height label_batch = ( meta_data_df . get_column ( \"_label\" ) + \"_\" + meta_data_df . get_column ( \"_batch\" ) ) . alias ( \"_label_batch\" ) label_batch_counts = label_batch . value_counts () . filter ( pl . col ( \"count\" ) >= MINIMUM_CLASS_MEMBERS ) label_batch_freq_mask = label_batch . is_in ( label_batch_counts . get_column ( \"_label_batch\" ) ) feature_df = feature_df . filter ( label_batch_freq_mask ) meta_data_df = meta_data_df . filter ( label_batch_freq_mask ) logging . info ( \"Dropped %d rows containing batch label pairs with frequency less than %d \" , n_rows - feature_df . height , MINIMUM_CLASS_MEMBERS , ) return feature_df , meta_data_df","title":"drop_infrequent_pairs"},{"location":"harmonize/","text":"Harmonization utilities The fisseq_data_pipeline.harmonize module provides utilities for batch-effect correction using ComBat via the neuroHarmonize package. Harmonization is an essential step when combining data from multiple experiments or sources, ensuring that technical variation (batch effects) does not obscure biological signal. Overview fit_harmonizer : Learn a ComBat-based harmonization model from feature and metadata DataFrames. harmonize : Apply a fitted harmonization model to adjust new feature matrices for batch effects. Example usage import polars as pl from fisseq_data_pipeline.harmonize import fit_harmonizer, harmonize # Example feature matrix feature_df = pl.DataFrame({ \"gene1\": [1.0, 2.0, 3.0, 4.0], \"gene2\": [2.0, 3.0, 4.0, 5.0], }) # Example metadata with batch column meta_data_df = pl.DataFrame({ \"_batch\": [0, 0, 1, 1], \"_is_control\": [True, True, True, False], }) # Fit harmonizer (on control samples only) harmonizer = fit_harmonizer(feature_df, meta_data_df, fit_only_on_control=True) # Apply harmonization to full dataset harmonized_df = harmonize(feature_df, meta_data_df, harmonizer) print(harmonized_df) API Reference fisseq_data_pipeline . harmonize . fit_harmonizer ( feature_df , meta_data_df , fit_only_on_control = False ) Fit a ComBat-based harmonization model using neuroHarmonize . Parameters: feature_df ( DataFrame ) \u2013 Feature matrix with shape (n_samples, n_features), numeric only. meta_data_df ( DataFrame ) \u2013 Metadata aligned with rows in feature_df . Must contain a _batch column indicating batch membership. If fit_only_on_control=True , must also contain a boolean _is_control column. fit_only_on_control ( bool , default: False ) \u2013 If True, compute the harmonization model only from control samples (rows where _is_control is True). Returns: Harmonizer \u2013 A fitted harmonization model dictionary returned by neuroHarmonize.harmonizationLearn . Source code in src/fisseq_data_pipeline/harmonize.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def fit_harmonizer ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame , fit_only_on_control : bool = False , ) -> Harmonizer : \"\"\" Fit a ComBat-based harmonization model using `neuroHarmonize`. Parameters ---------- feature_df : pl.DataFrame Feature matrix with shape (n_samples, n_features), numeric only. meta_data_df : pl.DataFrame Metadata aligned with rows in `feature_df`. Must contain a `_batch` column indicating batch membership. If `fit_only_on_control=True`, must also contain a boolean `_is_control` column. fit_only_on_control : bool, default=False If True, compute the harmonization model only from control samples (rows where `_is_control` is True). Returns ------- Harmonizer A fitted harmonization model dictionary returned by ``neuroHarmonize.harmonizationLearn``. \"\"\" if fit_only_on_control : logging . info ( \"Filtering control samples, number of samples before filtering= %d \" , len ( feature_df ), ) feature_df = feature_df . filter ( meta_data_df . get_column ( \"_is_control\" )) meta_data_df = meta_data_df . filter ( meta_data_df . get_column ( \"_is_control\" )) logging . info ( \"Filtering complete, remaining train set samples shape= %s \" , len ( feature_df . shape ), ) logging . info ( \"Fitting harmonizer\" ) covar_df = meta_data_df . select ( pl . col ( \"_batch\" ) . alias ( \"SITE\" )) . to_pandas () model , _ = neuroHarmonize . harmonizationLearn ( feature_df . to_numpy (), covar_df ) logging . info ( \"Done\" ) return model fisseq_data_pipeline . harmonize . harmonize ( feature_df , meta_data_df , harmonizer ) Apply a fitted harmonization model to adjust features for batch effects. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix to harmonize; shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata aligned with rows in feature_df . Must contain a _batch column indicating batch membership. harmonizer ( Harmonizer ) \u2013 A fitted model dictionary produced by fit_harmonizer . Returns: DataFrame \u2013 Harmonized feature matrix with the same shape and column names as the input feature_df . Source code in src/fisseq_data_pipeline/harmonize.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def harmonize ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame , harmonizer : Harmonizer , ) -> pl . DataFrame : \"\"\" Apply a fitted harmonization model to adjust features for batch effects. Parameters ---------- feature_df : pl.DataFrame Feature matrix to harmonize; shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata aligned with rows in `feature_df`. Must contain a `_batch` column indicating batch membership. harmonizer : Harmonizer A fitted model dictionary produced by ``fit_harmonizer``. Returns ------- pl.DataFrame Harmonized feature matrix with the same shape and column names as the input `feature_df`. \"\"\" logging . info ( \"Setting up harmonization\" ) covar_df = meta_data_df . select ( pl . col ( \"_batch\" ) . alias ( \"SITE\" )) . to_pandas () logging . info ( \"Fitting harmonizer\" ) harmonized_matrix = neuroHarmonize . harmonizationApply ( feature_df . to_numpy (), covar_df , harmonizer ) logging . info ( \"Copying data\" ) feature_df = pl . DataFrame ( harmonized_matrix , schema = feature_df . columns ) logging . info ( \"Done\" ) return feature_df","title":"Harmonize"},{"location":"harmonize/#harmonization-utilities","text":"The fisseq_data_pipeline.harmonize module provides utilities for batch-effect correction using ComBat via the neuroHarmonize package. Harmonization is an essential step when combining data from multiple experiments or sources, ensuring that technical variation (batch effects) does not obscure biological signal.","title":"Harmonization utilities"},{"location":"harmonize/#overview","text":"fit_harmonizer : Learn a ComBat-based harmonization model from feature and metadata DataFrames. harmonize : Apply a fitted harmonization model to adjust new feature matrices for batch effects.","title":"Overview"},{"location":"harmonize/#example-usage","text":"import polars as pl from fisseq_data_pipeline.harmonize import fit_harmonizer, harmonize # Example feature matrix feature_df = pl.DataFrame({ \"gene1\": [1.0, 2.0, 3.0, 4.0], \"gene2\": [2.0, 3.0, 4.0, 5.0], }) # Example metadata with batch column meta_data_df = pl.DataFrame({ \"_batch\": [0, 0, 1, 1], \"_is_control\": [True, True, True, False], }) # Fit harmonizer (on control samples only) harmonizer = fit_harmonizer(feature_df, meta_data_df, fit_only_on_control=True) # Apply harmonization to full dataset harmonized_df = harmonize(feature_df, meta_data_df, harmonizer) print(harmonized_df)","title":"Example usage"},{"location":"harmonize/#api-reference","text":"","title":"API Reference"},{"location":"harmonize/#fisseq_data_pipeline.harmonize.fit_harmonizer","text":"Fit a ComBat-based harmonization model using neuroHarmonize . Parameters: feature_df ( DataFrame ) \u2013 Feature matrix with shape (n_samples, n_features), numeric only. meta_data_df ( DataFrame ) \u2013 Metadata aligned with rows in feature_df . Must contain a _batch column indicating batch membership. If fit_only_on_control=True , must also contain a boolean _is_control column. fit_only_on_control ( bool , default: False ) \u2013 If True, compute the harmonization model only from control samples (rows where _is_control is True). Returns: Harmonizer \u2013 A fitted harmonization model dictionary returned by neuroHarmonize.harmonizationLearn . Source code in src/fisseq_data_pipeline/harmonize.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def fit_harmonizer ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame , fit_only_on_control : bool = False , ) -> Harmonizer : \"\"\" Fit a ComBat-based harmonization model using `neuroHarmonize`. Parameters ---------- feature_df : pl.DataFrame Feature matrix with shape (n_samples, n_features), numeric only. meta_data_df : pl.DataFrame Metadata aligned with rows in `feature_df`. Must contain a `_batch` column indicating batch membership. If `fit_only_on_control=True`, must also contain a boolean `_is_control` column. fit_only_on_control : bool, default=False If True, compute the harmonization model only from control samples (rows where `_is_control` is True). Returns ------- Harmonizer A fitted harmonization model dictionary returned by ``neuroHarmonize.harmonizationLearn``. \"\"\" if fit_only_on_control : logging . info ( \"Filtering control samples, number of samples before filtering= %d \" , len ( feature_df ), ) feature_df = feature_df . filter ( meta_data_df . get_column ( \"_is_control\" )) meta_data_df = meta_data_df . filter ( meta_data_df . get_column ( \"_is_control\" )) logging . info ( \"Filtering complete, remaining train set samples shape= %s \" , len ( feature_df . shape ), ) logging . info ( \"Fitting harmonizer\" ) covar_df = meta_data_df . select ( pl . col ( \"_batch\" ) . alias ( \"SITE\" )) . to_pandas () model , _ = neuroHarmonize . harmonizationLearn ( feature_df . to_numpy (), covar_df ) logging . info ( \"Done\" ) return model","title":"fit_harmonizer"},{"location":"harmonize/#fisseq_data_pipeline.harmonize.harmonize","text":"Apply a fitted harmonization model to adjust features for batch effects. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix to harmonize; shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata aligned with rows in feature_df . Must contain a _batch column indicating batch membership. harmonizer ( Harmonizer ) \u2013 A fitted model dictionary produced by fit_harmonizer . Returns: DataFrame \u2013 Harmonized feature matrix with the same shape and column names as the input feature_df . Source code in src/fisseq_data_pipeline/harmonize.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def harmonize ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame , harmonizer : Harmonizer , ) -> pl . DataFrame : \"\"\" Apply a fitted harmonization model to adjust features for batch effects. Parameters ---------- feature_df : pl.DataFrame Feature matrix to harmonize; shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata aligned with rows in `feature_df`. Must contain a `_batch` column indicating batch membership. harmonizer : Harmonizer A fitted model dictionary produced by ``fit_harmonizer``. Returns ------- pl.DataFrame Harmonized feature matrix with the same shape and column names as the input `feature_df`. \"\"\" logging . info ( \"Setting up harmonization\" ) covar_df = meta_data_df . select ( pl . col ( \"_batch\" ) . alias ( \"SITE\" )) . to_pandas () logging . info ( \"Fitting harmonizer\" ) harmonized_matrix = neuroHarmonize . harmonizationApply ( feature_df . to_numpy (), covar_df , harmonizer ) logging . info ( \"Copying data\" ) feature_df = pl . DataFrame ( harmonized_matrix , schema = feature_df . columns ) logging . info ( \"Done\" ) return feature_df","title":"harmonize"},{"location":"normalize/","text":"Normalize The fisseq_data_pipeline.normalize module provides utilities for computing and applying z-score normalization to feature matrices. Normalization is typically run as part of the FISSEQ pipeline, but these functions can also be used independently when you need to standardize feature values. Overview Normalizer : A dataclass container storing per-column means and standard deviations. fit_normalizer : Compute normalization statistics (means and stds) from a feature DataFrame, optionally restricted to control samples. normalize : Apply z-score normalization to a feature DataFrame using a fitted Normalizer . Example usage import polars as pl from fisseq_data_pipeline.normalize import fit_normalizer, normalize # Example feature matrix feature_df = pl.DataFrame({ \"x\": [1.0, 2.0, 3.0], \"y\": [2.0, 4.0, 6.0], }) # Fit the normalizer normalizer = fit_normalizer(feature_df) # Apply normalization normalized_df = normalize(feature_df, normalizer) print(normalized_df) Output shape: (3, 2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 x \u2502 y \u2502 \u2502 --- \u2502 --- \u2502 \u2502 f64 \u2502 f64 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 -1.0 \u2502 -1.0 \u2502 \u2502 0.0 \u2502 0.0 \u2502 \u2502 1.0 \u2502 1.0 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 API reference fisseq_data_pipeline.normalize.Normalizer dataclass Container for normalization statistics. Attributes: means ( DataFrame ) \u2013 A 1xn DataFrame containing the mean value of each column. stds ( DataFrame ) \u2013 A 1xn DataFrame containing the standard deviation of each column. Source code in src/fisseq_data_pipeline/normalize.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @dataclasses . dataclass class Normalizer : \"\"\" Container for normalization statistics. Attributes ---------- means : pl.DataFrame A 1xn DataFrame containing the mean value of each column. stds : pl.DataFrame A 1xn DataFrame containing the standard deviation of each column. \"\"\" means : pl . DataFrame stds : pl . DataFrame fisseq_data_pipeline . normalize . fit_normalizer ( feature_df , meta_data_df = None , fit_only_on_control = False ) Compute column-wise means and standard deviations for feature normalization. Zero variance columns will be dropped from the normalizer to avoid a divide by zero error. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix with samples as rows and features as columns. meta_data_df ( Optional [ DataFrame ] , default: None ) \u2013 Metadata aligned row-wise with feature_df . Required if fit_only_on_control=True . fit_only_on_control ( bool , default: False ) \u2013 If True, compute normalization statistics only from control samples indicated by the _is_control column in meta_data_df . Returns: Normalizer \u2013 Object containing per-column means and standard deviations. Source code in src/fisseq_data_pipeline/normalize.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def fit_normalizer ( feature_df : pl . DataFrame , meta_data_df : Optional [ pl . DataFrame ] = None , fit_only_on_control : bool = False , ) -> Normalizer : \"\"\" Compute column-wise means and standard deviations for feature normalization. Zero variance columns will be dropped from the normalizer to avoid a divide by zero error. Parameters ---------- feature_df : pl.DataFrame Feature matrix with samples as rows and features as columns. meta_data_df : Optional[pl.DataFrame], default=None Metadata aligned row-wise with `feature_df`. Required if ``fit_only_on_control=True``. fit_only_on_control : bool, default=False If True, compute normalization statistics only from control samples indicated by the ``_is_control`` column in `meta_data_df`. Returns ------- Normalizer Object containing per-column means and standard deviations. \"\"\" if fit_only_on_control and meta_data_df is None : raise ValueError ( \"Meta data required to fit to control samples\" ) elif fit_only_on_control : logging . info ( \"Filtering control samples, number of samples before filtering= %d \" , len ( feature_df ), ) feature_df = feature_df . filter ( meta_data_df . get_column ( \"_is_control\" )) logging . info ( \"Filtering complete, remaining train set samples shape= %s \" , len ( feature_df . shape ), ) logging . info ( \"Fitting Normalizer\" ) means = feature_df . mean () stds = feature_df . std () zero_var_cols = [ k for k , v in stds . row ( 0 , named = True ) . items () if v < np . finfo ( np . float32 ) . eps ] if len ( zero_var_cols ) > 0 : logging . warning ( \"Dropping %d zero variance columns\" , len ( zero_var_cols )) means = means . select ( pl . exclude ( zero_var_cols )) stds = stds . select ( pl . exclude ( zero_var_cols )) normalizer = Normalizer ( means = means , stds = stds ) logging . info ( \"Done\" ) return normalizer fisseq_data_pipeline . normalize . normalize ( feature_df , normalizer ) Apply z-score normalization to features using precomputed statistics. Only columns included in the normalizer will be present in the output Parameters: feature_df ( DataFrame ) \u2013 Feature matrix to normalize; shape (n_samples, n_features). normalizer ( Normalizer ) \u2013 Precomputed means and standard deviations to use for scaling. Returns: DataFrame \u2013 Normalized feature matrix with the same shape as feature_df . Source code in src/fisseq_data_pipeline/normalize.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def normalize ( feature_df : pl . DataFrame , normalizer : Normalizer ) -> pl . DataFrame : \"\"\" Apply z-score normalization to features using precomputed statistics. Only columns included in the normalizer will be present in the output Parameters ---------- feature_df : pl.DataFrame Feature matrix to normalize; shape (n_samples, n_features). normalizer : Normalizer Precomputed means and standard deviations to use for scaling. Returns ------- pl.DataFrame Normalized feature matrix with the same shape as `feature_df`. \"\"\" logging . info ( \"Setting up normalization, data shape= %s \" , feature_df . shape ) means = normalizer . means . row ( 0 , named = True ) stds = normalizer . stds . row ( 0 , named = True ) n_cols = feature_df . width logging . info ( \"Running normalization\" ) feature_df = feature_df . select ( [(( pl . col ( c ) - means [ c ]) / stds [ c ]) . alias ( c ) for c in normalizer . stds . columns ] ) if feature_df . width < n_cols : logging . warning ( \"Dropped %d columns from feature_df that were not included in the\" \" normalizer\" , n_cols - feature_df . width , ) logging . info ( \"Done\" ) return feature_df","title":"Normalize"},{"location":"normalize/#normalize","text":"The fisseq_data_pipeline.normalize module provides utilities for computing and applying z-score normalization to feature matrices. Normalization is typically run as part of the FISSEQ pipeline, but these functions can also be used independently when you need to standardize feature values.","title":"Normalize"},{"location":"normalize/#overview","text":"Normalizer : A dataclass container storing per-column means and standard deviations. fit_normalizer : Compute normalization statistics (means and stds) from a feature DataFrame, optionally restricted to control samples. normalize : Apply z-score normalization to a feature DataFrame using a fitted Normalizer .","title":"Overview"},{"location":"normalize/#example-usage","text":"import polars as pl from fisseq_data_pipeline.normalize import fit_normalizer, normalize # Example feature matrix feature_df = pl.DataFrame({ \"x\": [1.0, 2.0, 3.0], \"y\": [2.0, 4.0, 6.0], }) # Fit the normalizer normalizer = fit_normalizer(feature_df) # Apply normalization normalized_df = normalize(feature_df, normalizer) print(normalized_df)","title":"Example usage"},{"location":"normalize/#output","text":"shape: (3, 2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 x \u2502 y \u2502 \u2502 --- \u2502 --- \u2502 \u2502 f64 \u2502 f64 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 -1.0 \u2502 -1.0 \u2502 \u2502 0.0 \u2502 0.0 \u2502 \u2502 1.0 \u2502 1.0 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Output"},{"location":"normalize/#api-reference","text":"","title":"API reference"},{"location":"normalize/#fisseq_data_pipeline.normalize.Normalizer","text":"Container for normalization statistics. Attributes: means ( DataFrame ) \u2013 A 1xn DataFrame containing the mean value of each column. stds ( DataFrame ) \u2013 A 1xn DataFrame containing the standard deviation of each column. Source code in src/fisseq_data_pipeline/normalize.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @dataclasses . dataclass class Normalizer : \"\"\" Container for normalization statistics. Attributes ---------- means : pl.DataFrame A 1xn DataFrame containing the mean value of each column. stds : pl.DataFrame A 1xn DataFrame containing the standard deviation of each column. \"\"\" means : pl . DataFrame stds : pl . DataFrame","title":"Normalizer"},{"location":"normalize/#fisseq_data_pipeline.normalize.fit_normalizer","text":"Compute column-wise means and standard deviations for feature normalization. Zero variance columns will be dropped from the normalizer to avoid a divide by zero error. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix with samples as rows and features as columns. meta_data_df ( Optional [ DataFrame ] , default: None ) \u2013 Metadata aligned row-wise with feature_df . Required if fit_only_on_control=True . fit_only_on_control ( bool , default: False ) \u2013 If True, compute normalization statistics only from control samples indicated by the _is_control column in meta_data_df . Returns: Normalizer \u2013 Object containing per-column means and standard deviations. Source code in src/fisseq_data_pipeline/normalize.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def fit_normalizer ( feature_df : pl . DataFrame , meta_data_df : Optional [ pl . DataFrame ] = None , fit_only_on_control : bool = False , ) -> Normalizer : \"\"\" Compute column-wise means and standard deviations for feature normalization. Zero variance columns will be dropped from the normalizer to avoid a divide by zero error. Parameters ---------- feature_df : pl.DataFrame Feature matrix with samples as rows and features as columns. meta_data_df : Optional[pl.DataFrame], default=None Metadata aligned row-wise with `feature_df`. Required if ``fit_only_on_control=True``. fit_only_on_control : bool, default=False If True, compute normalization statistics only from control samples indicated by the ``_is_control`` column in `meta_data_df`. Returns ------- Normalizer Object containing per-column means and standard deviations. \"\"\" if fit_only_on_control and meta_data_df is None : raise ValueError ( \"Meta data required to fit to control samples\" ) elif fit_only_on_control : logging . info ( \"Filtering control samples, number of samples before filtering= %d \" , len ( feature_df ), ) feature_df = feature_df . filter ( meta_data_df . get_column ( \"_is_control\" )) logging . info ( \"Filtering complete, remaining train set samples shape= %s \" , len ( feature_df . shape ), ) logging . info ( \"Fitting Normalizer\" ) means = feature_df . mean () stds = feature_df . std () zero_var_cols = [ k for k , v in stds . row ( 0 , named = True ) . items () if v < np . finfo ( np . float32 ) . eps ] if len ( zero_var_cols ) > 0 : logging . warning ( \"Dropping %d zero variance columns\" , len ( zero_var_cols )) means = means . select ( pl . exclude ( zero_var_cols )) stds = stds . select ( pl . exclude ( zero_var_cols )) normalizer = Normalizer ( means = means , stds = stds ) logging . info ( \"Done\" ) return normalizer","title":"fit_normalizer"},{"location":"normalize/#fisseq_data_pipeline.normalize.normalize","text":"Apply z-score normalization to features using precomputed statistics. Only columns included in the normalizer will be present in the output Parameters: feature_df ( DataFrame ) \u2013 Feature matrix to normalize; shape (n_samples, n_features). normalizer ( Normalizer ) \u2013 Precomputed means and standard deviations to use for scaling. Returns: DataFrame \u2013 Normalized feature matrix with the same shape as feature_df . Source code in src/fisseq_data_pipeline/normalize.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def normalize ( feature_df : pl . DataFrame , normalizer : Normalizer ) -> pl . DataFrame : \"\"\" Apply z-score normalization to features using precomputed statistics. Only columns included in the normalizer will be present in the output Parameters ---------- feature_df : pl.DataFrame Feature matrix to normalize; shape (n_samples, n_features). normalizer : Normalizer Precomputed means and standard deviations to use for scaling. Returns ------- pl.DataFrame Normalized feature matrix with the same shape as `feature_df`. \"\"\" logging . info ( \"Setting up normalization, data shape= %s \" , feature_df . shape ) means = normalizer . means . row ( 0 , named = True ) stds = normalizer . stds . row ( 0 , named = True ) n_cols = feature_df . width logging . info ( \"Running normalization\" ) feature_df = feature_df . select ( [(( pl . col ( c ) - means [ c ]) / stds [ c ]) . alias ( c ) for c in normalizer . stds . columns ] ) if feature_df . width < n_cols : logging . warning ( \"Dropped %d columns from feature_df that were not included in the\" \" normalizer\" , n_cols - feature_df . width , ) logging . info ( \"Done\" ) return feature_df","title":"normalize"},{"location":"pipeline/","text":"Pipeline The FISSEQ data pipeline exposes a small CLI via the entry point fisseq-data-pipeline . Subcommands are provided by Python Fire : validate \u2014 Train/validate on a stratified split and write outputs. run \u2014 Production, single-pass run (not yet implemented). configure \u2014 Write a default configuration file. Quick start # validate with explicit config and output directory fisseq-data-pipeline validate \\ --input_data_path data.parquet \\ --config config.yaml \\ --output_dir out \\ --test_size 0.2 \\ --write_train_results true Write a default config to the current directory fisseq-data-pipeline configure Logging FISSEQ_PIPELINE_LOG_LEVEL=debug fisseq-data-pipeline validate \\ --input_data_path data.parquet Command Interface Validate fisseq_data_pipeline . pipeline . validate ( input_data_path , config = None , output_dir = None , test_size = 0.2 , write_train_results = True ) Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps Load dataset, derive feature/metadata frames, and clean invalid rows/columns. Build a stratification vector from _batch and _label and perform a single stratified train/test split. Fit a normalizer on the training split; transform train and test. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters: input_data_path ( PathLike ) \u2013 Path to a Parquet file to scan and process. config ( Config or PathLike , default: None ) \u2013 Configuration object or path. Must define feature columns and the names of _batch , _label , and _is_control fields. output_dir ( PathLike , default: None ) \u2013 Output directory. Defaults to the current working directory. test_size ( float , default: 0.2 ) \u2013 Fraction of samples assigned to the test split. write_train_results ( bool , default: True ) \u2013 If True, also write the train split's unmodified/normalized/ harmonized outputs. Outputs Written to output_dir : meta_data.test.parquet features.test.parquet normalized.test.parquet harmonized.test.parquet normalizer.pkl harmonizer.pkl If write_train_results=True : meta_data.train.parquet features.train.parquet normalized.train.parquet harmonized.train.parquet CLI Exposed via Fire at the fisseq-data-pipeline entry point, e.g.:: fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true Source code in src/fisseq_data_pipeline/pipeline.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def validate ( input_data_path : PathLike , config : Optional [ Config | PathLike ] = None , output_dir : Optional [ PathLike ] = None , test_size : float = 0.2 , write_train_results : bool = True , ) -> None : \"\"\" Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps -------------- 1. Load dataset, derive feature/metadata frames, and clean invalid rows/columns. 2. Build a stratification vector from ``_batch`` and ``_label`` and perform a single stratified train/test split. 3. Fit a normalizer on the training split; transform train and test. 4. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). 5. Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters ---------- input_data_path : PathLike Path to a Parquet file to scan and process. config : Config or PathLike, optional Configuration object or path. Must define feature columns and the names of ``_batch``, ``_label``, and ``_is_control`` fields. output_dir : PathLike, optional Output directory. Defaults to the current working directory. test_size : float, default=0.2 Fraction of samples assigned to the test split. write_train_results : bool, default=True If True, also write the train split's unmodified/normalized/ harmonized outputs. Outputs ------- Written to ``output_dir``: - ``meta_data.test.parquet`` - ``features.test.parquet`` - ``normalized.test.parquet`` - ``harmonized.test.parquet`` - ``normalizer.pkl`` - ``harmonizer.pkl`` If ``write_train_results=True``: - ``meta_data.train.parquet`` - ``features.train.parquet`` - ``normalized.train.parquet`` - ``harmonized.train.parquet`` CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point, e.g.:: ```bash fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true ``` \"\"\" setup_logging ( output_dir ) logging . info ( \"Starting validation with input path: %s \" , input_data_path ) data_df = pl . scan_parquet ( input_data_path ) output_dir = pathlib . Path . cwd () if output_dir is None else pathlib . Path ( output_dir ) logging . info ( \"Output directory set to: %s \" , output_dir ) logging . info ( \"Collecting data matrices\" ) config = Config ( config ) feature_df , meta_data_df = get_data_dfs ( data_df , config ) feature_df , meta_data_df = clean_data ( feature_df , meta_data_df ) feature_df , meta_data_df = drop_infrequent_pairs ( feature_df , meta_data_df ) train_feature_df , train_meta_df , test_feature_df , test_meta_df = train_test_split ( feature_df , meta_data_df , test_size = test_size ) logging . info ( \"Fitting normalizer on train data\" ) normalizer = fit_normalizer ( train_feature_df , meta_data_df = train_meta_df , fit_only_on_control = True , ) logging . info ( \"Running normalizer on train/test data\" ) train_normalized_df = normalize ( train_feature_df , normalizer ) test_normalized_df = normalize ( test_feature_df , normalizer ) logging . info ( \"Fitting harmonizer on train data\" ) harmonizer = fit_harmonizer ( train_normalized_df , train_meta_df , fit_only_on_control = True ) logging . info ( \"Harmonizing test data\" ) test_harmonized_df = harmonize ( test_normalized_df , test_meta_df , harmonizer ) # write outputs logging . info ( \"Writing test outputs to %s \" , output_dir ) test_meta_df . write_parquet ( output_dir / \"meta_data.test.parquet\" ) test_feature_df . write_parquet ( output_dir / \"features.test.parquet\" ) test_normalized_df . write_parquet ( output_dir / \"normalized.test.parquet\" ) test_harmonized_df . write_parquet ( output_dir / \"harmonized.test.parquet\" ) logging . info ( \"Writing fitted parameters to %s \" , output_dir ) with open ( output_dir / f \"normalizer.pkl\" , \"wb\" ) as f : pickle . dump ( normalizer , f ) with open ( output_dir / f \"harmonizer.pkl\" , \"wb\" ) as f : pickle . dump ( harmonizer , f ) if write_train_results : logging . info ( \"Harmonizing train data\" ) train_harmonized_df = harmonize ( train_normalized_df , train_meta_df , harmonizer ) train_meta_df . write_parquet ( output_dir / \"meta_data.train.parquet\" ) train_feature_df . write_parquet ( output_dir / \"features.train.parquet\" ) train_normalized_df . write_parquet ( output_dir / \"normalized.train.parquet\" ) train_harmonized_df . write_parquet ( output_dir / \"harmonized.train.parquet\" ) Run fisseq_data_pipeline . pipeline . run ( * args , ** kwargs ) Run the production pipeline on a full dataset. This function is a placeholder for a single-pass production run (no train/test split). It is not implemented yet. Raises: NotImplementedError \u2013 Always raised. The function body is not implemented. CLI Registered subcommand (placeholder):: fisseq-data-pipeline run Source code in src/fisseq_data_pipeline/pipeline.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def run ( * args , ** kwargs ) -> None : \"\"\" Run the production pipeline on a full dataset. This function is a placeholder for a single-pass production run (no train/test split). It is not implemented yet. Raises ------ NotImplementedError Always raised. The function body is not implemented. CLI --- Registered subcommand (placeholder):: ```bash fisseq-data-pipeline run ``` \"\"\" # TODO: implement run raise NotImplementedError () options: show_signature: true show_signature_annotations: true show_source: true Configure fisseq_data_pipeline . pipeline . configure ( output_path = None ) Write a copy of the default configuration to output_path . Parameters: output_path ( PathLike , default: None ) \u2013 Target path for the configuration file. If None , writes config.yaml to the current working directory. Returns: None \u2013 CLI Exposed via Fire at the fisseq-data-pipeline entry point # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml Source code in src/fisseq_data_pipeline/pipeline.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def configure ( output_path : Optional [ PathLike ] = None ) -> None : \"\"\" Write a copy of the default configuration to ``output_path``. Parameters ---------- output_path : PathLike, optional Target path for the configuration file. If ``None``, writes ``config.yaml`` to the current working directory. Returns ------- None CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point ```bash # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml ``` \"\"\" if output_path is None : output_path = pathlib . Path . cwd () / \"config.yaml\" shutil . copy ( DEFAULT_CFG_PATH , output_path ) options: show_signature: true show_signature_annotations: true show_source: true Auxiliary functions This functions are not exposed to the command line, and are for internal use only. fisseq_data_pipeline . pipeline . setup_logging ( log_dir = None ) Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable FISSEQ_PIPELINE_LOG_LEVEL (default: \"info\" ). Parameters: log_dir ( PathLike , default: None ) \u2013 Directory where log files will be written. If None , the current working directory is used. Source code in src/fisseq_data_pipeline/pipeline.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def setup_logging ( log_dir : Optional [ PathLike ] = None ) -> None : \"\"\" Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable ``FISSEQ_PIPELINE_LOG_LEVEL`` (default: ``\"info\"``). Parameters ---------- log_dir : PathLike, optional Directory where log files will be written. If ``None``, the current working directory is used. \"\"\" log_levels = { \"debug\" : logging . DEBUG , \"info\" : logging . INFO , \"warning\" : logging . WARNING , \"error\" : logging . ERROR , \"critical\" : logging . CRITICAL , } if log_dir is None : log_dir = pathlib . Path . cwd () else : log_dir = pathlib . Path ( log_dir ) dt_str = datetime . datetime . now () . strftime ( \"%Y%m %d :%H%M%S\" ) filename = f \"fisseq-data-pipeline- { dt_str } .log\" log_path = log_dir / filename handlers = [ logging . StreamHandler (), logging . FileHandler ( log_path , mode = \"w\" )] log_level = os . getenv ( \"FISSEQ_PIPELINE_LOG_LEVEL\" , \"info\" ) log_level = log_levels . get ( log_level , logging . INFO ) logging . basicConfig ( level = log_level , format = \" %(asctime)s [ %(levelname)s ] [ %(funcName)s ] %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" , handlers = handlers , ) fisseq_data_pipeline . pipeline . main () CLI entry that registers Fire subcommands. Subcommands validate : Train/validate on a stratified split and write outputs. run : Production, single-pass run (not yet implemented). configure : Write a default configuration file. CLI Invoked as the fisseq-data-pipeline console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet Source code in src/fisseq_data_pipeline/pipeline.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def main () -> None : \"\"\" CLI entry that registers Fire subcommands. Subcommands ----------- - ``validate`` : Train/validate on a stratified split and write outputs. - ``run`` : Production, single-pass run (not yet implemented). - ``configure`` : Write a default configuration file. CLI --- Invoked as the ``fisseq-data-pipeline`` console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet \"\"\" try : fire . Fire ({ \"validate\" : validate , \"run\" : run , \"configure\" : configure }) except : logging . exception ( \"Run failed due to the following exception:\" ) raise","title":"Pipeline"},{"location":"pipeline/#pipeline","text":"The FISSEQ data pipeline exposes a small CLI via the entry point fisseq-data-pipeline . Subcommands are provided by Python Fire : validate \u2014 Train/validate on a stratified split and write outputs. run \u2014 Production, single-pass run (not yet implemented). configure \u2014 Write a default configuration file.","title":"Pipeline"},{"location":"pipeline/#quick-start","text":"# validate with explicit config and output directory fisseq-data-pipeline validate \\ --input_data_path data.parquet \\ --config config.yaml \\ --output_dir out \\ --test_size 0.2 \\ --write_train_results true","title":"Quick start"},{"location":"pipeline/#write-a-default-config-to-the-current-directory","text":"fisseq-data-pipeline configure","title":"Write a default config to the current directory"},{"location":"pipeline/#logging","text":"FISSEQ_PIPELINE_LOG_LEVEL=debug fisseq-data-pipeline validate \\ --input_data_path data.parquet","title":"Logging"},{"location":"pipeline/#command-interface","text":"","title":"Command Interface"},{"location":"pipeline/#validate","text":"","title":"Validate"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.validate","text":"Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps Load dataset, derive feature/metadata frames, and clean invalid rows/columns. Build a stratification vector from _batch and _label and perform a single stratified train/test split. Fit a normalizer on the training split; transform train and test. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters: input_data_path ( PathLike ) \u2013 Path to a Parquet file to scan and process. config ( Config or PathLike , default: None ) \u2013 Configuration object or path. Must define feature columns and the names of _batch , _label , and _is_control fields. output_dir ( PathLike , default: None ) \u2013 Output directory. Defaults to the current working directory. test_size ( float , default: 0.2 ) \u2013 Fraction of samples assigned to the test split. write_train_results ( bool , default: True ) \u2013 If True, also write the train split's unmodified/normalized/ harmonized outputs. Outputs Written to output_dir : meta_data.test.parquet features.test.parquet normalized.test.parquet harmonized.test.parquet normalizer.pkl harmonizer.pkl If write_train_results=True : meta_data.train.parquet features.train.parquet normalized.train.parquet harmonized.train.parquet CLI Exposed via Fire at the fisseq-data-pipeline entry point, e.g.:: fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true Source code in src/fisseq_data_pipeline/pipeline.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def validate ( input_data_path : PathLike , config : Optional [ Config | PathLike ] = None , output_dir : Optional [ PathLike ] = None , test_size : float = 0.2 , write_train_results : bool = True , ) -> None : \"\"\" Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps -------------- 1. Load dataset, derive feature/metadata frames, and clean invalid rows/columns. 2. Build a stratification vector from ``_batch`` and ``_label`` and perform a single stratified train/test split. 3. Fit a normalizer on the training split; transform train and test. 4. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). 5. Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters ---------- input_data_path : PathLike Path to a Parquet file to scan and process. config : Config or PathLike, optional Configuration object or path. Must define feature columns and the names of ``_batch``, ``_label``, and ``_is_control`` fields. output_dir : PathLike, optional Output directory. Defaults to the current working directory. test_size : float, default=0.2 Fraction of samples assigned to the test split. write_train_results : bool, default=True If True, also write the train split's unmodified/normalized/ harmonized outputs. Outputs ------- Written to ``output_dir``: - ``meta_data.test.parquet`` - ``features.test.parquet`` - ``normalized.test.parquet`` - ``harmonized.test.parquet`` - ``normalizer.pkl`` - ``harmonizer.pkl`` If ``write_train_results=True``: - ``meta_data.train.parquet`` - ``features.train.parquet`` - ``normalized.train.parquet`` - ``harmonized.train.parquet`` CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point, e.g.:: ```bash fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true ``` \"\"\" setup_logging ( output_dir ) logging . info ( \"Starting validation with input path: %s \" , input_data_path ) data_df = pl . scan_parquet ( input_data_path ) output_dir = pathlib . Path . cwd () if output_dir is None else pathlib . Path ( output_dir ) logging . info ( \"Output directory set to: %s \" , output_dir ) logging . info ( \"Collecting data matrices\" ) config = Config ( config ) feature_df , meta_data_df = get_data_dfs ( data_df , config ) feature_df , meta_data_df = clean_data ( feature_df , meta_data_df ) feature_df , meta_data_df = drop_infrequent_pairs ( feature_df , meta_data_df ) train_feature_df , train_meta_df , test_feature_df , test_meta_df = train_test_split ( feature_df , meta_data_df , test_size = test_size ) logging . info ( \"Fitting normalizer on train data\" ) normalizer = fit_normalizer ( train_feature_df , meta_data_df = train_meta_df , fit_only_on_control = True , ) logging . info ( \"Running normalizer on train/test data\" ) train_normalized_df = normalize ( train_feature_df , normalizer ) test_normalized_df = normalize ( test_feature_df , normalizer ) logging . info ( \"Fitting harmonizer on train data\" ) harmonizer = fit_harmonizer ( train_normalized_df , train_meta_df , fit_only_on_control = True ) logging . info ( \"Harmonizing test data\" ) test_harmonized_df = harmonize ( test_normalized_df , test_meta_df , harmonizer ) # write outputs logging . info ( \"Writing test outputs to %s \" , output_dir ) test_meta_df . write_parquet ( output_dir / \"meta_data.test.parquet\" ) test_feature_df . write_parquet ( output_dir / \"features.test.parquet\" ) test_normalized_df . write_parquet ( output_dir / \"normalized.test.parquet\" ) test_harmonized_df . write_parquet ( output_dir / \"harmonized.test.parquet\" ) logging . info ( \"Writing fitted parameters to %s \" , output_dir ) with open ( output_dir / f \"normalizer.pkl\" , \"wb\" ) as f : pickle . dump ( normalizer , f ) with open ( output_dir / f \"harmonizer.pkl\" , \"wb\" ) as f : pickle . dump ( harmonizer , f ) if write_train_results : logging . info ( \"Harmonizing train data\" ) train_harmonized_df = harmonize ( train_normalized_df , train_meta_df , harmonizer ) train_meta_df . write_parquet ( output_dir / \"meta_data.train.parquet\" ) train_feature_df . write_parquet ( output_dir / \"features.train.parquet\" ) train_normalized_df . write_parquet ( output_dir / \"normalized.train.parquet\" ) train_harmonized_df . write_parquet ( output_dir / \"harmonized.train.parquet\" )","title":"validate"},{"location":"pipeline/#run","text":"","title":"Run"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.run","text":"Run the production pipeline on a full dataset. This function is a placeholder for a single-pass production run (no train/test split). It is not implemented yet. Raises: NotImplementedError \u2013 Always raised. The function body is not implemented. CLI Registered subcommand (placeholder):: fisseq-data-pipeline run Source code in src/fisseq_data_pipeline/pipeline.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def run ( * args , ** kwargs ) -> None : \"\"\" Run the production pipeline on a full dataset. This function is a placeholder for a single-pass production run (no train/test split). It is not implemented yet. Raises ------ NotImplementedError Always raised. The function body is not implemented. CLI --- Registered subcommand (placeholder):: ```bash fisseq-data-pipeline run ``` \"\"\" # TODO: implement run raise NotImplementedError () options: show_signature: true show_signature_annotations: true show_source: true","title":"run"},{"location":"pipeline/#configure","text":"","title":"Configure"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.configure","text":"Write a copy of the default configuration to output_path . Parameters: output_path ( PathLike , default: None ) \u2013 Target path for the configuration file. If None , writes config.yaml to the current working directory. Returns: None \u2013 CLI Exposed via Fire at the fisseq-data-pipeline entry point # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml Source code in src/fisseq_data_pipeline/pipeline.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def configure ( output_path : Optional [ PathLike ] = None ) -> None : \"\"\" Write a copy of the default configuration to ``output_path``. Parameters ---------- output_path : PathLike, optional Target path for the configuration file. If ``None``, writes ``config.yaml`` to the current working directory. Returns ------- None CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point ```bash # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml ``` \"\"\" if output_path is None : output_path = pathlib . Path . cwd () / \"config.yaml\" shutil . copy ( DEFAULT_CFG_PATH , output_path ) options: show_signature: true show_signature_annotations: true show_source: true","title":"configure"},{"location":"pipeline/#auxiliary-functions","text":"This functions are not exposed to the command line, and are for internal use only.","title":"Auxiliary functions"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.setup_logging","text":"Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable FISSEQ_PIPELINE_LOG_LEVEL (default: \"info\" ). Parameters: log_dir ( PathLike , default: None ) \u2013 Directory where log files will be written. If None , the current working directory is used. Source code in src/fisseq_data_pipeline/pipeline.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def setup_logging ( log_dir : Optional [ PathLike ] = None ) -> None : \"\"\" Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable ``FISSEQ_PIPELINE_LOG_LEVEL`` (default: ``\"info\"``). Parameters ---------- log_dir : PathLike, optional Directory where log files will be written. If ``None``, the current working directory is used. \"\"\" log_levels = { \"debug\" : logging . DEBUG , \"info\" : logging . INFO , \"warning\" : logging . WARNING , \"error\" : logging . ERROR , \"critical\" : logging . CRITICAL , } if log_dir is None : log_dir = pathlib . Path . cwd () else : log_dir = pathlib . Path ( log_dir ) dt_str = datetime . datetime . now () . strftime ( \"%Y%m %d :%H%M%S\" ) filename = f \"fisseq-data-pipeline- { dt_str } .log\" log_path = log_dir / filename handlers = [ logging . StreamHandler (), logging . FileHandler ( log_path , mode = \"w\" )] log_level = os . getenv ( \"FISSEQ_PIPELINE_LOG_LEVEL\" , \"info\" ) log_level = log_levels . get ( log_level , logging . INFO ) logging . basicConfig ( level = log_level , format = \" %(asctime)s [ %(levelname)s ] [ %(funcName)s ] %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" , handlers = handlers , )","title":"setup_logging"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.main","text":"CLI entry that registers Fire subcommands. Subcommands validate : Train/validate on a stratified split and write outputs. run : Production, single-pass run (not yet implemented). configure : Write a default configuration file. CLI Invoked as the fisseq-data-pipeline console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet Source code in src/fisseq_data_pipeline/pipeline.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def main () -> None : \"\"\" CLI entry that registers Fire subcommands. Subcommands ----------- - ``validate`` : Train/validate on a stratified split and write outputs. - ``run`` : Production, single-pass run (not yet implemented). - ``configure`` : Write a default configuration file. CLI --- Invoked as the ``fisseq-data-pipeline`` console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet \"\"\" try : fire . Fire ({ \"validate\" : validate , \"run\" : run , \"configure\" : configure }) except : logging . exception ( \"Run failed due to the following exception:\" ) raise","title":"main"},{"location":"utils/config/","text":"Configuration utilities The fisseq_data_pipeline.utils.config module provides a Config object for managing pipeline configuration. It allows loading configuration values from YAML files, Python dictionaries, or other Config objects, and ensures that all values are validated against a default configuration. Overview Config : A wrapper around a validated configuration dictionary. Loads from a path, dictionary, Config , or falls back to the default config.yaml . Allows access via both attribute-style ( cfg.feature_cols ) and dictionary-style ( cfg[\"feature_cols\"] ). Automatically fills in missing keys from the default configuration and removes invalid keys. DEFAULT_CFG_PATH : The path to the default configuration YAML file that ships with the pipeline. Example usage from fisseq_data_pipeline.utils.config import Config # Load default configuration cfg = Config(None) # Load from a YAML file cfg = Config(\"my_config.yaml\") # Load from a Python dict cfg = Config({\"feature_cols\": [\"f1\", \"f2\"], \"_batch\": \"batch\"}) # Load from an existing Config cfg2 = Config(cfg) # Access values print(cfg.feature_cols) print(cfg[\"_batch\"]) Validation Behavior When initializing a Config : Invalid keys not present in the default configuration are removed with a warning. Missing keys are filled with the default values from config.yaml. This ensures that the configuration is always complete and consistent with the pipeline defaults. API Reference fisseq_data_pipeline.utils.config.Config A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another Config instance. If no configuration is provided, the default configuration file is used. Parameters: config ( PathLike or dict or Config ) \u2013 If None , the default configuration file path is used. If a dict , the dictionary is validated and used directly. If a PathLike , the configuration is loaded from the YAML file. If a Config , the underlying configuration data is reused. Source code in src/fisseq_data_pipeline/utils/config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Config : \"\"\" A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another ``Config`` instance. If no configuration is provided, the default configuration file is used. Parameters ---------- config : PathLike or dict or Config, optional - If ``None``, the default configuration file path is used. - If a ``dict``, the dictionary is validated and used directly. - If a ``PathLike``, the configuration is loaded from the YAML file. - If a ``Config``, the underlying configuration data is reused. \"\"\" def __init__ ( self , config : Optional [ PathLike | ConfigDict | \"Config\" ]): if config is None : logging . info ( \"No config provided, using default config\" ) config = DEFAULT_CFG_PATH if isinstance ( config , Config ): data = config . _data else : if isinstance ( config , dict ): data = config else : config = pathlib . Path ( config ) with config . open ( \"r\" ) as f : data = yaml . safe_load ( f ) data = self . _verify_config ( data ) logging . debug ( \"Using config %s \" , config ) self . _data = data def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ] def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key ) def _verify_config ( self , cfg_data : ConfigDict ) -> ConfigDict : \"\"\" Verify the provided configuration against the default configuration. Invalid keys are removed, and missing keys are filled with defaults. Parameters ---------- cfg_data : dict The configuration data to verify. Returns ------- dict The validated configuration dictionary with defaults applied. \"\"\" with DEFAULT_CFG_PATH . open ( \"r\" ) as f : default_data = yaml . safe_load ( f ) for key in list ( cfg_data . keys ()): if key in default_data : continue logging . warning ( \"Removing invalid config option %s from provided config\" , key ) del cfg_data [ key ] for key in list ( default_data . keys ()): if key in cfg_data : continue logging . warning ( \"Key %s not in provided config using default value of %s \" , key , default_data [ key ], ) cfg_data [ key ] = default_data [ key ] return cfg_data __getattr__ ( name ) Retrieve a configuration value as an attribute. Source code in src/fisseq_data_pipeline/utils/config.py 47 48 49 def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ] __getitem__ ( key ) Retrieve a configuration value using dictionary-style indexing. Source code in src/fisseq_data_pipeline/utils/config.py 51 52 53 def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key ) fisseq_data_pipeline . utils . config . DEFAULT_CFG_PATH = pathlib . Path ( __file__ ) . parent . parent / 'config.yaml' module-attribute","title":"Config"},{"location":"utils/config/#configuration-utilities","text":"The fisseq_data_pipeline.utils.config module provides a Config object for managing pipeline configuration. It allows loading configuration values from YAML files, Python dictionaries, or other Config objects, and ensures that all values are validated against a default configuration.","title":"Configuration utilities"},{"location":"utils/config/#overview","text":"Config : A wrapper around a validated configuration dictionary. Loads from a path, dictionary, Config , or falls back to the default config.yaml . Allows access via both attribute-style ( cfg.feature_cols ) and dictionary-style ( cfg[\"feature_cols\"] ). Automatically fills in missing keys from the default configuration and removes invalid keys. DEFAULT_CFG_PATH : The path to the default configuration YAML file that ships with the pipeline.","title":"Overview"},{"location":"utils/config/#example-usage","text":"from fisseq_data_pipeline.utils.config import Config # Load default configuration cfg = Config(None) # Load from a YAML file cfg = Config(\"my_config.yaml\") # Load from a Python dict cfg = Config({\"feature_cols\": [\"f1\", \"f2\"], \"_batch\": \"batch\"}) # Load from an existing Config cfg2 = Config(cfg) # Access values print(cfg.feature_cols) print(cfg[\"_batch\"])","title":"Example usage"},{"location":"utils/config/#validation-behavior","text":"When initializing a Config : Invalid keys not present in the default configuration are removed with a warning. Missing keys are filled with the default values from config.yaml. This ensures that the configuration is always complete and consistent with the pipeline defaults.","title":"Validation Behavior"},{"location":"utils/config/#api-reference","text":"","title":"API Reference"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.Config","text":"A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another Config instance. If no configuration is provided, the default configuration file is used. Parameters: config ( PathLike or dict or Config ) \u2013 If None , the default configuration file path is used. If a dict , the dictionary is validated and used directly. If a PathLike , the configuration is loaded from the YAML file. If a Config , the underlying configuration data is reused. Source code in src/fisseq_data_pipeline/utils/config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Config : \"\"\" A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another ``Config`` instance. If no configuration is provided, the default configuration file is used. Parameters ---------- config : PathLike or dict or Config, optional - If ``None``, the default configuration file path is used. - If a ``dict``, the dictionary is validated and used directly. - If a ``PathLike``, the configuration is loaded from the YAML file. - If a ``Config``, the underlying configuration data is reused. \"\"\" def __init__ ( self , config : Optional [ PathLike | ConfigDict | \"Config\" ]): if config is None : logging . info ( \"No config provided, using default config\" ) config = DEFAULT_CFG_PATH if isinstance ( config , Config ): data = config . _data else : if isinstance ( config , dict ): data = config else : config = pathlib . Path ( config ) with config . open ( \"r\" ) as f : data = yaml . safe_load ( f ) data = self . _verify_config ( data ) logging . debug ( \"Using config %s \" , config ) self . _data = data def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ] def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key ) def _verify_config ( self , cfg_data : ConfigDict ) -> ConfigDict : \"\"\" Verify the provided configuration against the default configuration. Invalid keys are removed, and missing keys are filled with defaults. Parameters ---------- cfg_data : dict The configuration data to verify. Returns ------- dict The validated configuration dictionary with defaults applied. \"\"\" with DEFAULT_CFG_PATH . open ( \"r\" ) as f : default_data = yaml . safe_load ( f ) for key in list ( cfg_data . keys ()): if key in default_data : continue logging . warning ( \"Removing invalid config option %s from provided config\" , key ) del cfg_data [ key ] for key in list ( default_data . keys ()): if key in cfg_data : continue logging . warning ( \"Key %s not in provided config using default value of %s \" , key , default_data [ key ], ) cfg_data [ key ] = default_data [ key ] return cfg_data","title":"Config"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.Config.__getattr__","text":"Retrieve a configuration value as an attribute. Source code in src/fisseq_data_pipeline/utils/config.py 47 48 49 def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ]","title":"__getattr__"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.Config.__getitem__","text":"Retrieve a configuration value using dictionary-style indexing. Source code in src/fisseq_data_pipeline/utils/config.py 51 52 53 def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key )","title":"__getitem__"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.DEFAULT_CFG_PATH","text":"","title":"DEFAULT_CFG_PATH"},{"location":"utils/utils/","text":"Utility functions The fisseq_data_pipeline.utils.utils module provides helper functions for feature selection, dataset construction, and splitting. These utilities are used internally by the pipeline but can also be reused in standalone scripts. Overview get_feature_selector : Build a Polars selector expression for feature columns based on the pipeline configuration. get_feature_columns : Select feature columns from a LazyFrame using a config. get_data_dfs : Build aligned feature and metadata DataFrames from a Polars LazyFrame . train_test_split : Create stratified train/test splits for features and metadata. Environment variables FISSEQ_PIPELINE_RAND_STATE Random seed used for reproducible stratified train/test splits. Default: 42 . Example # Use a fixed seed of 1234 for train/test splitting FISSEQ_PIPELINE_RAND_STATE=1234 fisseq-data-pipeline validate ... Example Usage import polars as pl from fisseq_data_pipeline.utils.config import Config from fisseq_data_pipeline.utils.utils import ( get_data_dfs, train_test_split ) # Example dataset df = pl.DataFrame({ \"gene1\": [1.0, 2.0, 3.0, 4.0], \"gene2\": [5.0, 6.0, 7.0, 8.0], \"batch\": [\"A\", \"A\", \"B\", \"B\"], \"label\": [\"X\", \"X\", \"Y\", \"Y\"], \"is_ctrl\": [True, False, True, False], }).lazy() # Example config (using dict for simplicity) cfg = Config({ \"feature_cols\": [\"gene1\", \"gene2\"], \"batch_col_name\": \"batch\", \"label_col_name\": \"label\", \"control_sample_query\": \"col('is_ctrl')\", }) # Build feature + metadata DataFrames feature_df, meta_data_df = get_data_dfs(df, cfg) # Stratified train/test split train_f, train_m, test_f, test_m = train_test_split(feature_df, meta_data_df, test_size=0.5) API Reference fisseq_data_pipeline . utils . utils . get_feature_selector ( data_df , config ) Get feature column feature selector based on config Parameters: data_df ( LazyFrame ) \u2013 The input data as a Polars LazyFrame. config ( Config ) \u2013 Configuration object containing a feature_cols attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns: PlSelector \u2013 A selector that can be used to select feature columns in .select call. Source code in src/fisseq_data_pipeline/utils/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def get_feature_selector ( data_df : pl . LazyFrame , config : Config ) -> PlSelector : \"\"\" Get feature column feature selector based on config Parameters ---------- data_df : pl.LazyFrame The input data as a Polars LazyFrame. config : Config Configuration object containing a ``feature_cols`` attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns ------- PlSelector A selector that can be used to select feature columns in ``.select`` call. \"\"\" if isinstance ( config . feature_cols , str ): selector_type = \"regex\" selector = cs . matches ( config . feature_cols ) else : selector_type = \"list\" feature_cols = set ( config . feature_cols ) missing = feature_cols - set ( data_df . columns ) if len ( missing ) != 0 : logging . warning ( \"Some columns are specified in the config but are not currently\" \" present in the dataframe, this can happen if columns are\" \" removed during data cleaning. The following columns will be\" \" ignored: %s \" , missing , ) # Column order must be preserved not_missing = feature_cols - missing selector = pl . col ( col for col in list ( config . feature_cols ) if col in not_missing ) logging . debug ( \"Using feature %s selector: %s \" , selector_type , config . feature_cols ) return selector fisseq_data_pipeline . utils . utils . get_feature_columns ( data_df , config ) Select feature columns from a Polars LazyFrame based on the configuration Parameters: data_df ( LazyFrame ) \u2013 The input data as a Polars LazyFrame. config ( Config ) \u2013 Configuration object containing a feature_cols attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns: LazyFrame \u2013 A Polars LazyFrame containing only the selected feature columns, cast to the given dtype. Source code in src/fisseq_data_pipeline/utils/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_feature_columns ( data_df : pl . LazyFrame , config : Config ) -> pl . LazyFrame : \"\"\" Select feature columns from a Polars LazyFrame based on the configuration Parameters ---------- data_df : pl.LazyFrame The input data as a Polars LazyFrame. config : Config Configuration object containing a ``feature_cols`` attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns ------- pl.LazyFrame A Polars LazyFrame containing only the selected feature columns, cast to the given dtype. \"\"\" selector = get_feature_selector ( data_df , config ) data_df = data_df . select ( selector ) return data_df fisseq_data_pipeline . utils . utils . get_data_dfs ( data_df , config , dtype = pl . Float32 ) Construct a numeric feature matrix and a metadata DataFrame from a Polars LazyFrame. The computation graph: 1. Adds a row index column ( _sample_idx ) for stable sample tracking. 2. Selects feature columns defined in config.feature_cols and casts them to dtype . 3. Extracts the batch column ( _batch ), the label column ( _label ), and a boolean control mask ( _is_control ) using config.control_sample_query . 4. Collects the LazyFrame once and returns NumPy features plus eager Polars metadata. Parameters: data_df ( LazyFrame ) \u2013 Input dataset as a Polars LazyFrame. config ( Config ) \u2013 Configuration object containing: - feature_cols : regex or list of feature column names. - batch_col_name : column name to be exposed as _batch . - label_col_name : column name to be exposed as _label . - control_sample_query : SQL WHERE clause fragment defining control samples (used to produce _is_control ). dtype ( DataType , default: Float32 ) \u2013 Target dtype for feature columns (default: pl.Float32 ). Returns: Tuple [ ndarray , DataFrame ] \u2013 Feature matrix: np.ndarray of shape (n_samples, n_features) , with columns from config.feature_cols cast to dtype . Metadata DataFrame: eager pl.DataFrame with columns: _batch : values from config.batch_col_name . _label : values from config.label_col_name . _is_control : bool mask indicating control samples. _sample_idx : integer row index for reference. Source code in src/fisseq_data_pipeline/utils/utils.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def get_data_dfs ( data_df : pl . LazyFrame , config : Config , dtype : pl . DataType = pl . Float32 , ) -> Tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\" Construct a numeric feature matrix and a metadata DataFrame from a Polars LazyFrame. The computation graph: 1. Adds a row index column (``_sample_idx``) for stable sample tracking. 2. Selects feature columns defined in ``config.feature_cols`` and casts them to ``dtype``. 3. Extracts the batch column (``_batch``), the label column (``_label``), and a boolean control mask (``_is_control``) using ``config.control_sample_query``. 4. Collects the LazyFrame once and returns NumPy features plus eager Polars metadata. Parameters ---------- data_df : pl.LazyFrame Input dataset as a Polars LazyFrame. config : Config Configuration object containing: - ``feature_cols``: regex or list of feature column names. - ``batch_col_name``: column name to be exposed as ``_batch``. - ``label_col_name``: column name to be exposed as ``_label``. - ``control_sample_query``: SQL WHERE clause fragment defining control samples (used to produce ``_is_control``). dtype : pl.DataType, optional Target dtype for feature columns (default: ``pl.Float32``). Returns ------- Tuple[np.ndarray, pl.DataFrame] - Feature matrix: ``np.ndarray`` of shape ``(n_samples, n_features)``, with columns from ``config.feature_cols`` cast to ``dtype``. - Metadata DataFrame: eager ``pl.DataFrame`` with columns: * ``_batch``: values from ``config.batch_col_name``. * ``_label``: values from ``config.label_col_name``. * ``_is_control``: bool mask indicating control samples. * ``_sample_idx``: integer row index for reference. \"\"\" logging . info ( \"Starting get_data_matrices for batch_col= %s , dtype= %s \" , config . batch_col_name , dtype , ) # Attach row indices to preserve mapping later base = data_df . with_row_index ( name = \"_sample_idx\" ) . cache () # Build feature selector feature_expr = get_feature_selector ( base , config ) . cast ( dtype = dtype ) logging . debug ( \"Feature selector resolved: %s \" , feature_expr ) # Control mask expr logging . debug ( \"Parsing control sample query: %s \" , config . control_sample_query ) control_mask_expr = pl . sql_expr ( config . control_sample_query ) . alias ( \"_is_control\" ) batch_expr = pl . col ( config . batch_col_name ) . alias ( \"_batch\" ) label_expr = pl . col ( config . label_col_name ) . alias ( \"_label\" ) # Execute the full plan logging . info ( \"Collecting LazyFrame into DataFrame\" ) df = ( base . with_columns ( label_expr , batch_expr , control_mask_expr , ) . select ( feature_expr , pl . col ( \"_batch\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ), pl . col ( \"_label\" ), ) . collect () ) logging . info ( \"Collection complete: shape= %s \" , df . shape ) # Get feature dataframe feature_df = df . select ( feature_expr ) logging . debug ( \"Feature dataframe shape: %s \" , feature_df . shape ) meta_data_df = df . select ( pl . col ( \"_batch\" ), pl . col ( \"_label\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ) ) logging . debug ( \"Meta data dataframe: shape= %s \" , meta_data_df . shape ) logging . info ( \"Finished get_data_matrices\" ) return feature_df , meta_data_df fisseq_data_pipeline . utils . utils . train_test_split ( feature_df , meta_data_df , test_size ) Split feature and metadata DataFrames into stratified train and test sets. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix with shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata aligned row-wise with feature_df . Must contain columns _label (class labels) and _batch (batch identifiers). test_size ( float ) \u2013 Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns: train_feature_df ( DataFrame ) \u2013 Features for the training set. train_meta_data_df ( DataFrame ) \u2013 Metadata for the training set. test_feature_df ( DataFrame ) \u2013 Features for the test set. test_meta_data_df ( DataFrame ) \u2013 Metadata for the test set. Notes Each (_label, _batch) group must have at least two samples for stratification to succeed. The split is reproducible if RANDOM_STATE is fixed. Source code in src/fisseq_data_pipeline/utils/utils.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def train_test_split ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame , test_size : float , ) -> Tuple [ pl . DataFrame , pl . DataFrame , pl . DataFrame , pl . DataFrame ]: \"\"\" Split feature and metadata DataFrames into stratified train and test sets. Parameters ---------- feature_df : pl.DataFrame Feature matrix with shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata aligned row-wise with ``feature_df``. Must contain columns ``_label`` (class labels) and ``_batch`` (batch identifiers). test_size : float Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns ------- train_feature_df : pl.DataFrame Features for the training set. train_meta_data_df : pl.DataFrame Metadata for the training set. test_feature_df : pl.DataFrame Features for the test set. test_meta_data_df : pl.DataFrame Metadata for the test set. Notes ----- - Each ``(_label, _batch)`` group must have at least two samples for stratification to succeed. - The split is reproducible if ``RANDOM_STATE`` is fixed. \"\"\" logging . info ( \"Creating train test split\" ) stratify = [ f \" { label } : { batch } \" for label , batch in zip ( meta_data_df . get_column ( \"_label\" ), meta_data_df . get_column ( \"_batch\" ) ) ] train_idx , test_idx = sklearn . model_selection . train_test_split ( np . arange ( len ( meta_data_df )), test_size = test_size , stratify = stratify , random_state = RANDOM_STATE , ) logging . info ( \"Created splits, copying data\" ) train_feature_df = feature_df [ train_idx , :] train_meta_data_df = meta_data_df [ train_idx , :] test_feature_df = feature_df [ test_idx , :] test_meta_data_df = meta_data_df [ test_idx , :] logging . info ( \"Created train set containing %d samples and test set containing %d samples\" , len ( train_idx ), len ( test_idx ), ) return train_feature_df , train_meta_data_df , test_feature_df , test_meta_data_df","title":"Utils"},{"location":"utils/utils/#utility-functions","text":"The fisseq_data_pipeline.utils.utils module provides helper functions for feature selection, dataset construction, and splitting. These utilities are used internally by the pipeline but can also be reused in standalone scripts.","title":"Utility functions"},{"location":"utils/utils/#overview","text":"get_feature_selector : Build a Polars selector expression for feature columns based on the pipeline configuration. get_feature_columns : Select feature columns from a LazyFrame using a config. get_data_dfs : Build aligned feature and metadata DataFrames from a Polars LazyFrame . train_test_split : Create stratified train/test splits for features and metadata.","title":"Overview"},{"location":"utils/utils/#environment-variables","text":"FISSEQ_PIPELINE_RAND_STATE Random seed used for reproducible stratified train/test splits. Default: 42 .","title":"Environment variables"},{"location":"utils/utils/#example","text":"# Use a fixed seed of 1234 for train/test splitting FISSEQ_PIPELINE_RAND_STATE=1234 fisseq-data-pipeline validate ...","title":"Example"},{"location":"utils/utils/#example-usage","text":"import polars as pl from fisseq_data_pipeline.utils.config import Config from fisseq_data_pipeline.utils.utils import ( get_data_dfs, train_test_split ) # Example dataset df = pl.DataFrame({ \"gene1\": [1.0, 2.0, 3.0, 4.0], \"gene2\": [5.0, 6.0, 7.0, 8.0], \"batch\": [\"A\", \"A\", \"B\", \"B\"], \"label\": [\"X\", \"X\", \"Y\", \"Y\"], \"is_ctrl\": [True, False, True, False], }).lazy() # Example config (using dict for simplicity) cfg = Config({ \"feature_cols\": [\"gene1\", \"gene2\"], \"batch_col_name\": \"batch\", \"label_col_name\": \"label\", \"control_sample_query\": \"col('is_ctrl')\", }) # Build feature + metadata DataFrames feature_df, meta_data_df = get_data_dfs(df, cfg) # Stratified train/test split train_f, train_m, test_f, test_m = train_test_split(feature_df, meta_data_df, test_size=0.5)","title":"Example Usage"},{"location":"utils/utils/#api-reference","text":"","title":"API Reference"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.get_feature_selector","text":"Get feature column feature selector based on config Parameters: data_df ( LazyFrame ) \u2013 The input data as a Polars LazyFrame. config ( Config ) \u2013 Configuration object containing a feature_cols attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns: PlSelector \u2013 A selector that can be used to select feature columns in .select call. Source code in src/fisseq_data_pipeline/utils/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def get_feature_selector ( data_df : pl . LazyFrame , config : Config ) -> PlSelector : \"\"\" Get feature column feature selector based on config Parameters ---------- data_df : pl.LazyFrame The input data as a Polars LazyFrame. config : Config Configuration object containing a ``feature_cols`` attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns ------- PlSelector A selector that can be used to select feature columns in ``.select`` call. \"\"\" if isinstance ( config . feature_cols , str ): selector_type = \"regex\" selector = cs . matches ( config . feature_cols ) else : selector_type = \"list\" feature_cols = set ( config . feature_cols ) missing = feature_cols - set ( data_df . columns ) if len ( missing ) != 0 : logging . warning ( \"Some columns are specified in the config but are not currently\" \" present in the dataframe, this can happen if columns are\" \" removed during data cleaning. The following columns will be\" \" ignored: %s \" , missing , ) # Column order must be preserved not_missing = feature_cols - missing selector = pl . col ( col for col in list ( config . feature_cols ) if col in not_missing ) logging . debug ( \"Using feature %s selector: %s \" , selector_type , config . feature_cols ) return selector","title":"get_feature_selector"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.get_feature_columns","text":"Select feature columns from a Polars LazyFrame based on the configuration Parameters: data_df ( LazyFrame ) \u2013 The input data as a Polars LazyFrame. config ( Config ) \u2013 Configuration object containing a feature_cols attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns: LazyFrame \u2013 A Polars LazyFrame containing only the selected feature columns, cast to the given dtype. Source code in src/fisseq_data_pipeline/utils/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_feature_columns ( data_df : pl . LazyFrame , config : Config ) -> pl . LazyFrame : \"\"\" Select feature columns from a Polars LazyFrame based on the configuration Parameters ---------- data_df : pl.LazyFrame The input data as a Polars LazyFrame. config : Config Configuration object containing a ``feature_cols`` attribute, which may be either: - str : A regex pattern to match column names. - list[str] : A list of explicit column names to select. Returns ------- pl.LazyFrame A Polars LazyFrame containing only the selected feature columns, cast to the given dtype. \"\"\" selector = get_feature_selector ( data_df , config ) data_df = data_df . select ( selector ) return data_df","title":"get_feature_columns"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.get_data_dfs","text":"Construct a numeric feature matrix and a metadata DataFrame from a Polars LazyFrame. The computation graph: 1. Adds a row index column ( _sample_idx ) for stable sample tracking. 2. Selects feature columns defined in config.feature_cols and casts them to dtype . 3. Extracts the batch column ( _batch ), the label column ( _label ), and a boolean control mask ( _is_control ) using config.control_sample_query . 4. Collects the LazyFrame once and returns NumPy features plus eager Polars metadata. Parameters: data_df ( LazyFrame ) \u2013 Input dataset as a Polars LazyFrame. config ( Config ) \u2013 Configuration object containing: - feature_cols : regex or list of feature column names. - batch_col_name : column name to be exposed as _batch . - label_col_name : column name to be exposed as _label . - control_sample_query : SQL WHERE clause fragment defining control samples (used to produce _is_control ). dtype ( DataType , default: Float32 ) \u2013 Target dtype for feature columns (default: pl.Float32 ). Returns: Tuple [ ndarray , DataFrame ] \u2013 Feature matrix: np.ndarray of shape (n_samples, n_features) , with columns from config.feature_cols cast to dtype . Metadata DataFrame: eager pl.DataFrame with columns: _batch : values from config.batch_col_name . _label : values from config.label_col_name . _is_control : bool mask indicating control samples. _sample_idx : integer row index for reference. Source code in src/fisseq_data_pipeline/utils/utils.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def get_data_dfs ( data_df : pl . LazyFrame , config : Config , dtype : pl . DataType = pl . Float32 , ) -> Tuple [ pl . DataFrame , pl . DataFrame ]: \"\"\" Construct a numeric feature matrix and a metadata DataFrame from a Polars LazyFrame. The computation graph: 1. Adds a row index column (``_sample_idx``) for stable sample tracking. 2. Selects feature columns defined in ``config.feature_cols`` and casts them to ``dtype``. 3. Extracts the batch column (``_batch``), the label column (``_label``), and a boolean control mask (``_is_control``) using ``config.control_sample_query``. 4. Collects the LazyFrame once and returns NumPy features plus eager Polars metadata. Parameters ---------- data_df : pl.LazyFrame Input dataset as a Polars LazyFrame. config : Config Configuration object containing: - ``feature_cols``: regex or list of feature column names. - ``batch_col_name``: column name to be exposed as ``_batch``. - ``label_col_name``: column name to be exposed as ``_label``. - ``control_sample_query``: SQL WHERE clause fragment defining control samples (used to produce ``_is_control``). dtype : pl.DataType, optional Target dtype for feature columns (default: ``pl.Float32``). Returns ------- Tuple[np.ndarray, pl.DataFrame] - Feature matrix: ``np.ndarray`` of shape ``(n_samples, n_features)``, with columns from ``config.feature_cols`` cast to ``dtype``. - Metadata DataFrame: eager ``pl.DataFrame`` with columns: * ``_batch``: values from ``config.batch_col_name``. * ``_label``: values from ``config.label_col_name``. * ``_is_control``: bool mask indicating control samples. * ``_sample_idx``: integer row index for reference. \"\"\" logging . info ( \"Starting get_data_matrices for batch_col= %s , dtype= %s \" , config . batch_col_name , dtype , ) # Attach row indices to preserve mapping later base = data_df . with_row_index ( name = \"_sample_idx\" ) . cache () # Build feature selector feature_expr = get_feature_selector ( base , config ) . cast ( dtype = dtype ) logging . debug ( \"Feature selector resolved: %s \" , feature_expr ) # Control mask expr logging . debug ( \"Parsing control sample query: %s \" , config . control_sample_query ) control_mask_expr = pl . sql_expr ( config . control_sample_query ) . alias ( \"_is_control\" ) batch_expr = pl . col ( config . batch_col_name ) . alias ( \"_batch\" ) label_expr = pl . col ( config . label_col_name ) . alias ( \"_label\" ) # Execute the full plan logging . info ( \"Collecting LazyFrame into DataFrame\" ) df = ( base . with_columns ( label_expr , batch_expr , control_mask_expr , ) . select ( feature_expr , pl . col ( \"_batch\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ), pl . col ( \"_label\" ), ) . collect () ) logging . info ( \"Collection complete: shape= %s \" , df . shape ) # Get feature dataframe feature_df = df . select ( feature_expr ) logging . debug ( \"Feature dataframe shape: %s \" , feature_df . shape ) meta_data_df = df . select ( pl . col ( \"_batch\" ), pl . col ( \"_label\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ) ) logging . debug ( \"Meta data dataframe: shape= %s \" , meta_data_df . shape ) logging . info ( \"Finished get_data_matrices\" ) return feature_df , meta_data_df","title":"get_data_dfs"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.train_test_split","text":"Split feature and metadata DataFrames into stratified train and test sets. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix with shape (n_samples, n_features). meta_data_df ( DataFrame ) \u2013 Metadata aligned row-wise with feature_df . Must contain columns _label (class labels) and _batch (batch identifiers). test_size ( float ) \u2013 Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns: train_feature_df ( DataFrame ) \u2013 Features for the training set. train_meta_data_df ( DataFrame ) \u2013 Metadata for the training set. test_feature_df ( DataFrame ) \u2013 Features for the test set. test_meta_data_df ( DataFrame ) \u2013 Metadata for the test set. Notes Each (_label, _batch) group must have at least two samples for stratification to succeed. The split is reproducible if RANDOM_STATE is fixed. Source code in src/fisseq_data_pipeline/utils/utils.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def train_test_split ( feature_df : pl . DataFrame , meta_data_df : pl . DataFrame , test_size : float , ) -> Tuple [ pl . DataFrame , pl . DataFrame , pl . DataFrame , pl . DataFrame ]: \"\"\" Split feature and metadata DataFrames into stratified train and test sets. Parameters ---------- feature_df : pl.DataFrame Feature matrix with shape (n_samples, n_features). meta_data_df : pl.DataFrame Metadata aligned row-wise with ``feature_df``. Must contain columns ``_label`` (class labels) and ``_batch`` (batch identifiers). test_size : float Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns ------- train_feature_df : pl.DataFrame Features for the training set. train_meta_data_df : pl.DataFrame Metadata for the training set. test_feature_df : pl.DataFrame Features for the test set. test_meta_data_df : pl.DataFrame Metadata for the test set. Notes ----- - Each ``(_label, _batch)`` group must have at least two samples for stratification to succeed. - The split is reproducible if ``RANDOM_STATE`` is fixed. \"\"\" logging . info ( \"Creating train test split\" ) stratify = [ f \" { label } : { batch } \" for label , batch in zip ( meta_data_df . get_column ( \"_label\" ), meta_data_df . get_column ( \"_batch\" ) ) ] train_idx , test_idx = sklearn . model_selection . train_test_split ( np . arange ( len ( meta_data_df )), test_size = test_size , stratify = stratify , random_state = RANDOM_STATE , ) logging . info ( \"Created splits, copying data\" ) train_feature_df = feature_df [ train_idx , :] train_meta_data_df = meta_data_df [ train_idx , :] test_feature_df = feature_df [ test_idx , :] test_meta_data_df = meta_data_df [ test_idx , :] logging . info ( \"Created train set containing %d samples and test set containing %d samples\" , len ( train_idx ), len ( test_idx ), ) return train_feature_df , train_meta_data_df , test_feature_df , test_meta_data_df","title":"train_test_split"}]}