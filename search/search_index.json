{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FISSEQ Data Pipeline Welcome to the FISSEQ Data Pipeline documentation. Features Command-line interface (CLI) Access the pipeline with a single entry point: fisseq-data-pipeline [validate|run|configure] For more details on command line usage see Pipeline . Data cleaning Remove invalid rows/columns and rare label\u2013batch pairs, namely columns that contain all NaN values, followed by rows that contain any remaining NaN values. See Filter . Normalization Compute z-score normalization statistics on control samples and apply them across the dataset. By default z-score normalization is fit only to control samples. Fitting only to control samples ensures that biological variation is captured even in the case where biological covariants are largely disjoint across batches. See Normalize . Installation This package in its current state should be considered experimental, and is thus not hosted on PyPI. However, the package may be installed directly from Github using the command: pip install git+https://github.com/Lilferrit/fisseq-data-pipeline.git You may also clone the repository and install dependencies: git clone https://github.com/your-org/fisseq-data-pipeline.git cd fisseq-data-pipeline pip install -e . Running the Pipeline After installation the pipeline can be run from the command line. For more details see Pipeline . Configuration The pipeline may be configured using a yaml configuration file. For more details see Configuration .","title":"Home"},{"location":"#fisseq-data-pipeline","text":"Welcome to the FISSEQ Data Pipeline documentation.","title":"FISSEQ Data Pipeline"},{"location":"#features","text":"","title":"Features"},{"location":"#command-line-interface-cli","text":"Access the pipeline with a single entry point: fisseq-data-pipeline [validate|run|configure] For more details on command line usage see Pipeline .","title":"Command-line interface (CLI)"},{"location":"#data-cleaning","text":"Remove invalid rows/columns and rare label\u2013batch pairs, namely columns that contain all NaN values, followed by rows that contain any remaining NaN values. See Filter .","title":"Data cleaning"},{"location":"#normalization","text":"Compute z-score normalization statistics on control samples and apply them across the dataset. By default z-score normalization is fit only to control samples. Fitting only to control samples ensures that biological variation is captured even in the case where biological covariants are largely disjoint across batches. See Normalize .","title":"Normalization"},{"location":"#installation","text":"This package in its current state should be considered experimental, and is thus not hosted on PyPI. However, the package may be installed directly from Github using the command: pip install git+https://github.com/Lilferrit/fisseq-data-pipeline.git You may also clone the repository and install dependencies: git clone https://github.com/your-org/fisseq-data-pipeline.git cd fisseq-data-pipeline pip install -e .","title":"Installation"},{"location":"#running-the-pipeline","text":"After installation the pipeline can be run from the command line. For more details see Pipeline .","title":"Running the Pipeline"},{"location":"#configuration","text":"The pipeline may be configured using a yaml configuration file. For more details see Configuration .","title":"Configuration"},{"location":"configuration/","text":"Configuration YAML The FISSEQ pipeline is configured with a YAML file that defines how to interpret the dataset. A default configuration ( config.yaml ) ships with the pipeline and is used if no custom configuration is provided. Default configuration # Regex or list to select feature columns # (CellProfiler columns start with an uppercase and contain an underscore) feature_cols: \"^[A-Z][A-Za-z0-9]*_.*\" # SQL-like WHERE clause to select control samples. # The query will be interpolated into: # SELECT * FROM self WHERE {control_sample_query} control_sample_query: \"variantClass = 'WT'\" # The name of the column containing the batch identifier batch_col_name: \"tile_experiment_well\" # Column containing biological labels label_col_name: \"aaChanges\" Field descriptions feature_cols Type: str (regex pattern) or list[str] (explicit list). Default: ^[A-Z][A-Za-z0-9]*_.* Defines which columns are treated as features. For CellProfiler outputs, this matches columns starting with an uppercase letter and containing an underscore. control_sample_query Type: str (SQL-like WHERE clause). Default: variantClass = 'WT' Used to flag control samples. Applied as a boolean mask when constructing metadata. Example control_sample_query: \"treatment = 'DMSO'\" batch_col_name Type: str Default: tile_experiment_well Column containing batch identifiers (e.g., well, experiment, or run). Used for stratification and harmonization. label_col_name Type: str Default: aaChanges Column containing biological labels (e.g., variant information). Used for stratification during validation.","title":"Configuration"},{"location":"configuration/#configuration-yaml","text":"The FISSEQ pipeline is configured with a YAML file that defines how to interpret the dataset. A default configuration ( config.yaml ) ships with the pipeline and is used if no custom configuration is provided.","title":"Configuration YAML"},{"location":"configuration/#default-configuration","text":"# Regex or list to select feature columns # (CellProfiler columns start with an uppercase and contain an underscore) feature_cols: \"^[A-Z][A-Za-z0-9]*_.*\" # SQL-like WHERE clause to select control samples. # The query will be interpolated into: # SELECT * FROM self WHERE {control_sample_query} control_sample_query: \"variantClass = 'WT'\" # The name of the column containing the batch identifier batch_col_name: \"tile_experiment_well\" # Column containing biological labels label_col_name: \"aaChanges\"","title":"Default configuration"},{"location":"configuration/#field-descriptions","text":"","title":"Field descriptions"},{"location":"configuration/#feature_cols","text":"Type: str (regex pattern) or list[str] (explicit list). Default: ^[A-Z][A-Za-z0-9]*_.* Defines which columns are treated as features. For CellProfiler outputs, this matches columns starting with an uppercase letter and containing an underscore.","title":"feature_cols"},{"location":"configuration/#control_sample_query","text":"Type: str (SQL-like WHERE clause). Default: variantClass = 'WT' Used to flag control samples. Applied as a boolean mask when constructing metadata.","title":"control_sample_query"},{"location":"configuration/#example","text":"control_sample_query: \"treatment = 'DMSO'\"","title":"Example"},{"location":"configuration/#batch_col_name","text":"Type: str Default: tile_experiment_well Column containing batch identifiers (e.g., well, experiment, or run). Used for stratification and harmonization.","title":"batch_col_name"},{"location":"configuration/#label_col_name","text":"Type: str Default: aaChanges Column containing biological labels (e.g., variant information). Used for stratification during validation.","title":"label_col_name"},{"location":"filter/","text":"Data Cleaning Utilities The fisseq_data_pipeline.filter module provides functions to clean and filter feature/metadata tables prior to normalization and harmonization. These utilities are invoked automatically in the pipeline, but can also be used independently. Overview clean_data : Removes invalid rows/columns from feature and metadata tables while keeping them aligned. drop_infrequent_pairs : Drops rows from rare (label, batch) groups according to a configurable threshold. Environment variables FISSEQ_PIPELINE_MIN_CLASS_MEMBERS Minimum number of samples required per (label, batch) group when running drop_infrequent_pairs . Default: 2 . Example: # Require at least 5 samples per label\u2013batch group FISSEQ_PIPELINE_MIN_CLASS_MEMBERS=5 fisseq-data-pipeline validate ... Example Usage import polars as pl from fisseq_data_pipeline.filter import clean_data, drop_infrequent_pairs # Example feature matrix feature_df = pl.DataFrame({ \"f1\": [1.0, 2.0, float(\"nan\"), 4.0], \"f2\": [5.0, 6.0, 7.0, 8.0], }) # Example metadata with batch + label meta_df = pl.DataFrame({ \"_label\": [\"A\", \"A\", \"B\", \"B\"], \"_batch\": [\"X\", \"Y\", \"X\", \"Y\"], }) # Clean non-finite and zero-variance columns/rows feature_df, meta_df = clean_data(feature_df, meta_df) # Drop infrequent (label, batch) pairs feature_df, meta_df = drop_infrequent_pairs(feature_df, meta_df) API reference fisseq_data_pipeline . filter . clean_data ( feature_df , meta_data_df , stages = [ 'drop_cols_all_nonfinite' , 'drop_rows_any_nonfinite' , 'drop_rows_infrequent_pairs' ]) Sequentially apply a series of filters to feature and metadata LazyFrames. Each stage may be specified by name or as a callable implementing the FilterFun signature. Stages are executed in order, and each must return updated (feature_df, meta_data_df) pairs. Parameters: feature_df ( LazyFrame ) \u2013 LazyFrame containing numerical features. meta_data_df ( LazyFrame ) \u2013 LazyFrame containing associated metadata. stages ( Iterable [ str | FilterFun ] , default: ['drop_cols_all_nonfinite', 'drop_rows_any_nonfinite', 'drop_rows_infrequent_pairs'] ) \u2013 Ordered list of stage names or callables. The default pipeline includes: - \"drop_cols_all_nonfinite\" \u2014 remove columns that are all NaN/inf - \"drop_rows_any_nonfinite\" \u2014 drop rows with any non-finite values - \"drop_rows_infrequent_pairs\" \u2014 drop small (label,batch) groups Returns: ( LazyFrame , LazyFrame ) \u2013 The cleaned feature and metadata LazyFrames. Notes Invalid stage names are skipped with a warning. Source code in src/fisseq_data_pipeline/filter.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def clean_data ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame , stages : Iterable [ str | FilterFun ] = [ \"drop_cols_all_nonfinite\" , \"drop_rows_any_nonfinite\" , \"drop_rows_infrequent_pairs\" , ], ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Sequentially apply a series of filters to feature and metadata LazyFrames. Each stage may be specified by name or as a callable implementing the ``FilterFun`` signature. Stages are executed in order, and each must return updated (feature_df, meta_data_df) pairs. Parameters ---------- feature_df : pl.LazyFrame LazyFrame containing numerical features. meta_data_df : pl.LazyFrame LazyFrame containing associated metadata. stages : Iterable[str | FilterFun], optional Ordered list of stage names or callables. The default pipeline includes: - ``\"drop_cols_all_nonfinite\"`` \u2014 remove columns that are all NaN/inf - ``\"drop_rows_any_nonfinite\"`` \u2014 drop rows with any non-finite values - ``\"drop_rows_infrequent_pairs\"`` \u2014 drop small (label,batch) groups Returns ------- (pl.LazyFrame, pl.LazyFrame) The cleaned feature and metadata LazyFrames. Notes ----- - Invalid stage names are skipped with a warning. \"\"\" stage_lookup : dict [ str , FilterFun ] = { \"drop_cols_all_nonfinite\" : drop_cols_all_nonfinite , \"drop_rows_any_nonfinite\" : drop_rows_any_nonfinite , \"drop_rows_infrequent_pairs\" : drop_rows_infrequent_pairs , } for stage in stages : if isinstance ( stage , str ): if stage not in stage_lookup : logging . warning ( \"Skipping invalid filtering stage: %s \" , stage ) continue stage = stage_lookup [ stage ] feature_df , meta_data_df = stage ( feature_df , meta_data_df ) return feature_df , meta_data_df fisseq_data_pipeline . filter . drop_rows_infrequent_pairs ( feature_df , meta_data_df ) Remove rows belonging to rare ( _label , _batch ) groups. Rows are grouped by the concatenation of _label and _batch . Any group with a sample count less than MINIMUM_CLASS_MEMBERS is dropped. Parameters: feature_df ( LazyFrame ) \u2013 Feature-only table of shape (n_samples, n_features). meta_data_df ( LazyFrame ) \u2013 Metadata table row-aligned with feature_df . Must include \"_label\" and \"_batch\" columns. Returns: ( LazyFrame , LazyFrame ) \u2013 LazyFrames with infrequent (label, batch) groups removed. Row order and alignment are preserved. Source code in src/fisseq_data_pipeline/filter.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def drop_rows_infrequent_pairs ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Remove rows belonging to rare (``_label``, ``_batch``) groups. Rows are grouped by the concatenation of ``_label`` and ``_batch``. Any group with a sample count less than ``MINIMUM_CLASS_MEMBERS`` is dropped. Parameters ---------- feature_df : pl.LazyFrame Feature-only table of shape (n_samples, n_features). meta_data_df : pl.LazyFrame Metadata table row-aligned with ``feature_df``. Must include ``\"_label\"`` and ``\"_batch\"`` columns. Returns ------- (pl.LazyFrame, pl.LazyFrame) LazyFrames with infrequent (label, batch) groups removed. Row order and alignment are preserved. \"\"\" logging . info ( \"Adding query to remove infrequent batch/variant pairs\" ) # Add combined key + stable row index meta_data_df = meta_data_df . with_row_index ( \"_idx\" ) . with_columns ( ( pl . col ( \"_label\" ) + \"_\" + pl . col ( \"_batch\" )) . alias ( \"_label_batch\" ) ) # Compute per-(label,batch) counts and join back counts_df = meta_data_df . group_by ( \"_label_batch\" ) . agg ( pl . count () . alias ( \"_count\" )) meta_data_df = meta_data_df . join ( counts_df , on = \"_label_batch\" , how = \"left\" ) # Attach matching index to feature_df feature_df = feature_df . with_row_index ( \"_idx\" ) # Join count info to feature_df via _idx (so we can filter both) feature_df = feature_df . join ( meta_data_df . select ( \"_idx\" , \"_count\" ), on = \"_idx\" , how = \"left\" , ) # Filter both lazily keep_expr = pl . col ( \"_count\" ) >= MINIMUM_CLASS_MEMBERS exclude_cols = pl . exclude ([ \"_count\" , \"_idx\" ]) feature_df = feature_df . filter ( keep_expr ) . select ( exclude_cols ) meta_data_df = meta_data_df . filter ( keep_expr ) . select ( exclude_cols ) return feature_df , meta_data_df fisseq_data_pipeline . filter . drop_cols_all_nonfinite ( feature_df , meta_data_df ) Remove feature columns that contain only non-finite values. Any column in feature_df where all entries are NaN, +inf, or -inf is excluded from the output. Parameters: feature_df ( LazyFrame ) \u2013 LazyFrame containing numerical feature columns. meta_data_df ( LazyFrame ) \u2013 LazyFrame containing sample-level metadata (returned unchanged). Returns: ( LazyFrame , LazyFrame ) \u2013 Tuple of (feature_df, meta_data_df) where feature_df excludes all columns consisting solely of non-finite values. Source code in src/fisseq_data_pipeline/filter.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def drop_cols_all_nonfinite ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Remove feature columns that contain only non-finite values. Any column in ``feature_df`` where all entries are NaN, +inf, or -inf is excluded from the output. Parameters ---------- feature_df : pl.LazyFrame LazyFrame containing numerical feature columns. meta_data_df : pl.LazyFrame LazyFrame containing sample-level metadata (returned unchanged). Returns ------- (pl.LazyFrame, pl.LazyFrame) Tuple of (feature_df, meta_data_df) where feature_df excludes all columns consisting solely of non-finite values. \"\"\" # TODO: Can this also be a lazy expression? logging . info ( \"Scanning for all non-finite rows\" ) finite_summary = feature_df . select ( [ pl . col ( c ) . is_finite () . any () . alias ( c ) for c in feature_df . columns ] ) . collect () logging . info ( \"Adding query to remove columns containing all non-finite values\" ) non_finite_cols = [ c for c in feature_df . columns if not finite_summary [ c ][ 0 ]] feature_df = feature_df . select ( pl . exclude ( non_finite_cols )) logging . info ( \"Removing %d columns containing only non-finite values\" , len ( non_finite_cols ), ) return feature_df , meta_data_df fisseq_data_pipeline . filter . drop_rows_any_nonfinite ( feature_df , meta_data_df ) Remove rows that contain any non-finite feature values. This step filters out all samples where at least one feature value is NaN, +inf, or -inf. Row alignment between the feature and metadata tables is preserved. Parameters: feature_df ( LazyFrame ) \u2013 LazyFrame of numerical features. meta_data_df ( LazyFrame ) \u2013 Row-aligned LazyFrame of metadata. Returns: ( LazyFrame , LazyFrame ) \u2013 LazyFrames excluding all rows with any non-finite feature values. Source code in src/fisseq_data_pipeline/filter.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def drop_rows_any_nonfinite ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Remove rows that contain any non-finite feature values. This step filters out all samples where at least one feature value is NaN, +inf, or -inf. Row alignment between the feature and metadata tables is preserved. Parameters ---------- feature_df : pl.LazyFrame LazyFrame of numerical features. meta_data_df : pl.LazyFrame Row-aligned LazyFrame of metadata. Returns ------- (pl.LazyFrame, pl.LazyFrame) LazyFrames excluding all rows with any non-finite feature values. \"\"\" logging . info ( \"Adding query to remove any remaining rows that contain non-finite\" \" feature values\" ) feature_df = feature_df . with_row_index ( \"_idx\" ) . with_columns ( pl . all_horizontal ( pl . all () . is_finite ()) . alias ( \"_all_finite\" ) ) meta_data_df = meta_data_df . with_row_index ( \"_idx\" ) . join ( feature_df . select ( pl . col ( \"_idx\" ), pl . col ( \"_all_finite\" )), on = \"_idx\" ) filter_expr = pl . col ( \"_all_finite\" ) exclude_expr = pl . exclude ([ \"_all_finite\" , \"_idx\" ]) feature_df = feature_df . filter ( filter_expr ) . select ( exclude_expr ) meta_data_df = meta_data_df . filter ( filter_expr ) . select ( exclude_expr ) return feature_df , meta_data_df","title":"Filter"},{"location":"filter/#data-cleaning-utilities","text":"The fisseq_data_pipeline.filter module provides functions to clean and filter feature/metadata tables prior to normalization and harmonization. These utilities are invoked automatically in the pipeline, but can also be used independently.","title":"Data Cleaning Utilities"},{"location":"filter/#overview","text":"clean_data : Removes invalid rows/columns from feature and metadata tables while keeping them aligned. drop_infrequent_pairs : Drops rows from rare (label, batch) groups according to a configurable threshold.","title":"Overview"},{"location":"filter/#environment-variables","text":"FISSEQ_PIPELINE_MIN_CLASS_MEMBERS Minimum number of samples required per (label, batch) group when running drop_infrequent_pairs . Default: 2 . Example: # Require at least 5 samples per label\u2013batch group FISSEQ_PIPELINE_MIN_CLASS_MEMBERS=5 fisseq-data-pipeline validate ...","title":"Environment variables"},{"location":"filter/#example-usage","text":"import polars as pl from fisseq_data_pipeline.filter import clean_data, drop_infrequent_pairs # Example feature matrix feature_df = pl.DataFrame({ \"f1\": [1.0, 2.0, float(\"nan\"), 4.0], \"f2\": [5.0, 6.0, 7.0, 8.0], }) # Example metadata with batch + label meta_df = pl.DataFrame({ \"_label\": [\"A\", \"A\", \"B\", \"B\"], \"_batch\": [\"X\", \"Y\", \"X\", \"Y\"], }) # Clean non-finite and zero-variance columns/rows feature_df, meta_df = clean_data(feature_df, meta_df) # Drop infrequent (label, batch) pairs feature_df, meta_df = drop_infrequent_pairs(feature_df, meta_df)","title":"Example Usage"},{"location":"filter/#api-reference","text":"","title":"API reference"},{"location":"filter/#fisseq_data_pipeline.filter.clean_data","text":"Sequentially apply a series of filters to feature and metadata LazyFrames. Each stage may be specified by name or as a callable implementing the FilterFun signature. Stages are executed in order, and each must return updated (feature_df, meta_data_df) pairs. Parameters: feature_df ( LazyFrame ) \u2013 LazyFrame containing numerical features. meta_data_df ( LazyFrame ) \u2013 LazyFrame containing associated metadata. stages ( Iterable [ str | FilterFun ] , default: ['drop_cols_all_nonfinite', 'drop_rows_any_nonfinite', 'drop_rows_infrequent_pairs'] ) \u2013 Ordered list of stage names or callables. The default pipeline includes: - \"drop_cols_all_nonfinite\" \u2014 remove columns that are all NaN/inf - \"drop_rows_any_nonfinite\" \u2014 drop rows with any non-finite values - \"drop_rows_infrequent_pairs\" \u2014 drop small (label,batch) groups Returns: ( LazyFrame , LazyFrame ) \u2013 The cleaned feature and metadata LazyFrames. Notes Invalid stage names are skipped with a warning. Source code in src/fisseq_data_pipeline/filter.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def clean_data ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame , stages : Iterable [ str | FilterFun ] = [ \"drop_cols_all_nonfinite\" , \"drop_rows_any_nonfinite\" , \"drop_rows_infrequent_pairs\" , ], ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Sequentially apply a series of filters to feature and metadata LazyFrames. Each stage may be specified by name or as a callable implementing the ``FilterFun`` signature. Stages are executed in order, and each must return updated (feature_df, meta_data_df) pairs. Parameters ---------- feature_df : pl.LazyFrame LazyFrame containing numerical features. meta_data_df : pl.LazyFrame LazyFrame containing associated metadata. stages : Iterable[str | FilterFun], optional Ordered list of stage names or callables. The default pipeline includes: - ``\"drop_cols_all_nonfinite\"`` \u2014 remove columns that are all NaN/inf - ``\"drop_rows_any_nonfinite\"`` \u2014 drop rows with any non-finite values - ``\"drop_rows_infrequent_pairs\"`` \u2014 drop small (label,batch) groups Returns ------- (pl.LazyFrame, pl.LazyFrame) The cleaned feature and metadata LazyFrames. Notes ----- - Invalid stage names are skipped with a warning. \"\"\" stage_lookup : dict [ str , FilterFun ] = { \"drop_cols_all_nonfinite\" : drop_cols_all_nonfinite , \"drop_rows_any_nonfinite\" : drop_rows_any_nonfinite , \"drop_rows_infrequent_pairs\" : drop_rows_infrequent_pairs , } for stage in stages : if isinstance ( stage , str ): if stage not in stage_lookup : logging . warning ( \"Skipping invalid filtering stage: %s \" , stage ) continue stage = stage_lookup [ stage ] feature_df , meta_data_df = stage ( feature_df , meta_data_df ) return feature_df , meta_data_df","title":"clean_data"},{"location":"filter/#fisseq_data_pipeline.filter.drop_rows_infrequent_pairs","text":"Remove rows belonging to rare ( _label , _batch ) groups. Rows are grouped by the concatenation of _label and _batch . Any group with a sample count less than MINIMUM_CLASS_MEMBERS is dropped. Parameters: feature_df ( LazyFrame ) \u2013 Feature-only table of shape (n_samples, n_features). meta_data_df ( LazyFrame ) \u2013 Metadata table row-aligned with feature_df . Must include \"_label\" and \"_batch\" columns. Returns: ( LazyFrame , LazyFrame ) \u2013 LazyFrames with infrequent (label, batch) groups removed. Row order and alignment are preserved. Source code in src/fisseq_data_pipeline/filter.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def drop_rows_infrequent_pairs ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Remove rows belonging to rare (``_label``, ``_batch``) groups. Rows are grouped by the concatenation of ``_label`` and ``_batch``. Any group with a sample count less than ``MINIMUM_CLASS_MEMBERS`` is dropped. Parameters ---------- feature_df : pl.LazyFrame Feature-only table of shape (n_samples, n_features). meta_data_df : pl.LazyFrame Metadata table row-aligned with ``feature_df``. Must include ``\"_label\"`` and ``\"_batch\"`` columns. Returns ------- (pl.LazyFrame, pl.LazyFrame) LazyFrames with infrequent (label, batch) groups removed. Row order and alignment are preserved. \"\"\" logging . info ( \"Adding query to remove infrequent batch/variant pairs\" ) # Add combined key + stable row index meta_data_df = meta_data_df . with_row_index ( \"_idx\" ) . with_columns ( ( pl . col ( \"_label\" ) + \"_\" + pl . col ( \"_batch\" )) . alias ( \"_label_batch\" ) ) # Compute per-(label,batch) counts and join back counts_df = meta_data_df . group_by ( \"_label_batch\" ) . agg ( pl . count () . alias ( \"_count\" )) meta_data_df = meta_data_df . join ( counts_df , on = \"_label_batch\" , how = \"left\" ) # Attach matching index to feature_df feature_df = feature_df . with_row_index ( \"_idx\" ) # Join count info to feature_df via _idx (so we can filter both) feature_df = feature_df . join ( meta_data_df . select ( \"_idx\" , \"_count\" ), on = \"_idx\" , how = \"left\" , ) # Filter both lazily keep_expr = pl . col ( \"_count\" ) >= MINIMUM_CLASS_MEMBERS exclude_cols = pl . exclude ([ \"_count\" , \"_idx\" ]) feature_df = feature_df . filter ( keep_expr ) . select ( exclude_cols ) meta_data_df = meta_data_df . filter ( keep_expr ) . select ( exclude_cols ) return feature_df , meta_data_df","title":"drop_rows_infrequent_pairs"},{"location":"filter/#fisseq_data_pipeline.filter.drop_cols_all_nonfinite","text":"Remove feature columns that contain only non-finite values. Any column in feature_df where all entries are NaN, +inf, or -inf is excluded from the output. Parameters: feature_df ( LazyFrame ) \u2013 LazyFrame containing numerical feature columns. meta_data_df ( LazyFrame ) \u2013 LazyFrame containing sample-level metadata (returned unchanged). Returns: ( LazyFrame , LazyFrame ) \u2013 Tuple of (feature_df, meta_data_df) where feature_df excludes all columns consisting solely of non-finite values. Source code in src/fisseq_data_pipeline/filter.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def drop_cols_all_nonfinite ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Remove feature columns that contain only non-finite values. Any column in ``feature_df`` where all entries are NaN, +inf, or -inf is excluded from the output. Parameters ---------- feature_df : pl.LazyFrame LazyFrame containing numerical feature columns. meta_data_df : pl.LazyFrame LazyFrame containing sample-level metadata (returned unchanged). Returns ------- (pl.LazyFrame, pl.LazyFrame) Tuple of (feature_df, meta_data_df) where feature_df excludes all columns consisting solely of non-finite values. \"\"\" # TODO: Can this also be a lazy expression? logging . info ( \"Scanning for all non-finite rows\" ) finite_summary = feature_df . select ( [ pl . col ( c ) . is_finite () . any () . alias ( c ) for c in feature_df . columns ] ) . collect () logging . info ( \"Adding query to remove columns containing all non-finite values\" ) non_finite_cols = [ c for c in feature_df . columns if not finite_summary [ c ][ 0 ]] feature_df = feature_df . select ( pl . exclude ( non_finite_cols )) logging . info ( \"Removing %d columns containing only non-finite values\" , len ( non_finite_cols ), ) return feature_df , meta_data_df","title":"drop_cols_all_nonfinite"},{"location":"filter/#fisseq_data_pipeline.filter.drop_rows_any_nonfinite","text":"Remove rows that contain any non-finite feature values. This step filters out all samples where at least one feature value is NaN, +inf, or -inf. Row alignment between the feature and metadata tables is preserved. Parameters: feature_df ( LazyFrame ) \u2013 LazyFrame of numerical features. meta_data_df ( LazyFrame ) \u2013 Row-aligned LazyFrame of metadata. Returns: ( LazyFrame , LazyFrame ) \u2013 LazyFrames excluding all rows with any non-finite feature values. Source code in src/fisseq_data_pipeline/filter.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def drop_rows_any_nonfinite ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Remove rows that contain any non-finite feature values. This step filters out all samples where at least one feature value is NaN, +inf, or -inf. Row alignment between the feature and metadata tables is preserved. Parameters ---------- feature_df : pl.LazyFrame LazyFrame of numerical features. meta_data_df : pl.LazyFrame Row-aligned LazyFrame of metadata. Returns ------- (pl.LazyFrame, pl.LazyFrame) LazyFrames excluding all rows with any non-finite feature values. \"\"\" logging . info ( \"Adding query to remove any remaining rows that contain non-finite\" \" feature values\" ) feature_df = feature_df . with_row_index ( \"_idx\" ) . with_columns ( pl . all_horizontal ( pl . all () . is_finite ()) . alias ( \"_all_finite\" ) ) meta_data_df = meta_data_df . with_row_index ( \"_idx\" ) . join ( feature_df . select ( pl . col ( \"_idx\" ), pl . col ( \"_all_finite\" )), on = \"_idx\" ) filter_expr = pl . col ( \"_all_finite\" ) exclude_expr = pl . exclude ([ \"_all_finite\" , \"_idx\" ]) feature_df = feature_df . filter ( filter_expr ) . select ( exclude_expr ) meta_data_df = meta_data_df . filter ( filter_expr ) . select ( exclude_expr ) return feature_df , meta_data_df","title":"drop_rows_any_nonfinite"},{"location":"normalize/","text":"Normalize The fisseq_data_pipeline.normalize module provides utilities for computing and applying z-score normalization to feature matrices. Normalization is typically run as part of the FISSEQ pipeline, but these functions can also be used independently when you need to standardize feature values. Overview Normalizer : A dataclass container storing per-column means and standard deviations. fit_normalizer : Compute normalization statistics (means and stds) from a feature DataFrame, optionally restricted to control samples. normalize : Apply z-score normalization to a feature DataFrame using a fitted Normalizer . Example usage import polars as pl from fisseq_data_pipeline.normalize import fit_normalizer, normalize # Example feature matrix feature_df = pl.DataFrame({ \"x\": [1.0, 2.0, 3.0], \"y\": [2.0, 4.0, 6.0], }) # Fit the normalizer normalizer = fit_normalizer(feature_df) # Apply normalization normalized_df = normalize(feature_df, normalizer) print(normalized_df) Output shape: (3, 2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 x \u2502 y \u2502 \u2502 --- \u2502 --- \u2502 \u2502 f64 \u2502 f64 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 -1.0 \u2502 -1.0 \u2502 \u2502 0.0 \u2502 0.0 \u2502 \u2502 1.0 \u2502 1.0 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 API reference fisseq_data_pipeline.normalize.Normalizer dataclass Container object storing per-feature normalization statistics. Attributes: means ( DataFrame ) \u2013 A DataFrame of shape (n_batches, n_features) containing the mean value of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). stds ( DataFrame ) \u2013 A DataFrame of shape (n_batches, n_features) containing the standard deviation of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). is_batch_wise ( dict [ str , int ] or None ) \u2013 Whether statistics were computed batch-wise or globally Source code in src/fisseq_data_pipeline/normalize.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @dataclasses . dataclass class Normalizer : \"\"\" Container object storing per-feature normalization statistics. Attributes ---------- means : pl.DataFrame A DataFrame of shape (n_batches, n_features) containing the mean value of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). stds : pl.DataFrame A DataFrame of shape (n_batches, n_features) containing the standard deviation of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). is_batch_wise : dict[str, int] or None Whether statistics were computed batch-wise or globally \"\"\" means : pl . DataFrame stds : pl . DataFrame is_batch_wise : bool def save ( self , save_path : PathLike ) -> None : \"\"\" Serialize and save the Normalizer object to disk using pickle. This method stores the fitted normalization statistics \u2014 per-feature means, standard deviations, and batch-wise configuration flag \u2014 as a single binary file that can later be reloaded to reproduce the same normalization behavior. Parameters ---------- save_path : PathLike Destination file path for the serialized Normalizer object. The file is written in binary format (typically named ``normalizer.pkl``). Notes ----- - The file can be reloaded using ``pickle.load(open(path, \"rb\"))``. - Only the Normalizer object and its attributes are serialized; any external references (e.g., LazyFrames) are not included. - The resulting file is Python-version dependent and not guaranteed to be portable across major interpreter versions. Examples -------- >>> normalizer = fit_normalizer(feature_df, meta_data_df) >>> normalizer.save(\"output/normalizer.pkl\") To reload later: >>> with open(\"output/normalizer.pkl\", \"rb\") as f: ... normalizer = pickle.load(f) \"\"\" with open ( save_path , \"wb\" ) as f : pickle . dump ( self , f ) save ( save_path ) Serialize and save the Normalizer object to disk using pickle. This method stores the fitted normalization statistics \u2014 per-feature means, standard deviations, and batch-wise configuration flag \u2014 as a single binary file that can later be reloaded to reproduce the same normalization behavior. Parameters: save_path ( PathLike ) \u2013 Destination file path for the serialized Normalizer object. The file is written in binary format (typically named normalizer.pkl ). Notes The file can be reloaded using pickle.load(open(path, \"rb\")) . Only the Normalizer object and its attributes are serialized; any external references (e.g., LazyFrames) are not included. The resulting file is Python-version dependent and not guaranteed to be portable across major interpreter versions. Examples: >>> normalizer = fit_normalizer ( feature_df , meta_data_df ) >>> normalizer . save ( \"output/normalizer.pkl\" ) To reload later: >>> with open ( \"output/normalizer.pkl\" , \"rb\" ) as f : ... normalizer = pickle . load ( f ) Source code in src/fisseq_data_pipeline/normalize.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def save ( self , save_path : PathLike ) -> None : \"\"\" Serialize and save the Normalizer object to disk using pickle. This method stores the fitted normalization statistics \u2014 per-feature means, standard deviations, and batch-wise configuration flag \u2014 as a single binary file that can later be reloaded to reproduce the same normalization behavior. Parameters ---------- save_path : PathLike Destination file path for the serialized Normalizer object. The file is written in binary format (typically named ``normalizer.pkl``). Notes ----- - The file can be reloaded using ``pickle.load(open(path, \"rb\"))``. - Only the Normalizer object and its attributes are serialized; any external references (e.g., LazyFrames) are not included. - The resulting file is Python-version dependent and not guaranteed to be portable across major interpreter versions. Examples -------- >>> normalizer = fit_normalizer(feature_df, meta_data_df) >>> normalizer.save(\"output/normalizer.pkl\") To reload later: >>> with open(\"output/normalizer.pkl\", \"rb\") as f: ... normalizer = pickle.load(f) \"\"\" with open ( save_path , \"wb\" ) as f : pickle . dump ( self , f ) fisseq_data_pipeline . normalize . fit_normalizer ( feature_df , meta_data_df = None , fit_only_on_control = False , fit_batch_wise = True ) Compute per-feature means and standard deviations for z-score normalization. The function can operate in two modes: * Global normalization - if fit_batch_wise=False , compute one mean and standard deviation per feature across all samples. * Batch-wise normalization - if fit_batch_wise=True , compute separate statistics for each batch defined by the _batch column of meta_data_df . Optionally, the statistics may be estimated only from control samples indicated by a boolean _is_control column in the metadata. Columns with (near) zero variance are automatically excluded from the resulting normalizer to avoid divide-by-zero errors during scaling. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix of shape (n_samples, n_features). Each column represents a quantitative feature to be normalized. meta_data_df ( DataFrame , default: None ) \u2013 Metadata DataFrame aligned row-wise with feature_df . Must contain a _batch column if fit_batch_wise=True , and an _is_control column if fit_only_on_control=True . fit_only_on_control ( bool , default: False ) \u2013 If True, compute normalization statistics using only rows where meta_data_df[\"_is_control\"] is True. fit_batch_wise ( bool , default: True ) \u2013 If True, compute means and standard deviations separately for each batch. Requires that meta_data_df include a _batch column. Returns: Normalizer \u2013 A dataclass containing the per-feature means, standard deviations, and optional batch mapping for use in downstream normalization. Source code in src/fisseq_data_pipeline/normalize.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def fit_normalizer ( feature_df : pl . LazyFrame , meta_data_df : Optional [ pl . LazyFrame ] = None , fit_only_on_control : bool = False , fit_batch_wise : bool = True , ) -> Normalizer : \"\"\" Compute per-feature means and standard deviations for z-score normalization. The function can operate in two modes: * **Global normalization** - if `fit_batch_wise=False`, compute one mean and standard deviation per feature across all samples. * **Batch-wise normalization** - if `fit_batch_wise=True`, compute separate statistics for each batch defined by the `_batch` column of `meta_data_df`. Optionally, the statistics may be estimated only from control samples indicated by a boolean `_is_control` column in the metadata. Columns with (near) zero variance are automatically excluded from the resulting normalizer to avoid divide-by-zero errors during scaling. Parameters ---------- feature_df : pl.DataFrame Feature matrix of shape (n_samples, n_features). Each column represents a quantitative feature to be normalized. meta_data_df : pl.DataFrame, optional Metadata DataFrame aligned row-wise with `feature_df`. Must contain a `_batch` column if `fit_batch_wise=True`, and an `_is_control` column if `fit_only_on_control=True`. fit_only_on_control : bool, default=False If True, compute normalization statistics using only rows where `meta_data_df[\"_is_control\"]` is True. fit_batch_wise : bool, default=True If True, compute means and standard deviations separately for each batch. Requires that `meta_data_df` include a `_batch` column. Returns ------- Normalizer A dataclass containing the per-feature means, standard deviations, and optional batch mapping for use in downstream normalization. \"\"\" if ( fit_only_on_control or fit_batch_wise ) and meta_data_df is None : raise ValueError ( \"Meta data required to fit to control samples or by batch\" ) if fit_only_on_control : logging . info ( \"Adding query to filter for control samples\" ) feature_df = ( pl . concat ( ( feature_df , meta_data_df . select ( pl . col ( \"_is_control\" ))), how = \"horizontal\" , ) . filter ( pl . col ( \"_is_control\" )) . select ( pl . exclude ( \"_is_control\" )) ) meta_data_df = meta_data_df . filter ( pl . col ( \"_is_control\" )) # Handle batch column (batch-wise vs global) feature_df = _add_batch_col ( feature_df , meta_data_df , fit_batch_wise ) agg_exprs = [] for c in feature_df . columns : if c == \"_batch\" : continue agg_exprs . append ( pl . col ( c ) . mean () . alias ( f \" { c } _mean\" )) agg_exprs . append ( pl . col ( c ) . std () . alias ( f \" { c } _std\" )) agg_df = feature_df . group_by ( \"_batch\" ) . agg ( agg_exprs ) mean_cols = [ c for c in agg_df . columns if c . endswith ( \"_mean\" )] std_cols = [ c for c in agg_df . columns if c . endswith ( \"_std\" )] means , stds = ( agg_df . select ( [ pl . col ( c ) . alias ( c . removesuffix ( sfx )) for c in cols ] + [ pl . col ( \"_batch\" )] ) . collect () for cols , sfx in [( mean_cols , \"_mean\" ), ( std_cols , \"_std\" )] ) zero_var_cols = [ c for c in stds . columns if c != \"_batch\" and ( stds [ c ] <= np . finfo ( np . float32 ) . eps ) . any () ] if zero_var_cols : logging . warning ( \"Dropping %d zero-variance columns\" , len ( zero_var_cols )) means = means . select ( pl . exclude ( zero_var_cols )) stds = stds . select ( pl . exclude ( zero_var_cols )) logging . info ( \"Normalization statistics computed successfully\" ) return Normalizer ( means = means , stds = stds , is_batch_wise = fit_batch_wise ) fisseq_data_pipeline . normalize . normalize ( feature_df , normalizer , meta_data_df = None ) Apply z-score normalization to a feature matrix. Each feature value is standardized as: z = (x - mean) / std where mean and std are drawn from the appropriate batch in normalizer.means and normalizer.stds . When the normalizer was fitted batch-wise, the _batch column in meta_data_df determines which row of the stored statistics applies to each sample. Columns absent from the normalizer (e.g., zero-variance columns that were dropped during fitting) are automatically removed prior to scaling. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix of shape (n_samples, n_features) to be normalized. normalizer ( Normalizer ) \u2013 Object containing the means, standard deviations, and batch mapping produced by :func: fit_normalizer . meta_data_df ( DataFrame , default: None ) \u2013 Metadata aligned row-wise with feature_df . Required when the normalizer was fitted batch-wise (i.e., normalizer.mapping is not None), and must include a _batch column. Returns: DataFrame \u2013 Normalized feature matrix of shape (n_samples, n_retained_features), where columns with zero variance during fitting are omitted. Source code in src/fisseq_data_pipeline/normalize.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def normalize ( feature_df : pl . LazyFrame , normalizer : Normalizer , meta_data_df : Optional [ pl . LazyFrame ] = None , ) -> pl . LazyFrame : \"\"\" Apply z-score normalization to a feature matrix. Each feature value is standardized as: z = (x - mean) / std where `mean` and `std` are drawn from the appropriate batch in `normalizer.means` and `normalizer.stds`. When the normalizer was fitted batch-wise, the `_batch` column in `meta_data_df` determines which row of the stored statistics applies to each sample. Columns absent from the normalizer (e.g., zero-variance columns that were dropped during fitting) are automatically removed prior to scaling. Parameters ---------- feature_df : pl.DataFrame Feature matrix of shape (n_samples, n_features) to be normalized. normalizer : Normalizer Object containing the means, standard deviations, and batch mapping produced by :func:`fit_normalizer`. meta_data_df : pl.DataFrame, optional Metadata aligned row-wise with `feature_df`. Required when the normalizer was fitted batch-wise (i.e., `normalizer.mapping` is not None), and must include a `_batch` column. Returns ------- pl.DataFrame Normalized feature matrix of shape (n_samples, n_retained_features), where columns with zero variance during fitting are omitted. \"\"\" if normalizer . is_batch_wise and meta_data_df is None : raise ValueError ( \"Meta data required to use batch-wise normalizer\" ) logging . info ( \"Creating normalization query\" ) feature_df = _add_batch_col ( feature_df , meta_data_df , normalizer . is_batch_wise ) feature_cols = feature_df . columns norm_cols = normalizer . stds . columns if set ( norm_cols ) != set ( feature_cols ): feature_df = feature_df . select ( norm_cols ) logging . warning ( \"Dropped %d columns from feature_df not present in normalizer\" , len ( feature_cols ) - len ( norm_cols ), ) for suffix , df in [( \"_mean\" , normalizer . means ), ( \"_std\" , normalizer . stds )]: feature_df = feature_df . join ( df . lazy (), on = \"_batch\" , how = \"left\" , suffix = suffix ) feature_df = feature_df . select ( [ (( pl . col ( c ) - pl . col ( f \" { c } _mean\" )) / pl . col ( f \" { c } _std\" )) . alias ( c ) for c in norm_cols if c != \"_batch\" ] ) return feature_df","title":"Normalize"},{"location":"normalize/#normalize","text":"The fisseq_data_pipeline.normalize module provides utilities for computing and applying z-score normalization to feature matrices. Normalization is typically run as part of the FISSEQ pipeline, but these functions can also be used independently when you need to standardize feature values.","title":"Normalize"},{"location":"normalize/#overview","text":"Normalizer : A dataclass container storing per-column means and standard deviations. fit_normalizer : Compute normalization statistics (means and stds) from a feature DataFrame, optionally restricted to control samples. normalize : Apply z-score normalization to a feature DataFrame using a fitted Normalizer .","title":"Overview"},{"location":"normalize/#example-usage","text":"import polars as pl from fisseq_data_pipeline.normalize import fit_normalizer, normalize # Example feature matrix feature_df = pl.DataFrame({ \"x\": [1.0, 2.0, 3.0], \"y\": [2.0, 4.0, 6.0], }) # Fit the normalizer normalizer = fit_normalizer(feature_df) # Apply normalization normalized_df = normalize(feature_df, normalizer) print(normalized_df)","title":"Example usage"},{"location":"normalize/#output","text":"shape: (3, 2) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 x \u2502 y \u2502 \u2502 --- \u2502 --- \u2502 \u2502 f64 \u2502 f64 \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 -1.0 \u2502 -1.0 \u2502 \u2502 0.0 \u2502 0.0 \u2502 \u2502 1.0 \u2502 1.0 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Output"},{"location":"normalize/#api-reference","text":"","title":"API reference"},{"location":"normalize/#fisseq_data_pipeline.normalize.Normalizer","text":"Container object storing per-feature normalization statistics. Attributes: means ( DataFrame ) \u2013 A DataFrame of shape (n_batches, n_features) containing the mean value of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). stds ( DataFrame ) \u2013 A DataFrame of shape (n_batches, n_features) containing the standard deviation of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). is_batch_wise ( dict [ str , int ] or None ) \u2013 Whether statistics were computed batch-wise or globally Source code in src/fisseq_data_pipeline/normalize.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @dataclasses . dataclass class Normalizer : \"\"\" Container object storing per-feature normalization statistics. Attributes ---------- means : pl.DataFrame A DataFrame of shape (n_batches, n_features) containing the mean value of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). stds : pl.DataFrame A DataFrame of shape (n_batches, n_features) containing the standard deviation of each feature for each batch. When batch-wise normalization is not used, this has shape (1, n_features). is_batch_wise : dict[str, int] or None Whether statistics were computed batch-wise or globally \"\"\" means : pl . DataFrame stds : pl . DataFrame is_batch_wise : bool def save ( self , save_path : PathLike ) -> None : \"\"\" Serialize and save the Normalizer object to disk using pickle. This method stores the fitted normalization statistics \u2014 per-feature means, standard deviations, and batch-wise configuration flag \u2014 as a single binary file that can later be reloaded to reproduce the same normalization behavior. Parameters ---------- save_path : PathLike Destination file path for the serialized Normalizer object. The file is written in binary format (typically named ``normalizer.pkl``). Notes ----- - The file can be reloaded using ``pickle.load(open(path, \"rb\"))``. - Only the Normalizer object and its attributes are serialized; any external references (e.g., LazyFrames) are not included. - The resulting file is Python-version dependent and not guaranteed to be portable across major interpreter versions. Examples -------- >>> normalizer = fit_normalizer(feature_df, meta_data_df) >>> normalizer.save(\"output/normalizer.pkl\") To reload later: >>> with open(\"output/normalizer.pkl\", \"rb\") as f: ... normalizer = pickle.load(f) \"\"\" with open ( save_path , \"wb\" ) as f : pickle . dump ( self , f )","title":"Normalizer"},{"location":"normalize/#fisseq_data_pipeline.normalize.Normalizer.save","text":"Serialize and save the Normalizer object to disk using pickle. This method stores the fitted normalization statistics \u2014 per-feature means, standard deviations, and batch-wise configuration flag \u2014 as a single binary file that can later be reloaded to reproduce the same normalization behavior. Parameters: save_path ( PathLike ) \u2013 Destination file path for the serialized Normalizer object. The file is written in binary format (typically named normalizer.pkl ). Notes The file can be reloaded using pickle.load(open(path, \"rb\")) . Only the Normalizer object and its attributes are serialized; any external references (e.g., LazyFrames) are not included. The resulting file is Python-version dependent and not guaranteed to be portable across major interpreter versions. Examples: >>> normalizer = fit_normalizer ( feature_df , meta_data_df ) >>> normalizer . save ( \"output/normalizer.pkl\" ) To reload later: >>> with open ( \"output/normalizer.pkl\" , \"rb\" ) as f : ... normalizer = pickle . load ( f ) Source code in src/fisseq_data_pipeline/normalize.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def save ( self , save_path : PathLike ) -> None : \"\"\" Serialize and save the Normalizer object to disk using pickle. This method stores the fitted normalization statistics \u2014 per-feature means, standard deviations, and batch-wise configuration flag \u2014 as a single binary file that can later be reloaded to reproduce the same normalization behavior. Parameters ---------- save_path : PathLike Destination file path for the serialized Normalizer object. The file is written in binary format (typically named ``normalizer.pkl``). Notes ----- - The file can be reloaded using ``pickle.load(open(path, \"rb\"))``. - Only the Normalizer object and its attributes are serialized; any external references (e.g., LazyFrames) are not included. - The resulting file is Python-version dependent and not guaranteed to be portable across major interpreter versions. Examples -------- >>> normalizer = fit_normalizer(feature_df, meta_data_df) >>> normalizer.save(\"output/normalizer.pkl\") To reload later: >>> with open(\"output/normalizer.pkl\", \"rb\") as f: ... normalizer = pickle.load(f) \"\"\" with open ( save_path , \"wb\" ) as f : pickle . dump ( self , f )","title":"save"},{"location":"normalize/#fisseq_data_pipeline.normalize.fit_normalizer","text":"Compute per-feature means and standard deviations for z-score normalization. The function can operate in two modes: * Global normalization - if fit_batch_wise=False , compute one mean and standard deviation per feature across all samples. * Batch-wise normalization - if fit_batch_wise=True , compute separate statistics for each batch defined by the _batch column of meta_data_df . Optionally, the statistics may be estimated only from control samples indicated by a boolean _is_control column in the metadata. Columns with (near) zero variance are automatically excluded from the resulting normalizer to avoid divide-by-zero errors during scaling. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix of shape (n_samples, n_features). Each column represents a quantitative feature to be normalized. meta_data_df ( DataFrame , default: None ) \u2013 Metadata DataFrame aligned row-wise with feature_df . Must contain a _batch column if fit_batch_wise=True , and an _is_control column if fit_only_on_control=True . fit_only_on_control ( bool , default: False ) \u2013 If True, compute normalization statistics using only rows where meta_data_df[\"_is_control\"] is True. fit_batch_wise ( bool , default: True ) \u2013 If True, compute means and standard deviations separately for each batch. Requires that meta_data_df include a _batch column. Returns: Normalizer \u2013 A dataclass containing the per-feature means, standard deviations, and optional batch mapping for use in downstream normalization. Source code in src/fisseq_data_pipeline/normalize.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def fit_normalizer ( feature_df : pl . LazyFrame , meta_data_df : Optional [ pl . LazyFrame ] = None , fit_only_on_control : bool = False , fit_batch_wise : bool = True , ) -> Normalizer : \"\"\" Compute per-feature means and standard deviations for z-score normalization. The function can operate in two modes: * **Global normalization** - if `fit_batch_wise=False`, compute one mean and standard deviation per feature across all samples. * **Batch-wise normalization** - if `fit_batch_wise=True`, compute separate statistics for each batch defined by the `_batch` column of `meta_data_df`. Optionally, the statistics may be estimated only from control samples indicated by a boolean `_is_control` column in the metadata. Columns with (near) zero variance are automatically excluded from the resulting normalizer to avoid divide-by-zero errors during scaling. Parameters ---------- feature_df : pl.DataFrame Feature matrix of shape (n_samples, n_features). Each column represents a quantitative feature to be normalized. meta_data_df : pl.DataFrame, optional Metadata DataFrame aligned row-wise with `feature_df`. Must contain a `_batch` column if `fit_batch_wise=True`, and an `_is_control` column if `fit_only_on_control=True`. fit_only_on_control : bool, default=False If True, compute normalization statistics using only rows where `meta_data_df[\"_is_control\"]` is True. fit_batch_wise : bool, default=True If True, compute means and standard deviations separately for each batch. Requires that `meta_data_df` include a `_batch` column. Returns ------- Normalizer A dataclass containing the per-feature means, standard deviations, and optional batch mapping for use in downstream normalization. \"\"\" if ( fit_only_on_control or fit_batch_wise ) and meta_data_df is None : raise ValueError ( \"Meta data required to fit to control samples or by batch\" ) if fit_only_on_control : logging . info ( \"Adding query to filter for control samples\" ) feature_df = ( pl . concat ( ( feature_df , meta_data_df . select ( pl . col ( \"_is_control\" ))), how = \"horizontal\" , ) . filter ( pl . col ( \"_is_control\" )) . select ( pl . exclude ( \"_is_control\" )) ) meta_data_df = meta_data_df . filter ( pl . col ( \"_is_control\" )) # Handle batch column (batch-wise vs global) feature_df = _add_batch_col ( feature_df , meta_data_df , fit_batch_wise ) agg_exprs = [] for c in feature_df . columns : if c == \"_batch\" : continue agg_exprs . append ( pl . col ( c ) . mean () . alias ( f \" { c } _mean\" )) agg_exprs . append ( pl . col ( c ) . std () . alias ( f \" { c } _std\" )) agg_df = feature_df . group_by ( \"_batch\" ) . agg ( agg_exprs ) mean_cols = [ c for c in agg_df . columns if c . endswith ( \"_mean\" )] std_cols = [ c for c in agg_df . columns if c . endswith ( \"_std\" )] means , stds = ( agg_df . select ( [ pl . col ( c ) . alias ( c . removesuffix ( sfx )) for c in cols ] + [ pl . col ( \"_batch\" )] ) . collect () for cols , sfx in [( mean_cols , \"_mean\" ), ( std_cols , \"_std\" )] ) zero_var_cols = [ c for c in stds . columns if c != \"_batch\" and ( stds [ c ] <= np . finfo ( np . float32 ) . eps ) . any () ] if zero_var_cols : logging . warning ( \"Dropping %d zero-variance columns\" , len ( zero_var_cols )) means = means . select ( pl . exclude ( zero_var_cols )) stds = stds . select ( pl . exclude ( zero_var_cols )) logging . info ( \"Normalization statistics computed successfully\" ) return Normalizer ( means = means , stds = stds , is_batch_wise = fit_batch_wise )","title":"fit_normalizer"},{"location":"normalize/#fisseq_data_pipeline.normalize.normalize","text":"Apply z-score normalization to a feature matrix. Each feature value is standardized as: z = (x - mean) / std where mean and std are drawn from the appropriate batch in normalizer.means and normalizer.stds . When the normalizer was fitted batch-wise, the _batch column in meta_data_df determines which row of the stored statistics applies to each sample. Columns absent from the normalizer (e.g., zero-variance columns that were dropped during fitting) are automatically removed prior to scaling. Parameters: feature_df ( DataFrame ) \u2013 Feature matrix of shape (n_samples, n_features) to be normalized. normalizer ( Normalizer ) \u2013 Object containing the means, standard deviations, and batch mapping produced by :func: fit_normalizer . meta_data_df ( DataFrame , default: None ) \u2013 Metadata aligned row-wise with feature_df . Required when the normalizer was fitted batch-wise (i.e., normalizer.mapping is not None), and must include a _batch column. Returns: DataFrame \u2013 Normalized feature matrix of shape (n_samples, n_retained_features), where columns with zero variance during fitting are omitted. Source code in src/fisseq_data_pipeline/normalize.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def normalize ( feature_df : pl . LazyFrame , normalizer : Normalizer , meta_data_df : Optional [ pl . LazyFrame ] = None , ) -> pl . LazyFrame : \"\"\" Apply z-score normalization to a feature matrix. Each feature value is standardized as: z = (x - mean) / std where `mean` and `std` are drawn from the appropriate batch in `normalizer.means` and `normalizer.stds`. When the normalizer was fitted batch-wise, the `_batch` column in `meta_data_df` determines which row of the stored statistics applies to each sample. Columns absent from the normalizer (e.g., zero-variance columns that were dropped during fitting) are automatically removed prior to scaling. Parameters ---------- feature_df : pl.DataFrame Feature matrix of shape (n_samples, n_features) to be normalized. normalizer : Normalizer Object containing the means, standard deviations, and batch mapping produced by :func:`fit_normalizer`. meta_data_df : pl.DataFrame, optional Metadata aligned row-wise with `feature_df`. Required when the normalizer was fitted batch-wise (i.e., `normalizer.mapping` is not None), and must include a `_batch` column. Returns ------- pl.DataFrame Normalized feature matrix of shape (n_samples, n_retained_features), where columns with zero variance during fitting are omitted. \"\"\" if normalizer . is_batch_wise and meta_data_df is None : raise ValueError ( \"Meta data required to use batch-wise normalizer\" ) logging . info ( \"Creating normalization query\" ) feature_df = _add_batch_col ( feature_df , meta_data_df , normalizer . is_batch_wise ) feature_cols = feature_df . columns norm_cols = normalizer . stds . columns if set ( norm_cols ) != set ( feature_cols ): feature_df = feature_df . select ( norm_cols ) logging . warning ( \"Dropped %d columns from feature_df not present in normalizer\" , len ( feature_cols ) - len ( norm_cols ), ) for suffix , df in [( \"_mean\" , normalizer . means ), ( \"_std\" , normalizer . stds )]: feature_df = feature_df . join ( df . lazy (), on = \"_batch\" , how = \"left\" , suffix = suffix ) feature_df = feature_df . select ( [ (( pl . col ( c ) - pl . col ( f \" { c } _mean\" )) / pl . col ( f \" { c } _std\" )) . alias ( c ) for c in norm_cols if c != \"_batch\" ] ) return feature_df","title":"normalize"},{"location":"pipeline/","text":"Pipeline The FISSEQ data pipeline exposes a small CLI via the entry point fisseq-data-pipeline . Subcommands are provided by Python Fire : validate \u2014 Train/validate on a stratified split and write outputs. run \u2014 Production, single-pass run (not yet implemented). configure \u2014 Write a default configuration file. Quick start # validate with explicit config and output directory fisseq-data-pipeline validate \\ --input_data_path data.parquet \\ --config config.yaml \\ --output_dir out \\ --test_size 0.2 \\ --write_train_results true Write a default config to the current directory fisseq-data-pipeline configure Logging FISSEQ_PIPELINE_LOG_LEVEL=debug fisseq-data-pipeline validate \\ --input_data_path data.parquet Command Interface Validate fisseq_data_pipeline . pipeline . validate ( input_data_path , config = None , output_dir = None , test_size = 0.2 , write_train_results = True , eager_db_loading = False ) Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps Load dataset, derive feature/metadata frames, and clean invalid rows/columns. Build a stratification vector from _batch and _label and perform a single stratified train/test split. Fit a normalizer on the training split; transform train and test. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters: input_data_path ( PathLike ) \u2013 Path to a Parquet file to scan and process. config ( Config or PathLike , default: None ) \u2013 Configuration object or path. Must define feature columns and the names of _batch , _label , and _is_control fields. output_dir ( PathLike , default: None ) \u2013 Output directory. Defaults to the current working directory. test_size ( float , default: 0.2 ) \u2013 Fraction of samples assigned to the test split. write_train_results ( bool , default: True ) \u2013 If True, also write the train split's unmodified/normalized/ harmonized outputs. eager_db_loading ( bool , default: False ) \u2013 If True, fully load the input Parquet file into memory eagerly using :func: polars.read_parquet . This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func: polars.scan_parquet , which minimizes memory usage but may incur slower disk I/O during computation. Outputs Written to output_dir : meta_data.test.parquet features.test.parquet normalized.test.parquet harmonized.test.parquet normalizer.pkl harmonizer.pkl If write_train_results=True : meta_data.train.parquet features.train.parquet normalized.train.parquet harmonized.train.parquet CLI Exposed via Fire at the fisseq-data-pipeline entry point, e.g.:: fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true Source code in src/fisseq_data_pipeline/pipeline.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def validate ( input_data_path : PathLike , config : Optional [ Config | PathLike ] = None , output_dir : Optional [ PathLike ] = None , test_size : float = 0.2 , write_train_results : bool = True , eager_db_loading : bool = False , ) -> None : \"\"\" Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps -------------- 1. Load dataset, derive feature/metadata frames, and clean invalid rows/columns. 2. Build a stratification vector from ``_batch`` and ``_label`` and perform a single stratified train/test split. 3. Fit a normalizer on the training split; transform train and test. 4. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). 5. Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters ---------- input_data_path : PathLike Path to a Parquet file to scan and process. config : Config or PathLike, optional Configuration object or path. Must define feature columns and the names of ``_batch``, ``_label``, and ``_is_control`` fields. output_dir : PathLike, optional Output directory. Defaults to the current working directory. test_size : float, default=0.2 Fraction of samples assigned to the test split. write_train_results : bool, default=True If True, also write the train split's unmodified/normalized/ harmonized outputs. eager_db_loading : bool, default=False If True, fully load the input Parquet file into memory eagerly using :func:`polars.read_parquet`. This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func:`polars.scan_parquet`, which minimizes memory usage but may incur slower disk I/O during computation. Outputs ------- Written to ``output_dir``: - ``meta_data.test.parquet`` - ``features.test.parquet`` - ``normalized.test.parquet`` - ``harmonized.test.parquet`` - ``normalizer.pkl`` - ``harmonizer.pkl`` If ``write_train_results=True``: - ``meta_data.train.parquet`` - ``features.train.parquet`` - ``normalized.train.parquet`` - ``harmonized.train.parquet`` CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point, e.g.:: ```bash fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true ``` \"\"\" setup_logging ( output_dir ) logging . info ( \"Starting validation with input path: %s \" , input_data_path ) data_df = get_db ( input_data_path , eager_db_loading ) output_dir = pathlib . Path . cwd () if output_dir is None else pathlib . Path ( output_dir ) logging . info ( \"Output directory set to: %s \" , output_dir ) logging . info ( \"Collecting data matrices\" ) config = Config ( config ) feature_df , meta_data_df = get_data_dfs ( data_df , config ) feature_df , meta_data_df = clean_data ( feature_df , meta_data_df ) train_feature_df , train_meta_df , test_feature_df , test_meta_df = train_test_split ( feature_df , meta_data_df , test_size = test_size ) logging . info ( \"Writing feature matrices\" ) test_meta_df . sink_parquet ( output_dir / \"meta_data.test.parquet\" ) test_feature_df . sink_parquet ( output_dir / \"features.test.parquet\" ) logging . info ( \"Fitting normalizer on train data\" ) normalizer = fit_normalizer ( train_feature_df , meta_data_df = train_meta_df , fit_only_on_control = True , ) logging . info ( \"Running normalizer on train/test data\" ) train_normalized_df = normalize ( train_feature_df , normalizer , meta_data_df = train_meta_df ) test_normalized_df = normalize ( test_feature_df , normalizer , meta_data_df = test_meta_df ) logging . info ( \"Writing normalizer outputs\" ) test_normalized_df . sink_parquet ( output_dir / \"normalized.test.parquet\" ) normalizer . save ( output_dir / \"normalizer.pkl\" ) if write_train_results : logging . info ( \"Writing train output up to the normalization stage\" ) train_meta_df . sink_parquet ( output_dir / \"meta_data.train.parquet\" ) train_feature_df . sink_parquet ( output_dir / \"features.train.parquet\" ) train_normalized_df . sink_parquet ( output_dir / \"normalized.train.parquet\" ) Run fisseq_data_pipeline . pipeline . run ( input_data_path , config = None , output_dir = None , eager_db_loading = False ) Run the full batch-correction pipeline. This function performs a one-pass processing workflow that reads a full dataset, cleans invalid rows and columns, fits normalization statistics (optionally batch-wise and on control samples), applies normalization, and writes the resulting cleaned and normalized outputs to disk. Production Pipeline steps Load and scan the input Parquet dataset into a Polars LazyFrame. Derive feature and metadata frames using configuration-specified column selections. Clean the dataset by removing: Columns that contain only non-finite (NaN/inf) values. Rows containing any non-finite feature values. Fit a batch-wise normalizer (computed from control samples only). Apply normalization to the full cleaned dataset. Write the cleaned, normalized, and fitted model artifacts to disk. Parameters: input_data_path ( PathLike ) \u2013 Path to the input Parquet file containing the full dataset to process. config ( Config or PathLike , default: None ) \u2013 Path to configuration output_dir ( PathLike , default: None ) \u2013 Directory to which cleaned and normalized outputs will be written. Defaults to the current working directory if not specified. eager_db_loading ( bool , default: False ) \u2013 If True, fully load the input Parquet file into memory eagerly using :func: polars.read_parquet . This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func: polars.scan_parquet , which minimizes memory usage but may incur slower disk I/O during computation. Outputs Written to output_dir : meta_data.parquet \u2014 cleaned metadata table. features.parquet \u2014 cleaned feature matrix. normalized.parquet \u2014 z-score normalized feature matrix. normalizer.pkl \u2014 serialized :class: Normalizer object containing per-feature mean and standard deviation statistics. CLI Exposed via Fire at the fisseq-data-pipeline entry point, e.g.:: fisseq-data-pipeline run --input_data_path data.parquet --config config.yaml --output_dir out Source code in src/fisseq_data_pipeline/pipeline.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def run ( input_data_path : PathLike , config : Optional [ Config | PathLike ] = None , output_dir : Optional [ PathLike ] = None , eager_db_loading : bool = False , ) -> None : \"\"\" Run the full batch-correction pipeline. This function performs a one-pass processing workflow that reads a full dataset, cleans invalid rows and columns, fits normalization statistics (optionally batch-wise and on control samples), applies normalization, and writes the resulting cleaned and normalized outputs to disk. Production Pipeline steps ------------------------- 1. Load and scan the input Parquet dataset into a Polars LazyFrame. 2. Derive feature and metadata frames using configuration-specified column selections. 3. Clean the dataset by removing: - Columns that contain only non-finite (NaN/inf) values. - Rows containing any non-finite feature values. 4. Fit a batch-wise normalizer (computed from control samples only). 5. Apply normalization to the full cleaned dataset. 6. Write the cleaned, normalized, and fitted model artifacts to disk. Parameters ---------- input_data_path : PathLike Path to the input Parquet file containing the full dataset to process. config : Config or PathLike, optional Path to configuration output_dir : PathLike, optional Directory to which cleaned and normalized outputs will be written. Defaults to the current working directory if not specified. eager_db_loading : bool, default=False If True, fully load the input Parquet file into memory eagerly using :func:`polars.read_parquet`. This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func:`polars.scan_parquet`, which minimizes memory usage but may incur slower disk I/O during computation. Outputs ------- Written to ``output_dir``: - ``meta_data.parquet`` \u2014 cleaned metadata table. - ``features.parquet`` \u2014 cleaned feature matrix. - ``normalized.parquet`` \u2014 z-score normalized feature matrix. - ``normalizer.pkl`` \u2014 serialized :class:`Normalizer` object containing per-feature mean and standard deviation statistics. CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point, e.g.:: ```bash fisseq-data-pipeline run --input_data_path data.parquet --config config.yaml --output_dir out ``` \"\"\" setup_logging ( output_dir ) logging . info ( \"Starting validation with input path: %s \" , input_data_path ) data_df = get_db ( input_data_path , eager_db_loading ) output_dir = pathlib . Path . cwd () if output_dir is None else pathlib . Path ( output_dir ) logging . info ( \"Output directory set to: %s \" , output_dir ) logging . info ( \"Collecting data matrices\" ) config = Config ( config ) feature_df , meta_data_df = get_data_dfs ( data_df , config ) logging . info ( \"Cleaning data\" ) feature_df , meta_data_df = clean_data ( feature_df , meta_data_df , stages = [ \"drop_cols_all_nonfinite\" , \"drop_rows_any_nonfinite\" , ], ) logging . info ( \"Saving cleaned data\" ) feature_df . sink_parquet ( output_dir / \"meta_data.parquet\" ) meta_data_df . sink_parquet ( output_dir / \"features.parquet\" ) logging . info ( \"Fitting normalizer\" ) normalizer = fit_normalizer ( feature_df , meta_data_df = meta_data_df , fit_batch_wise = True , fit_only_on_control = True , ) logging . info ( \"Saving normalizer\" ) normalizer . save ( output_dir / \"normalizer.pkl\" ) logging . info ( \"Normalizing data\" ) normalized_df = normalize ( feature_df , normalizer , meta_data_df = meta_data_df ) logging . info ( \"Saving normalized data\" ) normalized_df . sink_parquet ( output_dir / \"normalized.parquet\" ) options: show_signature: true show_signature_annotations: true show_source: true Configure fisseq_data_pipeline . pipeline . configure ( output_path = None ) Write a copy of the default configuration to output_path . Parameters: output_path ( PathLike , default: None ) \u2013 Target path for the configuration file. If None , writes config.yaml to the current working directory. Returns: None \u2013 CLI Exposed via Fire at the fisseq-data-pipeline entry point # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml Source code in src/fisseq_data_pipeline/pipeline.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def configure ( output_path : Optional [ PathLike ] = None ) -> None : \"\"\" Write a copy of the default configuration to ``output_path``. Parameters ---------- output_path : PathLike, optional Target path for the configuration file. If ``None``, writes ``config.yaml`` to the current working directory. Returns ------- None CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point ```bash # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml ``` \"\"\" if output_path is None : output_path = pathlib . Path . cwd () / \"config.yaml\" shutil . copy ( DEFAULT_CFG_PATH , output_path ) options: show_signature: true show_signature_annotations: true show_source: true Auxiliary functions This functions are not exposed to the command line, and are for internal use only. fisseq_data_pipeline . pipeline . setup_logging ( log_dir = None ) Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable FISSEQ_PIPELINE_LOG_LEVEL (default: \"info\" ). Parameters: log_dir ( PathLike , default: None ) \u2013 Directory where log files will be written. If None , the current working directory is used. Source code in src/fisseq_data_pipeline/pipeline.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def setup_logging ( log_dir : Optional [ PathLike ] = None ) -> None : \"\"\" Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable ``FISSEQ_PIPELINE_LOG_LEVEL`` (default: ``\"info\"``). Parameters ---------- log_dir : PathLike, optional Directory where log files will be written. If ``None``, the current working directory is used. \"\"\" log_levels = { \"debug\" : logging . DEBUG , \"info\" : logging . INFO , \"warning\" : logging . WARNING , \"error\" : logging . ERROR , \"critical\" : logging . CRITICAL , } if log_dir is None : log_dir = pathlib . Path . cwd () else : log_dir = pathlib . Path ( log_dir ) dt_str = datetime . datetime . now () . strftime ( \"%Y%m %d :%H%M%S\" ) filename = f \"fisseq-data-pipeline- { dt_str } .log\" log_path = log_dir / filename handlers = [ logging . StreamHandler (), logging . FileHandler ( log_path , mode = \"w\" )] log_level = os . getenv ( \"FISSEQ_PIPELINE_LOG_LEVEL\" , \"info\" ) log_level = log_levels . get ( log_level , logging . INFO ) logging . basicConfig ( level = log_level , format = \" %(asctime)s [ %(levelname)s ] [ %(funcName)s ] %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" , handlers = handlers , ) fisseq_data_pipeline . pipeline . main () CLI entry that registers Fire subcommands. Subcommands validate : Train/validate on a stratified split and write outputs. run : Production, single-pass run (not yet implemented). configure : Write a default configuration file. CLI Invoked as the fisseq-data-pipeline console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet Source code in src/fisseq_data_pipeline/pipeline.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def main () -> None : \"\"\" CLI entry that registers Fire subcommands. Subcommands ----------- - ``validate`` : Train/validate on a stratified split and write outputs. - ``run`` : Production, single-pass run (not yet implemented). - ``configure`` : Write a default configuration file. CLI --- Invoked as the ``fisseq-data-pipeline`` console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet \"\"\" try : fire . Fire ({ \"validate\" : validate , \"run\" : run , \"configure\" : configure }) except : logging . exception ( \"Run failed due to the following exception:\" ) raise fisseq_data_pipeline . pipeline . get_db ( input_data_path , eager ) Load or scan a Parquet database as a Polars LazyFrame. Depending on the eager flag, this function either reads the entire Parquet file into memory eagerly or sets up a lazy scan that defers I/O until query execution. The result is always returned as a :class: polars.LazyFrame for downstream pipeline compatibility. Parameters: input_data_path ( PathLike ) \u2013 Path to the Parquet file containing the dataset to be processed. eager ( bool ) \u2013 If True, fully load the dataset into memory using :func: polars.read_parquet and immediately convert it to a lazy frame. This avoids repeated on-disk scans and can improve runtime on systems with sufficient memory. If False, construct a lazy, on-demand scan using :func: polars.scan_parquet , which minimizes memory usage but may be slower due to disk I/O during execution. Returns: LazyFrame \u2013 A lazy Polars representation of the dataset, ready for further transformations. Notes Eager loading is advantageous for repeated access or complex queries over a moderate-sized dataset that fits comfortably in RAM. Lazy scanning is preferred for very large datasets or when operating in a memory-constrained environment. Examples: >>> lf = get_db ( \"features.parquet\" , eager = False ) >>> lf = get_db ( \"features.parquet\" , eager = True ) Source code in src/fisseq_data_pipeline/pipeline.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_db ( input_data_path : PathLike , eager : bool ) -> pl . LazyFrame : \"\"\" Load or scan a Parquet database as a Polars LazyFrame. Depending on the ``eager`` flag, this function either reads the entire Parquet file into memory eagerly or sets up a lazy scan that defers I/O until query execution. The result is always returned as a :class:`polars.LazyFrame` for downstream pipeline compatibility. Parameters ---------- input_data_path : PathLike Path to the Parquet file containing the dataset to be processed. eager : bool If True, fully load the dataset into memory using :func:`polars.read_parquet` and immediately convert it to a lazy frame. This avoids repeated on-disk scans and can improve runtime on systems with sufficient memory. If False, construct a lazy, on-demand scan using :func:`polars.scan_parquet`, which minimizes memory usage but may be slower due to disk I/O during execution. Returns ------- polars.LazyFrame A lazy Polars representation of the dataset, ready for further transformations. Notes ----- - Eager loading is advantageous for repeated access or complex queries over a moderate-sized dataset that fits comfortably in RAM. - Lazy scanning is preferred for very large datasets or when operating in a memory-constrained environment. Examples -------- >>> lf = get_db(\"features.parquet\", eager=False) >>> lf = get_db(\"features.parquet\", eager=True) \"\"\" if eager : logging . info ( \"Eagerly loading database: %s \" , input_data_path ) data_df = pl . read_parquet ( input_data_path ) . lazy () else : logging . info ( \"Scanning database: %s \" , input_data_path ) data_df = pl . scan_parquet ( input_data_path ) logging . info ( \"Finished %s database.\" , \"loading\" if eager else \"scanning\" ) return data_df","title":"Pipeline"},{"location":"pipeline/#pipeline","text":"The FISSEQ data pipeline exposes a small CLI via the entry point fisseq-data-pipeline . Subcommands are provided by Python Fire : validate \u2014 Train/validate on a stratified split and write outputs. run \u2014 Production, single-pass run (not yet implemented). configure \u2014 Write a default configuration file.","title":"Pipeline"},{"location":"pipeline/#quick-start","text":"# validate with explicit config and output directory fisseq-data-pipeline validate \\ --input_data_path data.parquet \\ --config config.yaml \\ --output_dir out \\ --test_size 0.2 \\ --write_train_results true","title":"Quick start"},{"location":"pipeline/#write-a-default-config-to-the-current-directory","text":"fisseq-data-pipeline configure","title":"Write a default config to the current directory"},{"location":"pipeline/#logging","text":"FISSEQ_PIPELINE_LOG_LEVEL=debug fisseq-data-pipeline validate \\ --input_data_path data.parquet","title":"Logging"},{"location":"pipeline/#command-interface","text":"","title":"Command Interface"},{"location":"pipeline/#validate","text":"","title":"Validate"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.validate","text":"Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps Load dataset, derive feature/metadata frames, and clean invalid rows/columns. Build a stratification vector from _batch and _label and perform a single stratified train/test split. Fit a normalizer on the training split; transform train and test. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters: input_data_path ( PathLike ) \u2013 Path to a Parquet file to scan and process. config ( Config or PathLike , default: None ) \u2013 Configuration object or path. Must define feature columns and the names of _batch , _label , and _is_control fields. output_dir ( PathLike , default: None ) \u2013 Output directory. Defaults to the current working directory. test_size ( float , default: 0.2 ) \u2013 Fraction of samples assigned to the test split. write_train_results ( bool , default: True ) \u2013 If True, also write the train split's unmodified/normalized/ harmonized outputs. eager_db_loading ( bool , default: False ) \u2013 If True, fully load the input Parquet file into memory eagerly using :func: polars.read_parquet . This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func: polars.scan_parquet , which minimizes memory usage but may incur slower disk I/O during computation. Outputs Written to output_dir : meta_data.test.parquet features.test.parquet normalized.test.parquet harmonized.test.parquet normalizer.pkl harmonizer.pkl If write_train_results=True : meta_data.train.parquet features.train.parquet normalized.train.parquet harmonized.train.parquet CLI Exposed via Fire at the fisseq-data-pipeline entry point, e.g.:: fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true Source code in src/fisseq_data_pipeline/pipeline.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def validate ( input_data_path : PathLike , config : Optional [ Config | PathLike ] = None , output_dir : Optional [ PathLike ] = None , test_size : float = 0.2 , write_train_results : bool = True , eager_db_loading : bool = False , ) -> None : \"\"\" Train pipeline parameters and run on a stratified train/test split. Validation Pipeline steps -------------- 1. Load dataset, derive feature/metadata frames, and clean invalid rows/columns. 2. Build a stratification vector from ``_batch`` and ``_label`` and perform a single stratified train/test split. 3. Fit a normalizer on the training split; transform train and test. 4. Fit ComBat harmonizer on normalized training data; apply to the normalized test (and optionally train). 5. Write unmodified, normalized, and harmonized Parquet outputs, and save fitted models. Parameters ---------- input_data_path : PathLike Path to a Parquet file to scan and process. config : Config or PathLike, optional Configuration object or path. Must define feature columns and the names of ``_batch``, ``_label``, and ``_is_control`` fields. output_dir : PathLike, optional Output directory. Defaults to the current working directory. test_size : float, default=0.2 Fraction of samples assigned to the test split. write_train_results : bool, default=True If True, also write the train split's unmodified/normalized/ harmonized outputs. eager_db_loading : bool, default=False If True, fully load the input Parquet file into memory eagerly using :func:`polars.read_parquet`. This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func:`polars.scan_parquet`, which minimizes memory usage but may incur slower disk I/O during computation. Outputs ------- Written to ``output_dir``: - ``meta_data.test.parquet`` - ``features.test.parquet`` - ``normalized.test.parquet`` - ``harmonized.test.parquet`` - ``normalizer.pkl`` - ``harmonizer.pkl`` If ``write_train_results=True``: - ``meta_data.train.parquet`` - ``features.train.parquet`` - ``normalized.train.parquet`` - ``harmonized.train.parquet`` CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point, e.g.:: ```bash fisseq-data-pipeline validate --input_data_path data.parquet --config config.yaml --output_dir out --test_size 0.2 --write_train_results true ``` \"\"\" setup_logging ( output_dir ) logging . info ( \"Starting validation with input path: %s \" , input_data_path ) data_df = get_db ( input_data_path , eager_db_loading ) output_dir = pathlib . Path . cwd () if output_dir is None else pathlib . Path ( output_dir ) logging . info ( \"Output directory set to: %s \" , output_dir ) logging . info ( \"Collecting data matrices\" ) config = Config ( config ) feature_df , meta_data_df = get_data_dfs ( data_df , config ) feature_df , meta_data_df = clean_data ( feature_df , meta_data_df ) train_feature_df , train_meta_df , test_feature_df , test_meta_df = train_test_split ( feature_df , meta_data_df , test_size = test_size ) logging . info ( \"Writing feature matrices\" ) test_meta_df . sink_parquet ( output_dir / \"meta_data.test.parquet\" ) test_feature_df . sink_parquet ( output_dir / \"features.test.parquet\" ) logging . info ( \"Fitting normalizer on train data\" ) normalizer = fit_normalizer ( train_feature_df , meta_data_df = train_meta_df , fit_only_on_control = True , ) logging . info ( \"Running normalizer on train/test data\" ) train_normalized_df = normalize ( train_feature_df , normalizer , meta_data_df = train_meta_df ) test_normalized_df = normalize ( test_feature_df , normalizer , meta_data_df = test_meta_df ) logging . info ( \"Writing normalizer outputs\" ) test_normalized_df . sink_parquet ( output_dir / \"normalized.test.parquet\" ) normalizer . save ( output_dir / \"normalizer.pkl\" ) if write_train_results : logging . info ( \"Writing train output up to the normalization stage\" ) train_meta_df . sink_parquet ( output_dir / \"meta_data.train.parquet\" ) train_feature_df . sink_parquet ( output_dir / \"features.train.parquet\" ) train_normalized_df . sink_parquet ( output_dir / \"normalized.train.parquet\" )","title":"validate"},{"location":"pipeline/#run","text":"","title":"Run"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.run","text":"Run the full batch-correction pipeline. This function performs a one-pass processing workflow that reads a full dataset, cleans invalid rows and columns, fits normalization statistics (optionally batch-wise and on control samples), applies normalization, and writes the resulting cleaned and normalized outputs to disk. Production Pipeline steps Load and scan the input Parquet dataset into a Polars LazyFrame. Derive feature and metadata frames using configuration-specified column selections. Clean the dataset by removing: Columns that contain only non-finite (NaN/inf) values. Rows containing any non-finite feature values. Fit a batch-wise normalizer (computed from control samples only). Apply normalization to the full cleaned dataset. Write the cleaned, normalized, and fitted model artifacts to disk. Parameters: input_data_path ( PathLike ) \u2013 Path to the input Parquet file containing the full dataset to process. config ( Config or PathLike , default: None ) \u2013 Path to configuration output_dir ( PathLike , default: None ) \u2013 Directory to which cleaned and normalized outputs will be written. Defaults to the current working directory if not specified. eager_db_loading ( bool , default: False ) \u2013 If True, fully load the input Parquet file into memory eagerly using :func: polars.read_parquet . This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func: polars.scan_parquet , which minimizes memory usage but may incur slower disk I/O during computation. Outputs Written to output_dir : meta_data.parquet \u2014 cleaned metadata table. features.parquet \u2014 cleaned feature matrix. normalized.parquet \u2014 z-score normalized feature matrix. normalizer.pkl \u2014 serialized :class: Normalizer object containing per-feature mean and standard deviation statistics. CLI Exposed via Fire at the fisseq-data-pipeline entry point, e.g.:: fisseq-data-pipeline run --input_data_path data.parquet --config config.yaml --output_dir out Source code in src/fisseq_data_pipeline/pipeline.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def run ( input_data_path : PathLike , config : Optional [ Config | PathLike ] = None , output_dir : Optional [ PathLike ] = None , eager_db_loading : bool = False , ) -> None : \"\"\" Run the full batch-correction pipeline. This function performs a one-pass processing workflow that reads a full dataset, cleans invalid rows and columns, fits normalization statistics (optionally batch-wise and on control samples), applies normalization, and writes the resulting cleaned and normalized outputs to disk. Production Pipeline steps ------------------------- 1. Load and scan the input Parquet dataset into a Polars LazyFrame. 2. Derive feature and metadata frames using configuration-specified column selections. 3. Clean the dataset by removing: - Columns that contain only non-finite (NaN/inf) values. - Rows containing any non-finite feature values. 4. Fit a batch-wise normalizer (computed from control samples only). 5. Apply normalization to the full cleaned dataset. 6. Write the cleaned, normalized, and fitted model artifacts to disk. Parameters ---------- input_data_path : PathLike Path to the input Parquet file containing the full dataset to process. config : Config or PathLike, optional Path to configuration output_dir : PathLike, optional Directory to which cleaned and normalized outputs will be written. Defaults to the current working directory if not specified. eager_db_loading : bool, default=False If True, fully load the input Parquet file into memory eagerly using :func:`polars.read_parquet`. This avoids repeated on-disk scans and can significantly speed up processing on systems with sufficient RAM. If False (default), the dataset is accessed lazily using :func:`polars.scan_parquet`, which minimizes memory usage but may incur slower disk I/O during computation. Outputs ------- Written to ``output_dir``: - ``meta_data.parquet`` \u2014 cleaned metadata table. - ``features.parquet`` \u2014 cleaned feature matrix. - ``normalized.parquet`` \u2014 z-score normalized feature matrix. - ``normalizer.pkl`` \u2014 serialized :class:`Normalizer` object containing per-feature mean and standard deviation statistics. CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point, e.g.:: ```bash fisseq-data-pipeline run --input_data_path data.parquet --config config.yaml --output_dir out ``` \"\"\" setup_logging ( output_dir ) logging . info ( \"Starting validation with input path: %s \" , input_data_path ) data_df = get_db ( input_data_path , eager_db_loading ) output_dir = pathlib . Path . cwd () if output_dir is None else pathlib . Path ( output_dir ) logging . info ( \"Output directory set to: %s \" , output_dir ) logging . info ( \"Collecting data matrices\" ) config = Config ( config ) feature_df , meta_data_df = get_data_dfs ( data_df , config ) logging . info ( \"Cleaning data\" ) feature_df , meta_data_df = clean_data ( feature_df , meta_data_df , stages = [ \"drop_cols_all_nonfinite\" , \"drop_rows_any_nonfinite\" , ], ) logging . info ( \"Saving cleaned data\" ) feature_df . sink_parquet ( output_dir / \"meta_data.parquet\" ) meta_data_df . sink_parquet ( output_dir / \"features.parquet\" ) logging . info ( \"Fitting normalizer\" ) normalizer = fit_normalizer ( feature_df , meta_data_df = meta_data_df , fit_batch_wise = True , fit_only_on_control = True , ) logging . info ( \"Saving normalizer\" ) normalizer . save ( output_dir / \"normalizer.pkl\" ) logging . info ( \"Normalizing data\" ) normalized_df = normalize ( feature_df , normalizer , meta_data_df = meta_data_df ) logging . info ( \"Saving normalized data\" ) normalized_df . sink_parquet ( output_dir / \"normalized.parquet\" ) options: show_signature: true show_signature_annotations: true show_source: true","title":"run"},{"location":"pipeline/#configure","text":"","title":"Configure"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.configure","text":"Write a copy of the default configuration to output_path . Parameters: output_path ( PathLike , default: None ) \u2013 Target path for the configuration file. If None , writes config.yaml to the current working directory. Returns: None \u2013 CLI Exposed via Fire at the fisseq-data-pipeline entry point # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml Source code in src/fisseq_data_pipeline/pipeline.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 def configure ( output_path : Optional [ PathLike ] = None ) -> None : \"\"\" Write a copy of the default configuration to ``output_path``. Parameters ---------- output_path : PathLike, optional Target path for the configuration file. If ``None``, writes ``config.yaml`` to the current working directory. Returns ------- None CLI --- Exposed via Fire at the ``fisseq-data-pipeline`` entry point ```bash # Write config.yaml to CWD fisseq-data-pipeline configure # Write to a custom location fisseq-data-pipeline configure --output_path path/to/config.yaml ``` \"\"\" if output_path is None : output_path = pathlib . Path . cwd () / \"config.yaml\" shutil . copy ( DEFAULT_CFG_PATH , output_path ) options: show_signature: true show_signature_annotations: true show_source: true","title":"configure"},{"location":"pipeline/#auxiliary-functions","text":"This functions are not exposed to the command line, and are for internal use only.","title":"Auxiliary functions"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.setup_logging","text":"Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable FISSEQ_PIPELINE_LOG_LEVEL (default: \"info\" ). Parameters: log_dir ( PathLike , default: None ) \u2013 Directory where log files will be written. If None , the current working directory is used. Source code in src/fisseq_data_pipeline/pipeline.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def setup_logging ( log_dir : Optional [ PathLike ] = None ) -> None : \"\"\" Configure logging for the pipeline. A log file and a console stream are set up simultaneously. The log file is created in the specified directory (or the current working directory by default) with a timestamped filename. The log level is controlled by the environment variable ``FISSEQ_PIPELINE_LOG_LEVEL`` (default: ``\"info\"``). Parameters ---------- log_dir : PathLike, optional Directory where log files will be written. If ``None``, the current working directory is used. \"\"\" log_levels = { \"debug\" : logging . DEBUG , \"info\" : logging . INFO , \"warning\" : logging . WARNING , \"error\" : logging . ERROR , \"critical\" : logging . CRITICAL , } if log_dir is None : log_dir = pathlib . Path . cwd () else : log_dir = pathlib . Path ( log_dir ) dt_str = datetime . datetime . now () . strftime ( \"%Y%m %d :%H%M%S\" ) filename = f \"fisseq-data-pipeline- { dt_str } .log\" log_path = log_dir / filename handlers = [ logging . StreamHandler (), logging . FileHandler ( log_path , mode = \"w\" )] log_level = os . getenv ( \"FISSEQ_PIPELINE_LOG_LEVEL\" , \"info\" ) log_level = log_levels . get ( log_level , logging . INFO ) logging . basicConfig ( level = log_level , format = \" %(asctime)s [ %(levelname)s ] [ %(funcName)s ] %(message)s \" , datefmt = \"%Y-%m- %d %H:%M:%S\" , handlers = handlers , )","title":"setup_logging"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.main","text":"CLI entry that registers Fire subcommands. Subcommands validate : Train/validate on a stratified split and write outputs. run : Production, single-pass run (not yet implemented). configure : Write a default configuration file. CLI Invoked as the fisseq-data-pipeline console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet Source code in src/fisseq_data_pipeline/pipeline.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def main () -> None : \"\"\" CLI entry that registers Fire subcommands. Subcommands ----------- - ``validate`` : Train/validate on a stratified split and write outputs. - ``run`` : Production, single-pass run (not yet implemented). - ``configure`` : Write a default configuration file. CLI --- Invoked as the ``fisseq-data-pipeline`` console script. For example:: fisseq-data-pipeline validate --input_data_path data.parquet \"\"\" try : fire . Fire ({ \"validate\" : validate , \"run\" : run , \"configure\" : configure }) except : logging . exception ( \"Run failed due to the following exception:\" ) raise","title":"main"},{"location":"pipeline/#fisseq_data_pipeline.pipeline.get_db","text":"Load or scan a Parquet database as a Polars LazyFrame. Depending on the eager flag, this function either reads the entire Parquet file into memory eagerly or sets up a lazy scan that defers I/O until query execution. The result is always returned as a :class: polars.LazyFrame for downstream pipeline compatibility. Parameters: input_data_path ( PathLike ) \u2013 Path to the Parquet file containing the dataset to be processed. eager ( bool ) \u2013 If True, fully load the dataset into memory using :func: polars.read_parquet and immediately convert it to a lazy frame. This avoids repeated on-disk scans and can improve runtime on systems with sufficient memory. If False, construct a lazy, on-demand scan using :func: polars.scan_parquet , which minimizes memory usage but may be slower due to disk I/O during execution. Returns: LazyFrame \u2013 A lazy Polars representation of the dataset, ready for further transformations. Notes Eager loading is advantageous for repeated access or complex queries over a moderate-sized dataset that fits comfortably in RAM. Lazy scanning is preferred for very large datasets or when operating in a memory-constrained environment. Examples: >>> lf = get_db ( \"features.parquet\" , eager = False ) >>> lf = get_db ( \"features.parquet\" , eager = True ) Source code in src/fisseq_data_pipeline/pipeline.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_db ( input_data_path : PathLike , eager : bool ) -> pl . LazyFrame : \"\"\" Load or scan a Parquet database as a Polars LazyFrame. Depending on the ``eager`` flag, this function either reads the entire Parquet file into memory eagerly or sets up a lazy scan that defers I/O until query execution. The result is always returned as a :class:`polars.LazyFrame` for downstream pipeline compatibility. Parameters ---------- input_data_path : PathLike Path to the Parquet file containing the dataset to be processed. eager : bool If True, fully load the dataset into memory using :func:`polars.read_parquet` and immediately convert it to a lazy frame. This avoids repeated on-disk scans and can improve runtime on systems with sufficient memory. If False, construct a lazy, on-demand scan using :func:`polars.scan_parquet`, which minimizes memory usage but may be slower due to disk I/O during execution. Returns ------- polars.LazyFrame A lazy Polars representation of the dataset, ready for further transformations. Notes ----- - Eager loading is advantageous for repeated access or complex queries over a moderate-sized dataset that fits comfortably in RAM. - Lazy scanning is preferred for very large datasets or when operating in a memory-constrained environment. Examples -------- >>> lf = get_db(\"features.parquet\", eager=False) >>> lf = get_db(\"features.parquet\", eager=True) \"\"\" if eager : logging . info ( \"Eagerly loading database: %s \" , input_data_path ) data_df = pl . read_parquet ( input_data_path ) . lazy () else : logging . info ( \"Scanning database: %s \" , input_data_path ) data_df = pl . scan_parquet ( input_data_path ) logging . info ( \"Finished %s database.\" , \"loading\" if eager else \"scanning\" ) return data_df","title":"get_db"},{"location":"utils/config/","text":"Configuration utilities The fisseq_data_pipeline.utils.config module provides a Config object for managing pipeline configuration. It allows loading configuration values from YAML files, Python dictionaries, or other Config objects, and ensures that all values are validated against a default configuration. Overview Config : A wrapper around a validated configuration dictionary. Loads from a path, dictionary, Config , or falls back to the default config.yaml . Allows access via both attribute-style ( cfg.feature_cols ) and dictionary-style ( cfg[\"feature_cols\"] ). Automatically fills in missing keys from the default configuration and removes invalid keys. DEFAULT_CFG_PATH : The path to the default configuration YAML file that ships with the pipeline. Example usage from fisseq_data_pipeline.utils.config import Config # Load default configuration cfg = Config(None) # Load from a YAML file cfg = Config(\"my_config.yaml\") # Load from a Python dict cfg = Config({\"feature_cols\": [\"f1\", \"f2\"], \"_batch\": \"batch\"}) # Load from an existing Config cfg2 = Config(cfg) # Access values print(cfg.feature_cols) print(cfg[\"_batch\"]) Validation Behavior When initializing a Config : Invalid keys not present in the default configuration are removed with a warning. Missing keys are filled with the default values from config.yaml. This ensures that the configuration is always complete and consistent with the pipeline defaults. API Reference fisseq_data_pipeline.utils.config.Config A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another Config instance. If no configuration is provided, the default configuration file is used. Parameters: config ( PathLike or dict or Config ) \u2013 If None , the default configuration file path is used. If a dict , the dictionary is validated and used directly. If a PathLike , the configuration is loaded from the YAML file. If a Config , the underlying configuration data is reused. Source code in src/fisseq_data_pipeline/utils/config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Config : \"\"\" A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another ``Config`` instance. If no configuration is provided, the default configuration file is used. Parameters ---------- config : PathLike or dict or Config, optional - If ``None``, the default configuration file path is used. - If a ``dict``, the dictionary is validated and used directly. - If a ``PathLike``, the configuration is loaded from the YAML file. - If a ``Config``, the underlying configuration data is reused. \"\"\" def __init__ ( self , config : Optional [ PathLike | ConfigDict | \"Config\" ]): if config is None : logging . info ( \"No config provided, using default config\" ) config = DEFAULT_CFG_PATH if isinstance ( config , Config ): data = config . _data else : if isinstance ( config , dict ): data = config else : config = pathlib . Path ( config ) with config . open ( \"r\" ) as f : data = yaml . safe_load ( f ) data = self . _verify_config ( data ) logging . debug ( \"Using config %s \" , config ) self . _data = data def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ] def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key ) def _verify_config ( self , cfg_data : ConfigDict ) -> ConfigDict : \"\"\" Verify the provided configuration against the default configuration. Invalid keys are removed, and missing keys are filled with defaults. Parameters ---------- cfg_data : dict The configuration data to verify. Returns ------- dict The validated configuration dictionary with defaults applied. \"\"\" with DEFAULT_CFG_PATH . open ( \"r\" ) as f : default_data = yaml . safe_load ( f ) for key in list ( cfg_data . keys ()): if key in default_data : continue logging . warning ( \"Removing invalid config option %s from provided config\" , key ) del cfg_data [ key ] for key in list ( default_data . keys ()): if key in cfg_data : continue logging . warning ( \"Key %s not in provided config using default value of %s \" , key , default_data [ key ], ) cfg_data [ key ] = default_data [ key ] return cfg_data __getattr__ ( name ) Retrieve a configuration value as an attribute. Source code in src/fisseq_data_pipeline/utils/config.py 47 48 49 def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ] __getitem__ ( key ) Retrieve a configuration value using dictionary-style indexing. Source code in src/fisseq_data_pipeline/utils/config.py 51 52 53 def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key ) fisseq_data_pipeline . utils . config . DEFAULT_CFG_PATH = pathlib . Path ( __file__ ) . parent . parent / 'config.yaml' module-attribute","title":"Config"},{"location":"utils/config/#configuration-utilities","text":"The fisseq_data_pipeline.utils.config module provides a Config object for managing pipeline configuration. It allows loading configuration values from YAML files, Python dictionaries, or other Config objects, and ensures that all values are validated against a default configuration.","title":"Configuration utilities"},{"location":"utils/config/#overview","text":"Config : A wrapper around a validated configuration dictionary. Loads from a path, dictionary, Config , or falls back to the default config.yaml . Allows access via both attribute-style ( cfg.feature_cols ) and dictionary-style ( cfg[\"feature_cols\"] ). Automatically fills in missing keys from the default configuration and removes invalid keys. DEFAULT_CFG_PATH : The path to the default configuration YAML file that ships with the pipeline.","title":"Overview"},{"location":"utils/config/#example-usage","text":"from fisseq_data_pipeline.utils.config import Config # Load default configuration cfg = Config(None) # Load from a YAML file cfg = Config(\"my_config.yaml\") # Load from a Python dict cfg = Config({\"feature_cols\": [\"f1\", \"f2\"], \"_batch\": \"batch\"}) # Load from an existing Config cfg2 = Config(cfg) # Access values print(cfg.feature_cols) print(cfg[\"_batch\"])","title":"Example usage"},{"location":"utils/config/#validation-behavior","text":"When initializing a Config : Invalid keys not present in the default configuration are removed with a warning. Missing keys are filled with the default values from config.yaml. This ensures that the configuration is always complete and consistent with the pipeline defaults.","title":"Validation Behavior"},{"location":"utils/config/#api-reference","text":"","title":"API Reference"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.Config","text":"A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another Config instance. If no configuration is provided, the default configuration file is used. Parameters: config ( PathLike or dict or Config ) \u2013 If None , the default configuration file path is used. If a dict , the dictionary is validated and used directly. If a PathLike , the configuration is loaded from the YAML file. If a Config , the underlying configuration data is reused. Source code in src/fisseq_data_pipeline/utils/config.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class Config : \"\"\" A configuration object that wraps a dictionary of key-value pairs loaded from a provided path, dictionary, or another ``Config`` instance. If no configuration is provided, the default configuration file is used. Parameters ---------- config : PathLike or dict or Config, optional - If ``None``, the default configuration file path is used. - If a ``dict``, the dictionary is validated and used directly. - If a ``PathLike``, the configuration is loaded from the YAML file. - If a ``Config``, the underlying configuration data is reused. \"\"\" def __init__ ( self , config : Optional [ PathLike | ConfigDict | \"Config\" ]): if config is None : logging . info ( \"No config provided, using default config\" ) config = DEFAULT_CFG_PATH if isinstance ( config , Config ): data = config . _data else : if isinstance ( config , dict ): data = config else : config = pathlib . Path ( config ) with config . open ( \"r\" ) as f : data = yaml . safe_load ( f ) data = self . _verify_config ( data ) logging . debug ( \"Using config %s \" , config ) self . _data = data def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ] def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key ) def _verify_config ( self , cfg_data : ConfigDict ) -> ConfigDict : \"\"\" Verify the provided configuration against the default configuration. Invalid keys are removed, and missing keys are filled with defaults. Parameters ---------- cfg_data : dict The configuration data to verify. Returns ------- dict The validated configuration dictionary with defaults applied. \"\"\" with DEFAULT_CFG_PATH . open ( \"r\" ) as f : default_data = yaml . safe_load ( f ) for key in list ( cfg_data . keys ()): if key in default_data : continue logging . warning ( \"Removing invalid config option %s from provided config\" , key ) del cfg_data [ key ] for key in list ( default_data . keys ()): if key in cfg_data : continue logging . warning ( \"Key %s not in provided config using default value of %s \" , key , default_data [ key ], ) cfg_data [ key ] = default_data [ key ] return cfg_data","title":"Config"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.Config.__getattr__","text":"Retrieve a configuration value as an attribute. Source code in src/fisseq_data_pipeline/utils/config.py 47 48 49 def __getattr__ ( self , name : str ) -> Any : \"\"\"Retrieve a configuration value as an attribute.\"\"\" return self . _data [ name ]","title":"__getattr__"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.Config.__getitem__","text":"Retrieve a configuration value using dictionary-style indexing. Source code in src/fisseq_data_pipeline/utils/config.py 51 52 53 def __getitem__ ( self , key : str ) -> Any : \"\"\"Retrieve a configuration value using dictionary-style indexing.\"\"\" return self . __getattr__ ( key )","title":"__getitem__"},{"location":"utils/config/#fisseq_data_pipeline.utils.config.DEFAULT_CFG_PATH","text":"","title":"DEFAULT_CFG_PATH"},{"location":"utils/utils/","text":"Utility functions The fisseq_data_pipeline.utils.utils module provides helper functions for feature selection, dataset construction, and splitting. These utilities are used internally by the pipeline but can also be reused in standalone scripts. Overview get_feature_selector : Build a Polars selector expression for feature columns based on the pipeline configuration. get_data_dfs : Build aligned feature and metadata DataFrames from a Polars LazyFrame . train_test_split : Create stratified train/test splits for features and metadata. Environment variables FISSEQ_PIPELINE_RAND_STATE Random seed used for reproducible stratified train/test splits. Default: 42 . Example # Use a fixed seed of 1234 for train/test splitting FISSEQ_PIPELINE_RAND_STATE=1234 fisseq-data-pipeline validate ... Example Usage import polars as pl from fisseq_data_pipeline.utils.config import Config from fisseq_data_pipeline.utils.utils import ( get_data_dfs, train_test_split ) # Example dataset df = pl.DataFrame({ \"gene1\": [1.0, 2.0, 3.0, 4.0], \"gene2\": [5.0, 6.0, 7.0, 8.0], \"batch\": [\"A\", \"A\", \"B\", \"B\"], \"label\": [\"X\", \"X\", \"Y\", \"Y\"], \"is_ctrl\": [True, False, True, False], }).lazy() # Example config (using dict for simplicity) cfg = Config({ \"feature_cols\": [\"gene1\", \"gene2\"], \"batch_col_name\": \"batch\", \"label_col_name\": \"label\", \"control_sample_query\": \"col('is_ctrl')\", }) # Build feature + metadata DataFrames feature_df, meta_data_df = get_data_dfs(df, cfg) # Stratified train/test split train_f, train_m, test_f, test_m = train_test_split(feature_df, meta_data_df, test_size=0.5) API Reference fisseq_data_pipeline . utils . utils . get_feature_selector ( data_df , config ) Build a Polars column selector for feature columns based on the config. This utility interprets config.feature_cols and returns a Polars selector expression usable in .select() or .with_columns() calls. Selection modes: - Regex : if feature_cols is a string, all column names matching the regex are selected. - Explicit list : if feature_cols is a list of strings, those columns are selected in the given order. Missing columns are ignored with a warning. Parameters: data_df ( LazyFrame ) \u2013 Input dataset containing feature columns. config ( Config ) \u2013 Configuration object with a feature_cols attribute defining which columns to select (regex pattern or explicit list). Returns: PlSelector \u2013 A Polars selector expression suitable for use in .select() calls. Notes Missing columns are ignored but logged as a warning. Column order is preserved when using an explicit list. Source code in src/fisseq_data_pipeline/utils/utils.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_feature_selector ( data_df : pl . LazyFrame , config : Config ) -> PlSelector : \"\"\" Build a Polars column selector for feature columns based on the config. This utility interprets ``config.feature_cols`` and returns a Polars selector expression usable in ``.select()`` or ``.with_columns()`` calls. Selection modes: - **Regex**: if ``feature_cols`` is a string, all column names matching the regex are selected. - **Explicit list**: if ``feature_cols`` is a list of strings, those columns are selected in the given order. Missing columns are ignored with a warning. Parameters ---------- data_df : pl.LazyFrame Input dataset containing feature columns. config : Config Configuration object with a ``feature_cols`` attribute defining which columns to select (regex pattern or explicit list). Returns ------- PlSelector A Polars selector expression suitable for use in ``.select()`` calls. Notes ----- - Missing columns are ignored but logged as a warning. - Column order is preserved when using an explicit list. \"\"\" if isinstance ( config . feature_cols , str ): selector_type = \"regex\" selector = cs . matches ( config . feature_cols ) else : selector_type = \"list\" feature_cols = set ( config . feature_cols ) missing = feature_cols - set ( data_df . columns ) if len ( missing ) != 0 : logging . warning ( \"Some columns are specified in the config but are not currently\" \" present in the dataframe, this can happen if columns are\" \" removed during data cleaning. The following columns will be\" \" ignored: %s \" , missing , ) # Column order must be preserved not_missing = feature_cols - missing selector = pl . col ( col for col in list ( config . feature_cols ) if col in not_missing ) logging . debug ( \"Using feature %s selector: %s \" , selector_type , config . feature_cols ) return selector fisseq_data_pipeline . utils . utils . get_data_dfs ( data_df , config , dtype = pl . Float32 ) Construct separate feature and metadata DataFrames from a Polars LazyFrame. This function builds a complete computation graph that: 1. Adds a row index ( _sample_idx ) for reproducible sample tracking. 2. Selects and casts feature columns defined in config.feature_cols . 3. Extracts metadata columns: - _batch from config.batch_col_name - _label from config.label_col_name - _is_control by evaluating config.control_sample_query 4. Materializes the LazyFrame once to produce a feature matrix and an eager metadata DataFrame. Parameters: data_df ( LazyFrame ) \u2013 Input dataset containing both features and metadata. config ( Config ) \u2013 Configuration object dtype ( DataType , default: Float32 ) \u2013 Data type to cast feature columns to. Defaults to pl.Float32 . Returns: ( DataFrame , DataFrame ) \u2013 A tuple containing: - feature_df : eager pl.DataFrame of numerical features, shape (n_samples, n_features) , with values cast to dtype . - meta_data_df : eager pl.DataFrame with columns: * _batch \u2014 batch labels * _label \u2014 class labels * _is_control \u2014 boolean control mask * _sample_idx \u2014 stable sample index Notes The returned feature and metadata frames share identical row ordering. Source code in src/fisseq_data_pipeline/utils/utils.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def get_data_dfs ( data_df : pl . LazyFrame , config : Config , dtype : pl . DataType = pl . Float32 , ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Construct separate feature and metadata DataFrames from a Polars LazyFrame. This function builds a complete computation graph that: 1. Adds a row index (``_sample_idx``) for reproducible sample tracking. 2. Selects and casts feature columns defined in ``config.feature_cols``. 3. Extracts metadata columns: - ``_batch`` from ``config.batch_col_name`` - ``_label`` from ``config.label_col_name`` - ``_is_control`` by evaluating ``config.control_sample_query`` 4. Materializes the LazyFrame once to produce a feature matrix and an eager metadata DataFrame. Parameters ---------- data_df : pl.LazyFrame Input dataset containing both features and metadata. config : Config Configuration object dtype : pl.DataType, optional Data type to cast feature columns to. Defaults to ``pl.Float32``. Returns ------- (pl.DataFrame, pl.DataFrame) A tuple containing: - **feature_df** : eager ``pl.DataFrame`` of numerical features, shape ``(n_samples, n_features)``, with values cast to ``dtype``. - **meta_data_df** : eager ``pl.DataFrame`` with columns: * ``_batch`` \u2014 batch labels * ``_label`` \u2014 class labels * ``_is_control`` \u2014 boolean control mask * ``_sample_idx`` \u2014 stable sample index Notes ----- - The returned feature and metadata frames share identical row ordering. \"\"\" logging . info ( \"Starting get_data_matrices for batch_col= %s , dtype= %s \" , config . batch_col_name , dtype , ) # Attach row indices to preserve mapping later base = data_df . with_row_index ( name = \"_sample_idx\" ) . cache () # Build feature selector feature_expr = get_feature_selector ( base , config ) . cast ( dtype = dtype ) logging . debug ( \"Feature selector resolved: %s \" , feature_expr ) # Control mask expr logging . debug ( \"Parsing control sample query: %s \" , config . control_sample_query ) control_mask_expr = pl . sql_expr ( config . control_sample_query ) . alias ( \"_is_control\" ) batch_expr = pl . col ( config . batch_col_name ) . alias ( \"_batch\" ) label_expr = pl . col ( config . label_col_name ) . alias ( \"_label\" ) # Execute the full plan logging . info ( \"Creating combined dataframe query\" ) lf = base . with_columns ( label_expr , batch_expr , control_mask_expr , ) . select ( feature_expr , pl . col ( \"_batch\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ), pl . col ( \"_label\" ), ) # Get feature dataframe logging . info ( \"Creating feature dataframe query\" ) feature_df = lf . select ( feature_expr ) logging . info ( \"Creating metadata frame query\" ) meta_data_df = lf . select ( pl . col ( \"_batch\" ), pl . col ( \"_label\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ), ) return feature_df , meta_data_df fisseq_data_pipeline . utils . utils . train_test_split ( feature_df , meta_data_df , test_size ) Split feature and metadata DataFrames into stratified train and test sets. Parameters: feature_df ( LazyFrame ) \u2013 Feature matrix with shape (n_samples, n_features). meta_data_df ( LazyFrame ) \u2013 Metadata aligned row-wise with feature_df . Must contain columns _label (class labels) and _batch (batch identifiers). test_size ( float ) \u2013 Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns: train_feature_df ( LazyFrame ) \u2013 Features for the training set. train_meta_data_df ( LazyFrame ) \u2013 Metadata for the training set. test_feature_df ( LazyFrame ) \u2013 Features for the test set. test_meta_data_df ( LazyFrame ) \u2013 Metadata for the test set. Notes Each (_label, _batch) group must have at least two samples for stratification to succeed. The split is reproducible if RANDOM_STATE is fixed. Source code in src/fisseq_data_pipeline/utils/utils.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def train_test_split ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame , test_size : float , ) -> Tuple [ pl . LazyFrame , pl . LazyFrame , pl . LazyFrame , pl . LazyFrame ]: \"\"\" Split feature and metadata DataFrames into stratified train and test sets. Parameters ---------- feature_df : pl.LazyFrame Feature matrix with shape (n_samples, n_features). meta_data_df : pl.LazyFrame Metadata aligned row-wise with ``feature_df``. Must contain columns ``_label`` (class labels) and ``_batch`` (batch identifiers). test_size : float Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns ------- train_feature_df : pl.LazyFrame Features for the training set. train_meta_data_df : pl.LazyFrame Metadata for the training set. test_feature_df : pl.LazyFrame Features for the test set. test_meta_data_df : pl.LazyFrame Metadata for the test set. Notes ----- - Each ``(_label, _batch)`` group must have at least two samples for stratification to succeed. - The split is reproducible if ``RANDOM_STATE`` is fixed. \"\"\" logging . info ( \"Creating lazy stratified train/test split query\" ) lf_meta = meta_data_df . with_row_index ( \"row_id\" ) . with_columns ( pl . concat_str ( [ pl . col ( \"_label\" ) . cast ( pl . Utf8 ), pl . lit ( \":\" ), pl . col ( \"_batch\" ) . cast ( pl . Utf8 ), ] ) . alias ( \"grp\" ) ) lf_test_idx = ( lf_meta . group_by ( \"grp\" ) . agg ( pl . col ( \"row_id\" ) . sample ( fraction = test_size , seed = RANDOM_STATE )) . explode ( \"row_id\" ) ) lf_mask = ( lf_meta . join ( lf_test_idx , on = \"row_id\" , how = \"left\" ) . with_columns ( pl . col ( \"grp_right\" ) . is_not_null () . alias ( \"_is_test\" )) . select ([ \"row_id\" , \"_is_test\" ]) ) logging . info ( \"Split created, executing query\" ) is_test = lf_mask . select ( pl . col ( \"_is_test\" )) . collect () . get_column ( \"_is_test\" ) is_train = ~ is_test logging . info ( \"Split completed: %d train / %d test samples\" , is_train . sum (), is_test . sum () ) logging . info ( \"Copying data\" ) train_feature_df = feature_df . filter ( is_train ) test_feature_df = feature_df . filter ( is_test ) train_meta_data_df = meta_data_df . filter ( is_train ) test_meta_data_df = meta_data_df . filter ( is_test ) return train_feature_df , train_meta_data_df , test_feature_df , test_meta_data_df","title":"Utils"},{"location":"utils/utils/#utility-functions","text":"The fisseq_data_pipeline.utils.utils module provides helper functions for feature selection, dataset construction, and splitting. These utilities are used internally by the pipeline but can also be reused in standalone scripts.","title":"Utility functions"},{"location":"utils/utils/#overview","text":"get_feature_selector : Build a Polars selector expression for feature columns based on the pipeline configuration. get_data_dfs : Build aligned feature and metadata DataFrames from a Polars LazyFrame . train_test_split : Create stratified train/test splits for features and metadata.","title":"Overview"},{"location":"utils/utils/#environment-variables","text":"FISSEQ_PIPELINE_RAND_STATE Random seed used for reproducible stratified train/test splits. Default: 42 .","title":"Environment variables"},{"location":"utils/utils/#example","text":"# Use a fixed seed of 1234 for train/test splitting FISSEQ_PIPELINE_RAND_STATE=1234 fisseq-data-pipeline validate ...","title":"Example"},{"location":"utils/utils/#example-usage","text":"import polars as pl from fisseq_data_pipeline.utils.config import Config from fisseq_data_pipeline.utils.utils import ( get_data_dfs, train_test_split ) # Example dataset df = pl.DataFrame({ \"gene1\": [1.0, 2.0, 3.0, 4.0], \"gene2\": [5.0, 6.0, 7.0, 8.0], \"batch\": [\"A\", \"A\", \"B\", \"B\"], \"label\": [\"X\", \"X\", \"Y\", \"Y\"], \"is_ctrl\": [True, False, True, False], }).lazy() # Example config (using dict for simplicity) cfg = Config({ \"feature_cols\": [\"gene1\", \"gene2\"], \"batch_col_name\": \"batch\", \"label_col_name\": \"label\", \"control_sample_query\": \"col('is_ctrl')\", }) # Build feature + metadata DataFrames feature_df, meta_data_df = get_data_dfs(df, cfg) # Stratified train/test split train_f, train_m, test_f, test_m = train_test_split(feature_df, meta_data_df, test_size=0.5)","title":"Example Usage"},{"location":"utils/utils/#api-reference","text":"","title":"API Reference"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.get_feature_selector","text":"Build a Polars column selector for feature columns based on the config. This utility interprets config.feature_cols and returns a Polars selector expression usable in .select() or .with_columns() calls. Selection modes: - Regex : if feature_cols is a string, all column names matching the regex are selected. - Explicit list : if feature_cols is a list of strings, those columns are selected in the given order. Missing columns are ignored with a warning. Parameters: data_df ( LazyFrame ) \u2013 Input dataset containing feature columns. config ( Config ) \u2013 Configuration object with a feature_cols attribute defining which columns to select (regex pattern or explicit list). Returns: PlSelector \u2013 A Polars selector expression suitable for use in .select() calls. Notes Missing columns are ignored but logged as a warning. Column order is preserved when using an explicit list. Source code in src/fisseq_data_pipeline/utils/utils.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_feature_selector ( data_df : pl . LazyFrame , config : Config ) -> PlSelector : \"\"\" Build a Polars column selector for feature columns based on the config. This utility interprets ``config.feature_cols`` and returns a Polars selector expression usable in ``.select()`` or ``.with_columns()`` calls. Selection modes: - **Regex**: if ``feature_cols`` is a string, all column names matching the regex are selected. - **Explicit list**: if ``feature_cols`` is a list of strings, those columns are selected in the given order. Missing columns are ignored with a warning. Parameters ---------- data_df : pl.LazyFrame Input dataset containing feature columns. config : Config Configuration object with a ``feature_cols`` attribute defining which columns to select (regex pattern or explicit list). Returns ------- PlSelector A Polars selector expression suitable for use in ``.select()`` calls. Notes ----- - Missing columns are ignored but logged as a warning. - Column order is preserved when using an explicit list. \"\"\" if isinstance ( config . feature_cols , str ): selector_type = \"regex\" selector = cs . matches ( config . feature_cols ) else : selector_type = \"list\" feature_cols = set ( config . feature_cols ) missing = feature_cols - set ( data_df . columns ) if len ( missing ) != 0 : logging . warning ( \"Some columns are specified in the config but are not currently\" \" present in the dataframe, this can happen if columns are\" \" removed during data cleaning. The following columns will be\" \" ignored: %s \" , missing , ) # Column order must be preserved not_missing = feature_cols - missing selector = pl . col ( col for col in list ( config . feature_cols ) if col in not_missing ) logging . debug ( \"Using feature %s selector: %s \" , selector_type , config . feature_cols ) return selector","title":"get_feature_selector"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.get_data_dfs","text":"Construct separate feature and metadata DataFrames from a Polars LazyFrame. This function builds a complete computation graph that: 1. Adds a row index ( _sample_idx ) for reproducible sample tracking. 2. Selects and casts feature columns defined in config.feature_cols . 3. Extracts metadata columns: - _batch from config.batch_col_name - _label from config.label_col_name - _is_control by evaluating config.control_sample_query 4. Materializes the LazyFrame once to produce a feature matrix and an eager metadata DataFrame. Parameters: data_df ( LazyFrame ) \u2013 Input dataset containing both features and metadata. config ( Config ) \u2013 Configuration object dtype ( DataType , default: Float32 ) \u2013 Data type to cast feature columns to. Defaults to pl.Float32 . Returns: ( DataFrame , DataFrame ) \u2013 A tuple containing: - feature_df : eager pl.DataFrame of numerical features, shape (n_samples, n_features) , with values cast to dtype . - meta_data_df : eager pl.DataFrame with columns: * _batch \u2014 batch labels * _label \u2014 class labels * _is_control \u2014 boolean control mask * _sample_idx \u2014 stable sample index Notes The returned feature and metadata frames share identical row ordering. Source code in src/fisseq_data_pipeline/utils/utils.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def get_data_dfs ( data_df : pl . LazyFrame , config : Config , dtype : pl . DataType = pl . Float32 , ) -> Tuple [ pl . LazyFrame , pl . LazyFrame ]: \"\"\" Construct separate feature and metadata DataFrames from a Polars LazyFrame. This function builds a complete computation graph that: 1. Adds a row index (``_sample_idx``) for reproducible sample tracking. 2. Selects and casts feature columns defined in ``config.feature_cols``. 3. Extracts metadata columns: - ``_batch`` from ``config.batch_col_name`` - ``_label`` from ``config.label_col_name`` - ``_is_control`` by evaluating ``config.control_sample_query`` 4. Materializes the LazyFrame once to produce a feature matrix and an eager metadata DataFrame. Parameters ---------- data_df : pl.LazyFrame Input dataset containing both features and metadata. config : Config Configuration object dtype : pl.DataType, optional Data type to cast feature columns to. Defaults to ``pl.Float32``. Returns ------- (pl.DataFrame, pl.DataFrame) A tuple containing: - **feature_df** : eager ``pl.DataFrame`` of numerical features, shape ``(n_samples, n_features)``, with values cast to ``dtype``. - **meta_data_df** : eager ``pl.DataFrame`` with columns: * ``_batch`` \u2014 batch labels * ``_label`` \u2014 class labels * ``_is_control`` \u2014 boolean control mask * ``_sample_idx`` \u2014 stable sample index Notes ----- - The returned feature and metadata frames share identical row ordering. \"\"\" logging . info ( \"Starting get_data_matrices for batch_col= %s , dtype= %s \" , config . batch_col_name , dtype , ) # Attach row indices to preserve mapping later base = data_df . with_row_index ( name = \"_sample_idx\" ) . cache () # Build feature selector feature_expr = get_feature_selector ( base , config ) . cast ( dtype = dtype ) logging . debug ( \"Feature selector resolved: %s \" , feature_expr ) # Control mask expr logging . debug ( \"Parsing control sample query: %s \" , config . control_sample_query ) control_mask_expr = pl . sql_expr ( config . control_sample_query ) . alias ( \"_is_control\" ) batch_expr = pl . col ( config . batch_col_name ) . alias ( \"_batch\" ) label_expr = pl . col ( config . label_col_name ) . alias ( \"_label\" ) # Execute the full plan logging . info ( \"Creating combined dataframe query\" ) lf = base . with_columns ( label_expr , batch_expr , control_mask_expr , ) . select ( feature_expr , pl . col ( \"_batch\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ), pl . col ( \"_label\" ), ) # Get feature dataframe logging . info ( \"Creating feature dataframe query\" ) feature_df = lf . select ( feature_expr ) logging . info ( \"Creating metadata frame query\" ) meta_data_df = lf . select ( pl . col ( \"_batch\" ), pl . col ( \"_label\" ), pl . col ( \"_is_control\" ), pl . col ( \"_sample_idx\" ), ) return feature_df , meta_data_df","title":"get_data_dfs"},{"location":"utils/utils/#fisseq_data_pipeline.utils.utils.train_test_split","text":"Split feature and metadata DataFrames into stratified train and test sets. Parameters: feature_df ( LazyFrame ) \u2013 Feature matrix with shape (n_samples, n_features). meta_data_df ( LazyFrame ) \u2013 Metadata aligned row-wise with feature_df . Must contain columns _label (class labels) and _batch (batch identifiers). test_size ( float ) \u2013 Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns: train_feature_df ( LazyFrame ) \u2013 Features for the training set. train_meta_data_df ( LazyFrame ) \u2013 Metadata for the training set. test_feature_df ( LazyFrame ) \u2013 Features for the test set. test_meta_data_df ( LazyFrame ) \u2013 Metadata for the test set. Notes Each (_label, _batch) group must have at least two samples for stratification to succeed. The split is reproducible if RANDOM_STATE is fixed. Source code in src/fisseq_data_pipeline/utils/utils.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def train_test_split ( feature_df : pl . LazyFrame , meta_data_df : pl . LazyFrame , test_size : float , ) -> Tuple [ pl . LazyFrame , pl . LazyFrame , pl . LazyFrame , pl . LazyFrame ]: \"\"\" Split feature and metadata DataFrames into stratified train and test sets. Parameters ---------- feature_df : pl.LazyFrame Feature matrix with shape (n_samples, n_features). meta_data_df : pl.LazyFrame Metadata aligned row-wise with ``feature_df``. Must contain columns ``_label`` (class labels) and ``_batch`` (batch identifiers). test_size : float Proportion of the dataset to include in the test split. Should be a float between 0.0 and 1.0. Returns ------- train_feature_df : pl.LazyFrame Features for the training set. train_meta_data_df : pl.LazyFrame Metadata for the training set. test_feature_df : pl.LazyFrame Features for the test set. test_meta_data_df : pl.LazyFrame Metadata for the test set. Notes ----- - Each ``(_label, _batch)`` group must have at least two samples for stratification to succeed. - The split is reproducible if ``RANDOM_STATE`` is fixed. \"\"\" logging . info ( \"Creating lazy stratified train/test split query\" ) lf_meta = meta_data_df . with_row_index ( \"row_id\" ) . with_columns ( pl . concat_str ( [ pl . col ( \"_label\" ) . cast ( pl . Utf8 ), pl . lit ( \":\" ), pl . col ( \"_batch\" ) . cast ( pl . Utf8 ), ] ) . alias ( \"grp\" ) ) lf_test_idx = ( lf_meta . group_by ( \"grp\" ) . agg ( pl . col ( \"row_id\" ) . sample ( fraction = test_size , seed = RANDOM_STATE )) . explode ( \"row_id\" ) ) lf_mask = ( lf_meta . join ( lf_test_idx , on = \"row_id\" , how = \"left\" ) . with_columns ( pl . col ( \"grp_right\" ) . is_not_null () . alias ( \"_is_test\" )) . select ([ \"row_id\" , \"_is_test\" ]) ) logging . info ( \"Split created, executing query\" ) is_test = lf_mask . select ( pl . col ( \"_is_test\" )) . collect () . get_column ( \"_is_test\" ) is_train = ~ is_test logging . info ( \"Split completed: %d train / %d test samples\" , is_train . sum (), is_test . sum () ) logging . info ( \"Copying data\" ) train_feature_df = feature_df . filter ( is_train ) test_feature_df = feature_df . filter ( is_test ) train_meta_data_df = meta_data_df . filter ( is_train ) test_meta_data_df = meta_data_df . filter ( is_test ) return train_feature_df , train_meta_data_df , test_feature_df , test_meta_data_df","title":"train_test_split"}]}